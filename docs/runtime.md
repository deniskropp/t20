# Runtime Documentation

## Overview

This document provides comprehensive documentation for the runtime system, detailing its components, core concepts, API, and usage. The design emphasizes clarity, accessibility, and a clean, minimalist aesthetic, ensuring an informative and visually appealing user experience.

## Table of Contents

*   [Introduction](#introduction)
*   [Getting Started](#getting-started)
*   [Core Concepts](#core-concepts)
*   [API Reference](#api-reference)
*   [Advanced Usage & Tutorials](#advanced-usage--tutorials)
*   [Contribution Guide](#contribution-guide)
*   [Troubleshooting & FAQ](#troubleshooting--faq)

## Introduction

### Overview of Runtime Components

The runtime system is designed to facilitate the execution of multi-agent workflows. Key components include:

*   **`runtime.agent.Agent`**: The base class for all agents, defining their lifecycle, task execution logic, and interaction patterns. Agents are the fundamental units that perform tasks within the system.
*   **`runtime.core.Session`**: Manages the overall state and artifacts of a workflow execution. It handles session IDs, artifact storage, and provides an interface for agents to interact with the system's state.
*   **`runtime.core.ExecutionContext`**: Holds the context for a specific step or round within a workflow. It includes the plan, current step, and a collection of recorded artifacts from previous steps.
*   **`runtime.orchestrator.Orchestrator`**: A specialized agent responsible for generating and managing the workflow plan. It interprets the high-level goal and breaks it down into a series of tasks for other agents.
*   **`runtime.llm.LLM`**: An abstract base class that abstracts LLM interactions. It provides a unified interface for various LLM providers and models, supporting a factory pattern for easy integration and interchangeability.
*   **`runtime.custom_types`**: Defines the Pydantic models used for structured data exchange, including `Plan`, `Task`, `File`, `Artifact`, `Prompt`, and `Team`.

## Getting Started

### Installation

To install the runtime, navigate to the project's root directory and run:

```bash
pip install .
```

### Basic Usage

To run a workflow, use the `runtime.sysmain` module. You need to provide an initial task description (which serves as the high-level goal) and optionally specify the number of rounds and input files.

```bash
python -m runtime.sysmain "Your initial task description" -r 3 -f file1.txt file2.py
```

*   `"Your initial task description"`: The high-level goal for the workflow.
*   `-r 3`: Specifies that the workflow should run for 3 rounds.
*   `-f file1.txt file2.py`: Lists input files to be used during the task execution.

## Core Concepts

### Agents

Agents are the core actors in the system. Each agent has a name, role, goal, and can be configured with a specific LLM model and system prompt. They execute tasks based on the provided context and plan.

### Plans and Tasks

A `Plan` is a structured definition of a workflow, comprising a sequence of `Task` objects. Each `Task` has an ID, a description, a responsible role, an assigned agent, and a list of prerequisite task IDs.

### Artifacts

Artifacts are outputs generated by agents during task execution. They can be stored and retrieved using the `Session` and `ExecutionContext` objects. The `ExecutionContext.record_artifact` method is used to save artifacts for the current step, while `ExecutionContext.record_initial` is used for initial inputs.

### `AgentOutput` Model

The `AgentOutput` Pydantic model defines the structured response format for agents. It includes fields for the main `output` message, an optional `artifact` (containing files), and optional `team` updates (like new system prompts). This model ensures consistent communication between agents.

### LLM Integration

The `runtime.llm.LLM` class and its subclasses abstract LLM interactions. The `LLM.factory` method allows for easy instantiation of different LLM providers based on configuration.

## API Reference

### `runtime.agent.Agent`

*   **`__init__(self, name: str, role: str, goal: str, model: str, system_prompt: str)`**: Constructor for the `Agent` class.
*   **`update_system_prompt(self, new_prompt: str)`**: Updates the agent's system prompt.
*   **`execute_task(self, context: ExecutionContext) -> Optional[str]`**: Executes a task using the LLM based on the provided context. It prepares the prompt, including previous artifacts, and calls the LLM.
*   **`_run(self, context: ExecutionContext, prompt: str) -> Optional[str]`**: Handles the core LLM interaction, including sending the prompt, processing the response, parsing JSON, and extracting artifacts. It also manages retries for LLM calls.

### `runtime.core.Session`

*   **`__init__(self, session_id: str, agents: list, state: str, session_dir: str, project_root: str)`**: Constructor for the `Session` class.
*   **`add_artifact(self, name: str, content: Any)`**: Saves an artifact in the session directory. `content` can be a string, dictionary, or list. JSON content is saved with a `.json` extension.
*   **`get_artifact(self, name: str) -> Any`**: Loads an artifact from the session directory. Returns the content or `None` if an error occurs.

### `runtime.core.ExecutionContext`

*   **`__init__(self, session: Session, plan: Plan, step_index: int, round_num: int, items: Dict[str, ContextItem])`**: Constructor for `ExecutionContext`.
*   **`next(self)`**: Advances the step index.
*   **`reset(self)`**: Resets the step index to 0.
*   **`current_step(self) -> Task`**: Returns the current `Task` object.
*   **`record_artifact(self, key: str, value: Any, mem: bool = False)`**: Records an artifact for the current step. If `mem` is `True`, it's also stored in the context's memory.
*   **`record_initial(self, key: str, value: Any)`**: Records initial artifacts provided at the start of the workflow.

### `runtime.custom_types` Module

*   **`Task`**: Represents a single actionable step in a plan.
    *   `id`: Unique ID for the task.
    *   `description`: What the task is and how to execute it.
    *   `role`: Role responsible for performing this task.
    *   `agent`: Specific agent assigned to this task.
    *   `requires`: List of prerequisite task IDs.
*   **`File`**: Represents a generated file.
    *   `path`: File path or name.
    *   `content`: Full content of the file.
*   **`Artifact`**: Output artifact generated by a task.
    *   `task`: ID of the task that produced this artifact.
    *   `files`: List of `File` objects included in the artifact.
*   **`Role`**: Definition of a role in the team.
    *   `title`: Role title.
    *   `purpose`: High-level responsibilities and objectives.
*   **`Plan`**: Overall strategy with reasoning, roles, and tasks.
    *   `high_level_goal`: The overarching objective.
    *   `reasoning`: Why this plan is structured this way.
    *   `roles`: All roles needed for execution.
    *   `tasks`: Step-by-step ordered tasks.
*   **`Prompt`**: System prompt assigned to an agent.
    *   `agent`: Name of the agent this prompt is for.
    *   `role`: Role context of the agent.
    *   `system_prompt`: Full text of the system prompt.
*   **`Team`**: Metadata and coordination for the team.
    *   `notes`: General notes, reflections, or feedback.
    *   `prompts`: Updated system prompts for agents.
*   **`AgentOutput`**: Structured output returned by an agent.
    *   `output`: Message from the agent.
    *   `artifact`: Optional `Artifact` generated by the agent.
    *   `team`: Optional `Team` updates.
    *   `reasoning`: Explanation of how the agent arrived at this output.

## Advanced Usage & Tutorials

### Advanced Planning

The planning process is primarily handled by the `Orchestrator` agent. To influence plan generation, you can:

1.  **Refine the High-Level Goal:** A more specific and well-defined goal leads to a more effective plan.
2.  **Engineer Prompts:** Modifying the system prompt for the `Orchestrator` (e.g., by adjusting `runtime/prompts/general_planning.txt`) can guide its reasoning and task decomposition. This allows for customization of how the orchestrator approaches planning, including task dependencies and role assignments.
3.  **Provide Initial Files:** Including relevant context files during system startup can help the orchestrator create more informed plans.

## Contribution Guide

### Running Tests

To run the test suite, navigate to the project's root directory and execute:

```bash
pytest
```

This command will discover and run all tests defined within the project.

## Troubleshooting & FAQ

### How do I configure different LLM providers?

LLM providers are configured within the `runtime.yaml` configuration file. You can specify the model name and potentially provider-specific settings (like API keys or base URLs) in the configuration. The `runtime.llm.LLM.factory` method handles the instantiation based on the model name provided (e.g., `ollama:llama3`, `gemini-1.5-flash`, `openai:gpt-4`). Ensure that any necessary API keys or environment variables are set correctly before running the system.

---

*This documentation is styled using a minimalist design aesthetic. The color palette features deep blue (`#0A2540`) and vibrant teal (`#1ABC9C`), with clean typography using 'Inter' for body text and 'Fira Code' for code. The layout is designed for optimal readability and accessibility, with a clear navigation structure and ample white space.*