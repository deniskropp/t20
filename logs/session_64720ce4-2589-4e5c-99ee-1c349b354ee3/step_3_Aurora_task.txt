The overall goal is: 'Create a website that is a frontend to an AI system like this.'

Your role's specific goal is: 'Generate aesthetic layouts, color palettes, typography, and UI flows, ensuring accessibility and visual balance.'
Your specific sub-task is: 'Create high-fidelity mockups and a complete UI kit, including color palette, typography, and component styles based on the approved wireframes. Ensure all designs adhere to accessibility standards (WCAG).'

The team's roles are:
    {'roles': [{'title': 'Task-Agnostic Step (TAS) extractor (flash)', 'description': "Identify and return each 'Task Agnostic Step' (TAS) towards the high-level goal."}, {'title': 'Prompt Engineer', 'description': 'Structure workflows and ensure clarity in agent instructions, system prompt engineering.'}, {'title': 'Designer', 'description': 'Generate aesthetic layouts, color palettes, typography, and UI flows, ensuring accessibility and visual balance.'}, {'title': 'Engineer', 'description': 'Implement designs into clean, modular, and performant code, focusing on responsive design and accessibility.'}], 'steps': [{'name': 'uTASe', 'role': 'Task-Agnostic Step (TAS) extractor (flash)', 'task': "Extract the initial high-level task: 'Project Scoping and Foundational Design'."}, {'name': 'Lyra', 'role': 'Prompt Engineer', 'task': "Create a detailed workflow for the 'Project Scoping and Foundational Design' phase. Draft an initial system prompt for Aurora to begin the design process, focusing on user flows for an AI interaction interface."}, {'name': 'Aurora', 'role': 'Designer', 'task': "Develop low-fidelity wireframes and user flow diagrams for the AI frontend. Focus on the core interaction loop: user input, AI 'thinking' visualization, and final output display."}, {'name': 'Aurora', 'role': 'Designer', 'task': 'Create high-fidelity mockups and a complete UI kit, including color palette, typography, and component styles based on the approved wireframes. Ensure all designs adhere to accessibility standards (WCAG).'}, {'name': 'uTASe', 'role': 'Task-Agnostic Step (TAS) extractor (flash)', 'task': "Extract the next high-level task: 'Frontend Development and Component Implementation'."}, {'name': 'Lyra', 'role': 'Prompt Engineer', 'task': "Translate Aurora's high-fidelity designs and UI kit into a detailed set of technical specifications and user stories. Create a system prompt for Kodax to begin development."}, {'name': 'Kodax', 'role': 'Engineer', 'task': 'Set up the frontend project architecture and develop the static UI components based on the mockups and specifications. Ensure the application is fully responsive.'}, {'name': 'uTASe', 'role': 'Task-Agnostic Step (TAS) extractor (flash)', 'task': "Extract the next high-level task: 'AI System Integration'."}, {'name': 'Lyra', 'role': 'Prompt Engineer', 'task': "Define the API contract (request/response schema) for communication between the frontend and the AI system. Refine the AI's system prompt to ensure its output matches the required JSON structure for displaying the 'thinking' process."}, {'name': 'Kodax', 'role': 'Engineer', 'task': "Integrate the frontend with the AI system's API. Implement logic to handle data fetching, loading states, error handling, and rendering the structured AI response."}, {'name': 'uTASe', 'role': 'Task-Agnostic Step (TAS) extractor (flash)', 'task': "Extract the final high-level task: 'End-to-End Testing, Refinement, and Deployment'."}, {'name': 'Aurora', 'role': 'Designer', 'task': 'Conduct a full UI/UX and accessibility review of the integrated application. Document any discrepancies between the design and the final product.'}, {'name': 'Lyra', 'role': 'Prompt Engineer', 'task': "Perform end-to-end testing of the AI interaction flow. Iteratively refine the AI's system prompts based on test results to improve response quality and reliability."}, {'name': 'Kodax', 'role': 'Engineer', 'task': 'Implement feedback from the design and prompt engineering reviews. Finalize the codebase, set up a CI/CD pipeline, and deploy the application to a live environment.'}]}

Please use the following outputs from the other agents as your input:

Artifact from step_0_uTASe_result.txt (Task-Agnostic Step (TAS) extractor (flash)):
[
  {
    "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
    "name": "Scope Project and Design Foundation",
    "description": "Define the project's objectives, target audience, core features, and technical constraints. Establish the foundational architectural decisions and user experience principles that will guide the development process. This includes initial research, defining project scope, and creating a high-level plan.",
    "category": "Init",
    "purpose": "Establish the project's direction and fundamental design principles.",
    "keywords": [
      "scoping",
      "foundational design",
      "project planning",
      "architecture",
      "user experience"
    ],
    "applicability_notes": "This step is crucial for any new project to ensure a clear understanding of goals and a solid starting point. It applies across all domains and project types.",
    "examples_of_usage": [
      "Defining the features and target users for a new mobile application.",
      "Outlining the technical architecture and key performance indicators for a web service.",
      "Establishing the core functionality and user journey for a software tool."
    ],
    "typical_inputs": [
      "Project idea or business requirement",
      "Stakeholder input",
      "Market research"
    ],
    "typical_outputs": [
      "Project scope document",
      "High-level architecture plan",
      "Initial user personas",
      "Feasibility study"
    ]
  }
]

---

Artifact from step_1_Lyra_result.txt (Prompt Engineer):
{
  "workflow": [
    {
      "step_name": "Project Brief & AI Core Functionality Definition",
      "description": "Synthesize the overall project goal ('Create a website that is a frontend to an AI system') and define the specific type of AI (e.g., conversational, generative, analytical) and its primary interaction paradigm. This step establishes the foundational understanding for subsequent design and engineering efforts.",
      "responsible": ["Lyra"],
      "output": "Internal brief detailing the AI's purpose, core capabilities, and primary interaction model."
    },
    {
      "step_name": "User Persona & Key Use Case Identification",
      "description": "Identify the target user personas for the AI frontend and define their primary goals, motivations, and pain points when interacting with such a system. Outline critical use case scenarios that the frontend must support to address these user needs.",
      "responsible": ["Lyra", "Aurora"],
      "output": "Defined user personas and a prioritized list of core use cases/scenarios."
    },
    {
      "step_name": "Core AI Interaction User Flow & Low-Fidelity Wireframing",
      "description": "Based on the defined user personas and use cases, develop detailed user flow diagrams and accompanying low-fidelity wireframes. This step specifically focuses on the fundamental AI interaction loop: user input, visual representation of AI processing ('thinking'), and the display of AI output.",
      "responsible": ["Aurora"],
      "output": "Comprehensive user flow diagrams and low-fidelity wireframes for the core AI interaction screens."
    },
    {
      "step_name": "Foundational Design Review & Refinement",
      "description": "Conduct a review session with relevant stakeholders (Lyra, Kodax) to gather feedback on the initial user flows and wireframes. Assess usability, clarity, and technical feasibility. Iterate on the designs based on the feedback received to ensure alignment with project goals and user needs.",
      "responsible": ["Lyra", "Aurora", "Kodax"],
      "output": "Approved and refined user flow diagrams and low-fidelity wireframes, ready for high-fidelity design."
    }
  ],
  "aurora_system_prompt": {
    "role": "You are Aurora, a highly skilled UI/UX Designer specializing in creating intuitive and engaging user interfaces for AI-powered applications. Your expertise lies in translating complex AI interactions into clear, user-friendly experiences.",
    "context": "We are initiating the design phase for an AI system frontend. Your current objective is to establish the foundational user experience by mapping out the core interaction flows. The overarching goal is to ensure a transparent, responsive, and easy-to-understand interaction between the user and the AI.",
    "objective": "Design detailed user flow diagrams and corresponding low-fidelity wireframes for the primary AI interaction sequence. Focus on clarity, user feedback, and managing user expectations throughout the AI's processing cycle.",
    "design_focus_areas": [
      {
        "area": "User Input Mechanism",
        "description": "How does the user initiate interaction and provide input to the AI? Consider various input types (e.g., text, voice, file upload, structured forms, selections) and design clear, accessible input fields or controls. How is the user's input confirmed or prepared for submission?"
      },
      {
        "area": "AI 'Thinking' Visualization",
        "description": "Crucially, how do we visually communicate that the AI is actively processing the user's request? Design clear and reassuring visual cues for the 'thinking' state. This includes progressive loading indicators, animated elements (e.g., animated dots, pulsing icons), and informative status messages (e.g., 'AI is analyzing...', 'Generating response...', 'Please wait...'). The goal is to manage user expectations and reduce perceived latency during AI processing."
      },
      {
        "area": "AI Output Display",
        "description": "How is the AI's response presented clearly, concisely, and effectively to the user? Consider different output formats (e.g., plain text, markdown, structured data, images, charts, code snippets) and design layouts that enhance readability and comprehension. If the output is complex, how can it be broken down, summarized, or presented incrementally?"
      },
      {
        "area": "Interaction Feedback & Error Handling",
        "description": "How does the system provide immediate and clear feedback on successful operations, warnings, or errors? Design intuitive messages and visual indicators for scenarios such as invalid input, AI processing failures, or other system issues. How can users easily understand what went wrong and recover from errors or refine their input?"
      },
      {
        "area": "Follow-up & Iteration",
        "description": "Once an AI response is received, how can the user easily initiate a follow-up query, refine their previous input, or explore related options? Design mechanisms that facilitate continued conversation or iterative refinement of requests, enabling a fluid user journey."
      }
    ],
    "deliverables": [
      "Comprehensive user flow diagrams illustrating the complete end-to-end AI interaction, including all key states (user input, AI processing/thinking, AI output display, error states, and follow-up actions).",
      "Low-fidelity wireframes for each significant screen or state identified in the user flows. These wireframes should focus on layout, content hierarchy, primary interaction elements, and the placement of visual feedback mechanisms."
    ],
    "constraints_and_assumptions": [
      "Focus primarily on functional layout, user experience, and interaction patterns rather than detailed visual aesthetics (color, typography, final styling) at this initial stage.",
      "Assume a standard web-based interface for the frontend application.",
      "Prioritize clarity, usability, and effective communication of AI status over implementing complex or niche features."
    ]
  }
}

---

Artifact from step_2_Aurora_result.txt (Designer):
{
  "user_flow_diagrams": [
    {
      "id": "uf_ai_interaction_001",
      "name": "Core AI Interaction Flow",
      "description": "This flow details the primary interaction loop of the AI system, from user input to AI output.",
      "stages": [
        {
          "stage_id": "uf_stage_01",
          "name": "User Input",
          "description": "User provides input to the AI system.",
          "next_stage_ids": ["uf_stage_02"]
        },
        {
          "stage_id": "uf_stage_02",
          "name": "AI Processing (Thinking)",
          "description": "System visually indicates the AI is processing the request.",
          "next_stage_ids": ["uf_stage_03", "uf_stage_05"]
        },
        {
          "stage_id": "uf_stage_03",
          "name": "AI Output Display",
          "description": "AI's response is presented to the user.",
          "next_stage_ids": ["uf_stage_04"]
        },
        {
          "stage_id": "uf_stage_04",
          "name": "Follow-up/Iteration",
          "description": "User can provide further input or refine the previous request.",
          "next_stage_ids": ["uf_stage_02"]
        },
        {
          "stage_id": "uf_stage_05",
          "name": "Error State",
          "description": "If processing fails, an error is displayed to the user.",
          "next_stage_ids": ["uf_stage_01", "uf_stage_04"]
        }
      ]
    }
  ],
  "low_fidelity_wireframes": [
    {
      "id": "wf_input_001",
      "name": "Input Screen",
      "description": "Represents the primary screen where users input their queries.",
      "layout": {
        "header": "AI Assistant",
        "main_content": [
          {"type": "text_area", "label": "Your Request", "placeholder": "Type your question or command here...", "id": "user_input_textarea"},
          {"type": "button", "label": "Send", "id": "send_button", "state": "enabled"}
        ],
        "footer": null
      },
      "interaction_elements": [
        {"element_id": "user_input_textarea", "interaction": "user can type text"},
        {"element_id": "send_button", "interaction": "user clicks to submit input"}
      ]
    },
    {
      "id": "wf_processing_001",
      "name": "Processing Screen",
      "description": "Visual representation of the AI processing the user's input.",
      "layout": {
        "header": "AI Assistant",
        "main_content": [
          {"type": "text_area", "label": "Your Request", "value": "User's submitted query", "state": "disabled"},
          {"type": "visual_indicator", "indicator_type": "animated_dots", "label": "AI is thinking...", "id": "processing_indicator"},
          {"type": "status_message", "message": "Analyzing your request...", "id": "processing_message"}
        ],
        "footer": null
      },
      "interaction_elements": [
        {"element_id": "processing_indicator", "interaction": "provides visual feedback of AI activity"},
        {"element_id": "processing_message", "interaction": "informs user about the current processing stage"}
      ]
    },
    {
      "id": "wf_output_001",
      "name": "Output Screen",
      "description": "Displays the AI's response.",
      "layout": {
        "header": "AI Assistant",
        "main_content": [
          {"type": "text_area", "label": "Your Request", "value": "User's submitted query", "state": "disabled"},
          {"type": "output_display", "label": "AI Response", "content_type": "text", "placeholder": "AI's generated response will appear here...", "id": "ai_output_display"},
          {"type": "button", "label": "Refine Request", "id": "refine_button", "state": "enabled"},
          {"type": "button", "label": "New Request", "id": "new_request_button", "state": "enabled"}
        ],
        "footer": null
      },
      "interaction_elements": [
        {"element_id": "ai_output_display", "interaction": "displays AI-generated content"},
        {"element_id": "refine_button", "interaction": "user clicks to modify previous query"},
        {"element_id": "new_request_button", "interaction": "user clicks to start a fresh interaction"}
      ]
    },
    {
      "id": "wf_error_001",
      "name": "Error Screen",
      "description": "Displays an error message when the AI processing fails.",
      "layout": {
        "header": "AI Assistant",
        "main_content": [
          {"type": "text_area", "label": "Your Request", "value": "User's submitted query", "state": "disabled"},
          {"type": "alert_message", "message_type": "error", "message": "An error occurred. Please try again or refine your request.", "id": "error_message"},
          {"type": "button", "label": "Try Again", "id": "try_again_button", "state": "enabled"},
          {"type": "button", "label": "Refine Request", "id": "refine_error_button", "state": "enabled"}
        ],
        "footer": null
      },
      "interaction_elements": [
        {"element_id": "error_message", "interaction": "notifies user of a processing failure"},
        {"element_id": "try_again_button", "interaction": "user clicks to re-attempt the same request"},
        {"element_id": "refine_error_button", "interaction": "user clicks to modify the original query"}
      ]
    }
  ]
}



Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.