{
  "reasoning": "The user's sub-task is to 'Develop data validation rules and procedures.' I have received input from the Data Engineer (T13) and the Ingestion Specialist (T18). The Data Engineer's plan outlines the need for data validation (T19) as a key step in the ingestion pipeline, mentioning schema validation and content validation for 3D model data. The Ingestion Specialist's plan details the initial validation checks (file type, size, metadata presence) performed at the API endpoint. My task is to define the data validation rules and procedures, building upon these inputs. This means I need to elaborate on the types of validation, the specific rules, and how these procedures will be implemented within the overall data ingestion and management framework.",
  "output": "## Data Validation Rules and Procedures for the 3D API\n\nThis document defines the data validation rules and procedures to ensure the integrity, quality, and consistency of data ingested into the 3D API. These procedures build upon the initial validation performed during upload (T18) and the overall data management plan (T13).\n\n### 1. Validation Stages and Types\n\nData validation will occur at multiple stages of the ingestion pipeline:\n\n*   **Stage 1: In-Transit Validation (API Endpoint - T18):** Performed immediately upon receiving an asset. Focuses on basic integrity and format.\n*   **Stage 2: Pre-Processing Validation (Ingestion Pipeline):** Performed after initial upload but before intensive processing or storage. Ensures data is ready for transformation.\n*   **Stage 3: Post-Processing Validation (Ingestion Pipeline):** Performed after data transformation or conversion. Verifies the success and correctness of the processing steps.\n*   **Stage 4: Data-at-Rest Validation (Ongoing):** Periodic checks on data already stored to detect corruption or drift.\n\n### 2. Specific Validation Rules\n\n**2.1. File-Level Validation (Stage 1):\n\n*   **File Type:** Check against a predefined list of supported file extensions and MIME types (e.g., `.gltf`, `.glb`, `.obj`, `.fbx`, `.stl`, `.png`, `.jpg`, `.jpeg`). Reject unsupported types.\n*   **File Size:** Enforce maximum and minimum file size limits. Reject files that are too large or empty.\n*   **Integrity Check:** For certain formats (e.g., compressed archives), perform a basic integrity check (e.g., CRC check if applicable).\n\n**2.2. Metadata Validation (Stage 1 & 2):\n\n*   **Schema Conformance:** Validate all provided metadata against predefined JSON schemas. This ensures expected fields, data types, and formats (e.g., dates, UUIDs, numerical ranges).\n*   **Required Fields:** Ensure all mandatory metadata fields are present.\n*   **Value Constraints:** Check if metadata values fall within acceptable ranges or adhere to specific formats (e.g., valid enum values, positive integers for counts).\n*   **Uniqueness:** For specific fields (e.g., asset IDs, user IDs), ensure uniqueness.\n\n**2.3. 3D Model Content Validation (Stage 2 & 3):\n\n*   **Format Specific Parsing:** Attempt to parse the 3D model file using relevant libraries (e.g., `assimp`, `gltf-transform`). Fail if parsing errors occur.\n*   **Geometry Integrity:**\n    *   **Manifold Meshes:** Check for non-manifold geometry (e.g., holes, self-intersections) where applicable.\n    *   **Valid Normals & UVs:** Ensure normals are consistent and UV coordinates are within expected ranges.\n    *   **Vertex/Triangle Count:** Check against reasonable limits to prevent excessively complex models.\n*   **Texture Validation:**\n    *   **Format & Size:** Verify texture image files are valid and within size limits.\n    *   **Color Space:** Ensure textures use appropriate color spaces (e.g., sRGB for albedo, linear for normal maps).\n*   **Scene Graph Consistency:** For scene description files, validate hierarchy structure and transformation matrix validity.\n\n**2.4. Data Transformation Validation (Stage 3):\n\n*   **Conversion Success:** Verify that format conversions (e.g., OBJ to glTF) completed without errors.\n*   **Output Integrity:** Re-run critical content validation checks on the converted/processed asset to ensure no degradation or corruption occurred.\n*   **Metadata Updates:** Ensure any metadata generated or modified during processing is valid.\n\n**2.5. Data-at-Rest Validation (Stage 4):\n\n*   **Checksum Verification:** Periodically re-calculate and verify checksums of stored assets against stored values.\n*   **File System Checks:** Run file system integrity checks on storage volumes.\n*   **Data Sampling:** Randomly sample stored data and re-run validation checks to detect silent corruption.\n\n### 3. Validation Procedures and Implementation\n\n*   **Automated Checks:** All validation rules (except Stage 4) will be automated as part of the ingestion pipeline.\n*   **Schema Management:** A central repository for JSON schemas will be maintained. Tools like `ajv` (for Node.js) or similar libraries will be used for schema validation.\n*   **Processing Libraries:** Leverage established libraries for parsing and validating 3D file formats (e.g., `gltf-transform`, `three.js` loaders, `assimp`).\n*   **Error Reporting:** Validation failures will be logged with detailed information (file, rule violated, error message). Notifications will be sent for critical failures.\n*   **Quarantine/Rejection:** Assets failing Stage 1 or Stage 2 validation may be rejected outright or moved to a quarantine area for manual review. Assets failing Stage 3 validation will trigger rollback or re-processing attempts.\n*   **Feedback Loop:** Validation results will feed back into the `Improvement Analyst` (T29) and `Refinement Strategist` (T30) for identifying common issues and improving data quality over time.\n*   **Data Validator Role (T19):** This role is responsible for defining, implementing, and maintaining these validation rules and procedures, working closely with `Data Engineer` (T13) and `Ingestion Specialist` (T18).",
  "files": [
    {
      "name": "data_validation_rules.md",
      "content": "## Data Validation Rules and Procedures for the 3D API\n\nThis document defines the data validation rules and procedures to ensure the integrity, quality, and consistency of data ingested into the 3D API. These procedures build upon the initial validation performed during upload (T18) and the overall data management plan (T13).\n\n### 1. Validation Stages and Types\n\nData validation will occur at multiple stages of the ingestion pipeline:\n\n*   **Stage 1: In-Transit Validation (API Endpoint - T18):** Performed immediately upon receiving an asset. Focuses on basic integrity and format.\n*   **Stage 2: Pre-Processing Validation (Ingestion Pipeline):** Performed after initial upload but before intensive processing or storage. Ensures data is ready for transformation.\n*   **Stage 3: Post-Processing Validation (Ingestion Pipeline):** Performed after data transformation or conversion. Verifies the success and correctness of the processing steps.\n*   **Stage 4: Data-at-Rest Validation (Ongoing):** Periodic checks on data already stored to detect corruption or drift.\n\n### 2. Specific Validation Rules\n\n**2.1. File-Level Validation (Stage 1):\n\n*   **File Type:** Check against a predefined list of supported file extensions and MIME types (e.g., `.gltf`, `.glb`, `.obj`, `.fbx`, `.stl`, `.png`, `.jpg`, `.jpeg`). Reject unsupported types.\n*   **File Size:** Enforce maximum and minimum file size limits. Reject files that are too large or empty.\n*   **Integrity Check:** For certain formats (e.g., compressed archives), perform a basic integrity check (e.g., CRC check if applicable).\n\n**2.2. Metadata Validation (Stage 1 & 2):\n\n*   **Schema Conformance:** Validate all provided metadata against predefined JSON schemas. This ensures expected fields, data types, and formats (e.g., dates, UUIDs, numerical ranges).\n*   **Required Fields:** Ensure all mandatory metadata fields are present.\n*   **Value Constraints:** Check if metadata values fall within acceptable ranges or adhere to specific formats (e.g., valid enum values, positive integers for counts).\n*   **Uniqueness:** For specific fields (e.g., asset IDs, user IDs), ensure uniqueness.\n\n**2.3. 3D Model Content Validation (Stage 2 & 3):\n\n*   **Format Specific Parsing:** Attempt to parse the 3D model file using relevant libraries (e.g., `assimp`, `gltf-transform`). Fail if parsing errors occur.\n*   **Geometry Integrity:**\n    *   **Manifold Meshes:** Check for non-manifold geometry (e.g., holes, self-intersections) where applicable.\n    *   **Valid Normals & UVs:** Ensure normals are consistent and UV coordinates are within expected ranges.\n    *   **Vertex/Triangle Count:** Check against reasonable limits to prevent excessively complex models.\n*   **Texture Validation:**\n    *   **Format & Size:** Verify texture image files are valid and within size limits.\n    *   **Color Space:** Ensure textures use appropriate color spaces (e.g., sRGB for albedo, linear for normal maps).\n*   **Scene Graph Consistency:** For scene description files, validate hierarchy structure and transformation matrix validity.\n\n**2.4. Data Transformation Validation (Stage 3):\n\n*   **Conversion Success:** Verify that format conversions (e.g., OBJ to glTF) completed without errors.\n*   **Output Integrity:** Re-run critical content validation checks on the converted/processed asset to ensure no degradation or corruption occurred.\n*   **Metadata Updates:** Ensure any metadata generated or modified during processing is valid.\n\n**2.5. Data-at-Rest Validation (Stage 4):\n\n*   **Checksum Verification:** Periodically re-calculate and verify checksums of stored assets against stored values.\n*   **File System Checks:** Run file system integrity checks on storage volumes.\n*   **Data Sampling:** Randomly sample stored data and re-run validation checks to detect silent corruption.\n\n### 3. Validation Procedures and Implementation\n\n*   **Automated Checks:** All validation rules (except Stage 4) will be automated as part of the ingestion pipeline.\n*   **Schema Management:** A central repository for JSON schemas will be maintained. Tools like `ajv` (for Node.js) or similar libraries will be used for schema validation.\n*   **Processing Libraries:** Leverage established libraries for parsing and validating 3D file formats (e.g., `gltf-transform`, `three.js` loaders, `assimp`).\n*   **Error Reporting:** Validation failures will be logged with detailed information (file, rule violated, error message). Notifications will be sent for critical failures.\n*   **Quarantine/Rejection:** Assets failing Stage 1 or Stage 2 validation may be rejected outright or moved to a quarantine area for manual review. Assets failing Stage 3 validation will trigger rollback or re-processing attempts.\n*   **Feedback Loop:** Validation results will feed back into the `Improvement Analyst` (T29) and `Refinement Strategist` (T30) for identifying common issues and improving data quality over time.\n*   **Data Validator Role (T19):** This role is responsible for defining, implementing, and maintaining these validation rules and procedures, working closely with `Data Engineer` (T13) and `Ingestion Specialist` (T18)."
    }
  ]
}