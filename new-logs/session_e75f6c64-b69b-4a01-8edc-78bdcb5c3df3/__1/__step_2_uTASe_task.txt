The overall goal is: 'Bootstrap a Bootstrap 5 powered frontend for the Qdrant Service'

Your role's specific goal is: 'Identify and return each 'Task Agnostic Step' (TAS) towards the high-level goal.'
Your specific sub-task is: 'Define the user personas and key user journeys for interacting with the Qdrant Service frontend. Consider different types of users (e.g., developers, researchers) and their primary goals.'

The team's roles are:
    [Role(title='Task-Agnostic Step (TAS) extractor', purpose='Identify and extract granular, actionable steps from high-level goals, ensuring no task-specific knowledge is required for this extraction process. These steps should be general enough to be assigned to various specialized agents.'), Role(title='Prompt Engineer', purpose='Refine system prompts for AI agents based on their roles and the specific tasks they are assigned. This ensures clear communication, effective workflow structuring, and optimal performance of each agent.'), Role(title='Designer', purpose='Generate aesthetic layouts, color palettes, typography, and UI flows for the frontend. Ensure designs are accessible, visually balanced, and align with the overall project requirements.'), Role(title='Engineer', purpose='Implement the designed frontend components into clean, modular, and performant code. Focus on responsive design, accessibility, and seamless integration with the backend API.')]

Please use the following outputs from the other agents as your input:

--- Artifact 'files' from (User) in [initial]:
{
    "path": "main.py",
    "content": "import traceback\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any\nfrom qdrant_client import QdrantClient, models\nimport time\nimport base64\nimport io\nfrom PIL import Image\nimport open_clip\nimport uuid\nimport hashlib\nimport torch\n\n# --- Configuration ---\nQDRANT_HOST = \"localhost\"\nQDRANT_PORT = 6333\nSIMILARITY_THRESHOLD = 0.75 # Example threshold\nIMAGE_COLLECTION_NAME = \"image_metadata\"\n\n# Model-specific configurations, including vector dimensions.\nMODEL_CONFIG = {\n    \"ViT-B-32\": {\"dim\": 512, \"description\": \"ViT-B-32 from OpenAI\", \"pretrained\": \"openai\"},\n    \"RN50\": {\"dim\": 1024, \"description\": \"ResNet-50 from OpenAI\", \"pretrained\": \"openai\"},\n}\n\n# --- Pydantic Models ---\nclass ModelInfo(BaseModel):\n    name: str\n    description: str\n    dim: int\n\nclass TextBenchmarkRequest(BaseModel):\n    query: str\n    model_name: str\n\nclass ImageBenchmarkRequest(BaseModel):\n    image_base64: str\n    model_name: str\n\nclass IndexImageRequest(BaseModel):\n    image_base64: str\n    metadata: Dict[str, Any]\n\nclass SearchResult(BaseModel):\n    image_id: str\n    metadata: Dict[str, Any]\n    score: float\n\nclass BenchmarkResponse(BaseModel):\n    model_name: str\n    results: List[SearchResult]\n    latency_ms: float\n\n# --- FastAPI App Setup ---\napp = FastAPI()\n\n# --- Qdrant Client Initialization ---\nclass QdrantService:\n    def __init__(self, host: str, port: int):\n        #self.client = QdrantClient(host=host, port=port)\n        self.client = QdrantClient(path=\"./qdrant_data\")  # Using local storage for simplicity\n        self._loaded_models = {}\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"Using device: {self.device}\")\n\n\n    def get_available_models(self) -> List[ModelInfo]:\n        # In a real scenario, this could also query a model registry or check available collections.\n        # For now, we use the configured models.\n        model_names = [\n            ModelInfo(name=name, description=config[\"description\"], dim=config[\"dim\"]) for name, config in MODEL_CONFIG.items()\n        ]\n        return model_names\n\n    def _get_model_collection_name(self, model_name: str) -> str:\n        return f\"namespace-{model_name}\"\n\n    def _load_model(self, model_name: str):\n        if model_name not in self._loaded_models:\n            print(f\"Loading model {model_name}...\")\n            pretrained = MODEL_CONFIG[model_name].get(\"pretrained\", \"\")\n            model, _, preprocess = open_clip.create_model_and_transforms(\n                model_name, pretrained=pretrained, device=self.device\n            )\n            tokenizer = open_clip.get_tokenizer(model_name)\n            self._loaded_models[model_name] = (model, preprocess, tokenizer)\n        return self._loaded_models[model_name]\n\n    def _get_embedding(self, text_or_image: Any, model_name: str) -> List[float]:\n        model, preprocess, tokenizer = self._load_model(model_name)\n\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            if isinstance(text_or_image, str):\n                text = tokenizer(text_or_image).to(self.device)\n                embedding = model.encode_text(text)\n            elif isinstance(text_or_image, Image.Image):\n                image = preprocess(text_or_image).unsqueeze(0).to(self.device)\n                embedding = model.encode_image(image)\n            else:\n                raise ValueError(\"Input must be a string or a PIL Image.\")\n\n            embedding /= embedding.norm(dim=-1, keepdim=True)\n\n        return embedding.cpu().numpy().flatten().tolist()\n\n    def search_by_text(self, query: str, model_name: str, limit: int = 10) -> List[SearchResult]:\n        collection_name = self._get_model_collection_name(model_name)\n        try:\n            query_vector = self._get_embedding(query, model_name)\n            \n            search_result = self.client.search(\n                collection_name=collection_name,\n                query_vector=query_vector,\n                limit=limit,\n                query_filter=models.Filter(\n                    must_not=[\n                        models.FieldCondition(key=\"score\", range=models.Range(lt=SIMILARITY_THRESHOLD))\n                    ]\n                )\n            )\n            \n            results = []\n            for hit in search_result:\n                # Fetch metadata from the metadata collection\n                metadata_response = self.client.retrieve(\n                    collection_name=IMAGE_COLLECTION_NAME,\n                    ids=[hit.id],\n                    with_payload=True\n                )\n                if metadata_response:\n                    results.append(SearchResult(\n                        image_id=hit.id,\n                        metadata=metadata_response[0].payload,\n                        score=hit.score\n                    ))\n            return results\n        except Exception as e:\n            print(f\"Error searching text in {model_name}: {e}\")\n            raise HTTPException(status_code=500, detail=f\"Error performing text search for model {model_name}\")\n\n    def search_by_image(self, image_base64: str, model_name: str, limit: int = 10) -> List[SearchResult]:\n        collection_name = self._get_model_collection_name(model_name)\n        try:\n            image_bytes = base64.b64decode(image_base64)\n            image = Image.open(io.BytesIO(image_bytes))\n            \n            query_vector = self._get_embedding(image, model_name)\n\n            search_result = self.client.search(\n                collection_name=collection_name,\n                query_vector=query_vector,\n                limit=limit,\n                query_filter=models.Filter(\n                    must_not=[\n                        models.FieldCondition(key=\"score\", range=models.Range(lt=SIMILARITY_THRESHOLD))\n                    ]\n                )\n            )\n            \n            results = []\n            for hit in search_result:\n                # Fetch metadata from the metadata collection\n                metadata_response = self.client.retrieve(\n                    collection_name=IMAGE_COLLECTION_NAME,\n                    ids=[hit.id],\n                    with_payload=True\n                )\n                if metadata_response:\n                    results.append(SearchResult(\n                        image_id=hit.id,\n                        metadata=metadata_response[0].payload,\n                        score=hit.score\n                    ))\n            return results\n        except Exception as e:\n            print(f\"Error searching image in {model_name}: {e}\")\n            raise HTTPException(status_code=500, detail=f\"Error performing image search for model {model_name}\")\n\n    def index_image(self, image_base64: str, metadata: Dict[str, Any]) -> str:\n        try:\n            image_bytes = base64.b64decode(image_base64)\n            image = Image.open(io.BytesIO(image_bytes))\n            # Generate a deterministic UUID from the image content for the ID.\n            image_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, image_base64))\n\n            # Store metadata first\n            self.client.upsert(\n                collection_name=IMAGE_COLLECTION_NAME,\n                points= [\n                    models.PointStruct(\n                        id=image_id,\n                        payload=metadata,\n                        vector={}\n                    )\n                ]\n            )\n\n            # Index image embeddings in each model's namespace\n            for model_name, config in MODEL_CONFIG.items():\n                collection_name = self._get_model_collection_name(model_name)\n                vector_dim = config[\"dim\"]\n                # Ensure collection exists, create if not (this should ideally be done during setup)\n                if not self.client.collection_exists(collection_name=collection_name):\n                    self.client.create_collection(\n                        collection_name=collection_name,\n                        vectors_config=models.VectorParams(size=vector_dim, distance=models.Distance.COSINE) # Assuming COSINE distance\n                    )\n                \n                query_vector = self._get_embedding(image, model_name)\n                \n                self.client.upsert(\n                    collection_name=collection_name,\n                    points= [\n                        models.PointStruct(\n                            id=image_id,\n                            vector=query_vector,\n                            payload=metadata # Payload can be repeated or just use image_id for lookup in metadata table\n                        )\n                    ]\n                )\n            return image_id\n        except Exception as e:\n            #print(f\"Error indexing image: {e}\")\n            raise #HTTPException(status_code=500, detail=f\"Error indexing image: {e}\")\n\n\n# --- Initialize Services ---\nqdrant_service = QdrantService(host=QDRANT_HOST, port=QDRANT_PORT)\n\n# --- API Endpoints ---\n\n@app.get(\"/models\", response_model=List[ModelInfo])\ndef get_models() -> List[ModelInfo]:\n    \"\"\"Gets a list of available models for benchmarking.\"\"\"\n    return qdrant_service.get_available_models()\n\n@app.post(\"/benchmark/text\", response_model=BenchmarkResponse)\ndef benchmark_text(request: TextBenchmarkRequest) -> BenchmarkResponse:\n    \"\"\"Runs a benchmark search using a text query.\"\"\"\n    start_time = time.time()\n    results = qdrant_service.search_by_text(request.query, request.model_name)\n    end_time = time.time()\n    latency_ms = (end_time - start_time) * 1000\n    return BenchmarkResponse(model_name=request.model_name, results=results, latency_ms=latency_ms)\n\n@app.post(\"/benchmark/image\", response_model=BenchmarkResponse)\ndef benchmark_image(request: ImageBenchmarkRequest) -> BenchmarkResponse:\n    \"\"\"Runs a benchmark search using an image.\"\"\"\n    start_time = time.time()\n    results = qdrant_service.search_by_image(request.image_base64, request.model_name)\n    end_time = time.time()\n    latency_ms = (end_time - start_time) * 1000\n    return BenchmarkResponse(model_name=request.model_name, results=results, latency_ms=latency_ms)\n\n@app.post(\"/index_image\", response_model=Dict[str, str])\ndef index_image(request: IndexImageRequest) -> Dict[str, str]:\n    \"\"\"Adds a new image to the index of each model.\"\"\"\n    image_id = qdrant_service.index_image(request.image_base64, request.metadata)\n    return {\"message\": \"Image indexed successfully\", \"image_id\": image_id}\n\n# --- Helper function to check and initialize collections (run once manually or via setup script) ---\n# This part is illustrative and would typically be handled by a setup script.\ndef initialize_collections():\n    # This function is intended to be run once to set up the necessary collections in Qdrant.\n    # It's not part of the API endpoints but crucial for the backend to function.\n    print(\"Initializing Qdrant collections...\")\n    try:\n        # Ensure metadata collection exists\n        if not qdrant_service.client.collection_exists(collection_name=IMAGE_COLLECTION_NAME):\n            qdrant_service.client.create_collection(\n                collection_name=IMAGE_COLLECTION_NAME,\n                vectors_config=models.VectorParams(size=1, distance=models.Distance.COSINE) # Dummy config for metadata-only\n            )\n            print(f\"Collection '{IMAGE_COLLECTION_NAME}' created.\")\n        else:\n            print(f\"Collection '{IMAGE_COLLECTION_NAME}' already exists.\")\n\n        # Ensure model-specific collections exist (based on potentially available models)\n        # This part is tricky without knowing the models beforehand. A better approach is to create them on demand or via a config.\n        # For demonstration, let's assume a few models might exist.\n        for model_name, config in MODEL_CONFIG.items():\n            collection_name = qdrant_service._get_model_collection_name(model_name)\n            vector_dim = config[\"dim\"]\n            if not qdrant_service.client.collection_exists(collection_name=collection_name):\n                qdrant_service.client.create_collection(\n                    collection_name=collection_name,\n                    vectors_config=models.VectorParams(size=vector_dim, distance=models.Distance.COSINE)\n                )\n                print(f\"Collection '{collection_name}' created for model '{model_name}'.\")\n            else:\n                print(f\"Collection '{collection_name}' already exists.\")\n\n    except Exception as e:\n        traceback.print_exc()\n        print(f\"Error during Qdrant initialization: {e}\")\n\n# Call initialization (e.g., when the server starts, or via a separate setup command)\n# initialize_collections()\n\n\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Qdrant Service CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # Init command\n    init_parser = subparsers.add_parser(\"init\", help=\"Initialize Qdrant collections\")\n\n    # Models command\n    models_parser = subparsers.add_parser(\"models\", help=\"List available models\")\n\n    # Serve command\n    serve_parser = subparsers.add_parser(\"serve\", help=\"Start the FastAPI server\")\n    serve_parser.add_argument(\"--host\", default=\"0.0.0.0\", help=\"Host to run the server on\")\n    serve_parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to run the server on\")\n\n    # Index command\n    index_parser = subparsers.add_parser(\"index\", help=\"Index an image\")\n    index_parser.add_argument(\"--image_path\", required=True, help=\"Path to the image file\")\n    index_parser.add_argument(\"--metadata\", required=True, help=\"JSON string of image metadata (e.g., '{\\\"url\\\": \\\"example.com/img.jpg\\\"}')\")\n\n    # Search text command\n    search_text_parser = subparsers.add_parser(\"search-text\", help=\"Search by text query\")\n    search_text_parser.add_argument(\"--query\", required=True, help=\"Text query string\")\n    search_text_parser.add_argument(\"--model\", required=True, help=\"Model name to use for search (e.g., ViT-B-32)\")\n    search_text_parser.add_argument(\"--limit\", type=int, default=10, help=\"Number of results to return\")\n\n    # Search image command\n    search_image_parser = subparsers.add_parser(\"search-image\", help=\"Search by image file\")\n    search_image_parser.add_argument(\"--image_path\", required=True, help=\"Path to the image file\")\n    search_image_parser.add_argument(\"--model\", required=True, help=\"Model name to use for search (e.g., ViT-B-32)\")\n    search_image_parser.add_argument(\"--limit\", type=int, default=10, help=\"Number of results to return\")\n\n    args = parser.parse_args()\n\n    if args.command == \"init\":\n        print(\"Running Qdrant collection initialization...\")\n        initialize_collections()\n        print(\"Initialization complete.\")\n    elif args.command == \"models\":\n        print(\"Available models:\")\n        models_list = qdrant_service.get_available_models()\n        for model in models_list:\n            print(f\"- {model.name}: {model.description} (dim: {model.dim})\")\n    elif args.command == \"index\":\n        try:\n            with open(args.image_path, \"rb\") as f:\n                image_bytes = f.read()\n            image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n            import json\n            metadata = json.loads(args.metadata)\n            image_id = qdrant_service.index_image(image_base64, metadata)\n            print(f\"Image indexed successfully with ID: {image_id}\")\n        except Exception as e:\n            #print(f\"Error indexing image: {e}\")\n            raise\n    elif args.command == \"search-text\":\n        results = qdrant_service.search_by_text(args.query, args.model, args.limit)\n        print(f\"Search results: {results}\")\n    elif args.command == \"search-image\":\n        try:\n            with open(args.image_path, \"rb\") as f:\n                image_bytes = f.read()\n            image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n            results = qdrant_service.search_by_image(image_base64, args.model, args.limit)\n            print(f\"Search results: {results}\")\n        except Exception as e:\n            print(f\"Error searching image: {e}\")\n    elif args.command == \"serve\":\n        import uvicorn\n        # To run this:\n        # 1. Ensure Qdrant is running on localhost:6333\n        # 2. Install dependencies: pip install fastapi uvicorn pydantic qdrant-client open-clip-torch Pillow\n        # 3. Run this script: python backend/main.py serve\n        # 4. Access API at http://127.0.0.1:8000\n        print(f\"Starting FastAPI server on {args.host}:{args.port}...\")\n        uvicorn.run(app, host=args.host, port=args.port)\n    else:\n        parser.print_help()\n\n\nif __name__ == \"__main__\":\n    main()\n"
}



Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.