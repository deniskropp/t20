We are a meta-artificial intelligence team collaboratively developing an iterative plan to achieve the high-level goal of final synthesis.

High-Level Goal: Bootstrap 5 powered frontend for the Qdrant Service

Team Members:
- Name: 'GPTASe'
  Role: `Task-Agnostic Step (TAS) extractor`
  Goal: "Identify and extract each 'Task Agnostic Step' (TAS) towards the high-level goal."
- Name: 'Lyra'
  Role: `Prompt Engineer`
  Goal: "Structure workflows and ensure clarity in agent instructions, system prompt engineering"
- Name: 'TASe'
  Role: `Task-Agnostic Step (TAS) extractor`
  Goal: "Identify and extract each 'Task Agnostic Step' (TAS) towards the high-level goal."
- Name: 'uTASe'
  Role: `Task-Agnostic Step (TAS) extractor`
  Goal: "Identify and return each 'Task Agnostic Step' (TAS) towards the high-level goal."
- Name: 'Aurora'
  Role: `Designer`
  Goal: "Generate aesthetic layouts, color palettes, typography, and UI flows, ensuring accessibility and visual balance."
- Name: 'Kodax'
  Role: `Engineer`
  Goal: "Implement designs into clean, modular, and performant code, focusing on responsive design and accessibility."

Leverage each team member, guided by their goals, to maximize collaboration.
Use prompt engineering to refine the system prompts for each agent based on their roles and tasks.


--- File: 'main.py'
```
import traceback
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
from qdrant_client import QdrantClient, models
import time
import base64
import io
from PIL import Image
import open_clip
import uuid
import hashlib
import torch

# --- Configuration ---
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
SIMILARITY_THRESHOLD = 0.75 # Example threshold
IMAGE_COLLECTION_NAME = "image_metadata"

# Model-specific configurations, including vector dimensions.
MODEL_CONFIG = {
    "ViT-B-32": {"dim": 512, "description": "ViT-B-32 from OpenAI", "pretrained": "openai"},
    "RN50": {"dim": 1024, "description": "ResNet-50 from OpenAI", "pretrained": "openai"},
}

# --- Pydantic Models ---
class ModelInfo(BaseModel):
    name: str
    description: str
    dim: int

class TextBenchmarkRequest(BaseModel):
    query: str
    model_name: str

class ImageBenchmarkRequest(BaseModel):
    image_base64: str
    model_name: str

class IndexImageRequest(BaseModel):
    image_base64: str
    metadata: Dict[str, Any]

class SearchResult(BaseModel):
    image_id: str
    metadata: Dict[str, Any]
    score: float

class BenchmarkResponse(BaseModel):
    model_name: str
    results: List[SearchResult]
    latency_ms: float

# --- FastAPI App Setup ---
app = FastAPI()

# --- Qdrant Client Initialization ---
class QdrantService:
    def __init__(self, host: str, port: int):
        #self.client = QdrantClient(host=host, port=port)
        self.client = QdrantClient(path="./qdrant_data")  # Using local storage for simplicity
        self._loaded_models = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")


    def get_available_models(self) -> List[ModelInfo]:
        # In a real scenario, this could also query a model registry or check available collections.
        # For now, we use the configured models.
        model_names = [
            ModelInfo(name=name, description=config["description"], dim=config["dim"]) for name, config in MODEL_CONFIG.items()
        ]
        return model_names

    def _get_model_collection_name(self, model_name: str) -> str:
        return f"namespace-{model_name}"

    def _load_model(self, model_name: str):
        if model_name not in self._loaded_models:
            print(f"Loading model {model_name}...")
            pretrained = MODEL_CONFIG[model_name].get("pretrained", "")
            model, _, preprocess = open_clip.create_model_and_transforms(
                model_name, pretrained=pretrained, device=self.device
            )
            tokenizer = open_clip.get_tokenizer(model_name)
            self._loaded_models[model_name] = (model, preprocess, tokenizer)
        return self._loaded_models[model_name]

    def _get_embedding(self, text_or_image: Any, model_name: str) -> List[float]:
        model, preprocess, tokenizer = self._load_model(model_name)

        with torch.no_grad(), torch.cuda.amp.autocast():
            if isinstance(text_or_image, str):
                text = tokenizer(text_or_image).to(self.device)
                embedding = model.encode_text(text)
            elif isinstance(text_or_image, Image.Image):
                image = preprocess(text_or_image).unsqueeze(0).to(self.device)
                embedding = model.encode_image(image)
            else:
                raise ValueError("Input must be a string or a PIL Image.")

            embedding /= embedding.norm(dim=-1, keepdim=True)

        return embedding.cpu().numpy().flatten().tolist()

    def search_by_text(self, query: str, model_name: str, limit: int = 10) -> List[SearchResult]:
        collection_name = self._get_model_collection_name(model_name)
        try:
            query_vector = self._get_embedding(query, model_name)
            
            search_result = self.client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=limit,
                query_filter=models.Filter(
                    must_not=[
                        models.FieldCondition(key="score", range=models.Range(lt=SIMILARITY_THRESHOLD))
                    ]
                )
            )
            
            results = []
            for hit in search_result:
                # Fetch metadata from the metadata collection
                metadata_response = self.client.retrieve(
                    collection_name=IMAGE_COLLECTION_NAME,
                    ids=[hit.id],
                    with_payload=True
                )
                if metadata_response:
                    results.append(SearchResult(
                        image_id=hit.id,
                        metadata=metadata_response[0].payload,
                        score=hit.score
                    ))
            return results
        except Exception as e:
            print(f"Error searching text in {model_name}: {e}")
            raise HTTPException(status_code=500, detail=f"Error performing text search for model {model_name}")

    def search_by_image(self, image_base64: str, model_name: str, limit: int = 10) -> List[SearchResult]:
        collection_name = self._get_model_collection_name(model_name)
        try:
            image_bytes = base64.b64decode(image_base64)
            image = Image.open(io.BytesIO(image_bytes))
            
            query_vector = self._get_embedding(image, model_name)

            search_result = self.client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=limit,
                query_filter=models.Filter(
                    must_not=[
                        models.FieldCondition(key="score", range=models.Range(lt=SIMILARITY_THRESHOLD))
                    ]
                )
            )
            
            results = []
            for hit in search_result:
                # Fetch metadata from the metadata collection
                metadata_response = self.client.retrieve(
                    collection_name=IMAGE_COLLECTION_NAME,
                    ids=[hit.id],
                    with_payload=True
                )
                if metadata_response:
                    results.append(SearchResult(
                        image_id=hit.id,
                        metadata=metadata_response[0].payload,
                        score=hit.score
                    ))
            return results
        except Exception as e:
            print(f"Error searching image in {model_name}: {e}")
            raise HTTPException(status_code=500, detail=f"Error performing image search for model {model_name}")

    def index_image(self, image_base64: str, metadata: Dict[str, Any]) -> str:
        try:
            image_bytes = base64.b64decode(image_base64)
            image = Image.open(io.BytesIO(image_bytes))
            # Generate a deterministic UUID from the image content for the ID.
            image_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, image_base64))

            # Store metadata first
            self.client.upsert(
                collection_name=IMAGE_COLLECTION_NAME,
                points= [
                    models.PointStruct(
                        id=image_id,
                        payload=metadata,
                        vector={}
                    )
                ]
            )

            # Index image embeddings in each model's namespace
            for model_name, config in MODEL_CONFIG.items():
                collection_name = self._get_model_collection_name(model_name)
                vector_dim = config["dim"]
                # Ensure collection exists, create if not (this should ideally be done during setup)
                if not self.client.collection_exists(collection_name=collection_name):
                    self.client.create_collection(
                        collection_name=collection_name,
                        vectors_config=models.VectorParams(size=vector_dim, distance=models.Distance.COSINE) # Assuming COSINE distance
                    )
                
                query_vector = self._get_embedding(image, model_name)
                
                self.client.upsert(
                    collection_name=collection_name,
                    points= [
                        models.PointStruct(
                            id=image_id,
                            vector=query_vector,
                            payload=metadata # Payload can be repeated or just use image_id for lookup in metadata table
                        )
                    ]
                )
            return image_id
        except Exception as e:
            #print(f"Error indexing image: {e}")
            raise #HTTPException(status_code=500, detail=f"Error indexing image: {e}")


# --- Initialize Services ---
qdrant_service = QdrantService(host=QDRANT_HOST, port=QDRANT_PORT)

# --- API Endpoints ---

@app.get("/models", response_model=List[ModelInfo])
def get_models() -> List[ModelInfo]:
    """Gets a list of available models for benchmarking."""
    return qdrant_service.get_available_models()

@app.post("/benchmark/text", response_model=BenchmarkResponse)
def benchmark_text(request: TextBenchmarkRequest) -> BenchmarkResponse:
    """Runs a benchmark search using a text query."""
    start_time = time.time()
    results = qdrant_service.search_by_text(request.query, request.model_name)
    end_time = time.time()
    latency_ms = (end_time - start_time) * 1000
    return BenchmarkResponse(model_name=request.model_name, results=results, latency_ms=latency_ms)

@app.post("/benchmark/image", response_model=BenchmarkResponse)
def benchmark_image(request: ImageBenchmarkRequest) -> BenchmarkResponse:
    """Runs a benchmark search using an image."""
    start_time = time.time()
    results = qdrant_service.search_by_image(request.image_base64, request.model_name)
    end_time = time.time()
    latency_ms = (end_time - start_time) * 1000
    return BenchmarkResponse(model_name=request.model_name, results=results, latency_ms=latency_ms)

@app.post("/index_image", response_model=Dict[str, str])
def index_image(request: IndexImageRequest) -> Dict[str, str]:
    """Adds a new image to the index of each model."""
    image_id = qdrant_service.index_image(request.image_base64, request.metadata)
    return {"message": "Image indexed successfully", "image_id": image_id}

# --- Helper function to check and initialize collections (run once manually or via setup script) ---
# This part is illustrative and would typically be handled by a setup script.
def initialize_collections():
    # This function is intended to be run once to set up the necessary collections in Qdrant.
    # It's not part of the API endpoints but crucial for the backend to function.
    print("Initializing Qdrant collections...")
    try:
        # Ensure metadata collection exists
        if not qdrant_service.client.collection_exists(collection_name=IMAGE_COLLECTION_NAME):
            qdrant_service.client.create_collection(
                collection_name=IMAGE_COLLECTION_NAME,
                vectors_config=models.VectorParams(size=1, distance=models.Distance.COSINE) # Dummy config for metadata-only
            )
            print(f"Collection '{IMAGE_COLLECTION_NAME}' created.")
        else:
            print(f"Collection '{IMAGE_COLLECTION_NAME}' already exists.")

        # Ensure model-specific collections exist (based on potentially available models)
        # This part is tricky without knowing the models beforehand. A better approach is to create them on demand or via a config.
        # For demonstration, let's assume a few models might exist.
        for model_name, config in MODEL_CONFIG.items():
            collection_name = qdrant_service._get_model_collection_name(model_name)
            vector_dim = config["dim"]
            if not qdrant_service.client.collection_exists(collection_name=collection_name):
                qdrant_service.client.create_collection(
                    collection_name=collection_name,
                    vectors_config=models.VectorParams(size=vector_dim, distance=models.Distance.COSINE)
                )
                print(f"Collection '{collection_name}' created for model '{model_name}'.")
            else:
                print(f"Collection '{collection_name}' already exists.")

    except Exception as e:
        traceback.print_exc()
        print(f"Error during Qdrant initialization: {e}")

# Call initialization (e.g., when the server starts, or via a separate setup command)
# initialize_collections()


import argparse

def main():
    parser = argparse.ArgumentParser(description="Qdrant Service CLI")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Init command
    init_parser = subparsers.add_parser("init", help="Initialize Qdrant collections")

    # Models command
    models_parser = subparsers.add_parser("models", help="List available models")

    # Serve command
    serve_parser = subparsers.add_parser("serve", help="Start the FastAPI server")
    serve_parser.add_argument("--host", default="0.0.0.0", help="Host to run the server on")
    serve_parser.add_argument("--port", type=int, default=8000, help="Port to run the server on")

    # Index command
    index_parser = subparsers.add_parser("index", help="Index an image")
    index_parser.add_argument("--image_path", required=True, help="Path to the image file")
    index_parser.add_argument("--metadata", required=True, help="JSON string of image metadata (e.g., '{\"url\": \"example.com/img.jpg\"}')")

    # Search text command
    search_text_parser = subparsers.add_parser("search-text", help="Search by text query")
    search_text_parser.add_argument("--query", required=True, help="Text query string")
    search_text_parser.add_argument("--model", required=True, help="Model name to use for search (e.g., ViT-B-32)")
    search_text_parser.add_argument("--limit", type=int, default=10, help="Number of results to return")

    # Search image command
    search_image_parser = subparsers.add_parser("search-image", help="Search by image file")
    search_image_parser.add_argument("--image_path", required=True, help="Path to the image file")
    search_image_parser.add_argument("--model", required=True, help="Model name to use for search (e.g., ViT-B-32)")
    search_image_parser.add_argument("--limit", type=int, default=10, help="Number of results to return")

    args = parser.parse_args()

    if args.command == "init":
        print("Running Qdrant collection initialization...")
        initialize_collections()
        print("Initialization complete.")
    elif args.command == "models":
        print("Available models:")
        models_list = qdrant_service.get_available_models()
        for model in models_list:
            print(f"- {model.name}: {model.description} (dim: {model.dim})")
    elif args.command == "index":
        try:
            with open(args.image_path, "rb") as f:
                image_bytes = f.read()
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            import json
            metadata = json.loads(args.metadata)
            image_id = qdrant_service.index_image(image_base64, metadata)
            print(f"Image indexed successfully with ID: {image_id}")
        except Exception as e:
            #print(f"Error indexing image: {e}")
            raise
    elif args.command == "search-text":
        results = qdrant_service.search_by_text(args.query, args.model, args.limit)
        print(f"Search results: {results}")
    elif args.command == "search-image":
        try:
            with open(args.image_path, "rb") as f:
                image_bytes = f.read()
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            results = qdrant_service.search_by_image(image_base64, args.model, args.limit)
            print(f"Search results: {results}")
        except Exception as e:
            print(f"Error searching image: {e}")
    elif args.command == "serve":
        import uvicorn
        # To run this:
        # 1. Ensure Qdrant is running on localhost:6333
        # 2. Install dependencies: pip install fastapi uvicorn pydantic qdrant-client open-clip-torch Pillow
        # 3. Run this script: python backend/main.py serve
        # 4. Access API at http://127.0.0.1:8000
        print(f"Starting FastAPI server on {args.host}:{args.port}...")
        uvicorn.run(app, host=args.host, port=args.port)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

```