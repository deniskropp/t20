# Runtime Documentation (TypeScript Node.js)

## Overview

This document provides comprehensive documentation for the TypeScript Node.js runtime system, detailing its components, core concepts, API, and usage. The design emphasizes clarity, maintainability, and adherence to State-Of-The-Art (SOTA) coding principles for Node.js and TypeScript.

## Table of Contents

*   [Introduction](#introduction)
*   [Getting Started](#getting-started)
*   [Core Concepts](#core-concepts)
*   [API Reference](#api-reference)
*   [Advanced Usage & Tutorials](#advanced-usage--tutorials)
*   [Contribution Guide](#contribution-guide)
*   [Troubleshooting & FAQ](#troubleshooting--faq)

## Introduction

### Overview of Runtime Components

The runtime system is designed to facilitate the execution of multi-agent workflows. Key components include:

*   **`Agent`**: The base class for all agents, defining their lifecycle, task execution logic, and interaction patterns. Agents are the fundamental units that perform tasks within the system. Subclasses like `Orchestrator` provide specialized functionality.
*   **`Session`**: Manages the overall state and artifacts of a workflow execution. It handles session IDs, artifact storage, and provides an interface for agents to interact with the system's state.
*   **`ExecutionContext`**: Holds the context for a specific step or round within a workflow. It includes the plan, current step, and a collection of recorded artifacts from previous steps.
*   **`Orchestrator`**: A specialized agent responsible for generating and managing the workflow plan. It interprets the high-level goal and breaks it down into a series of tasks for other agents.
*   **`LLM`**: An abstract base class that abstracts LLM interactions. It provides a unified interface for various LLM providers (e.g., Gemini, Ollama, OpenAI, Mistral) and models, supporting a factory pattern for easy integration and interchangeability.
*   **`Logger`**: A singleton class for centralized logging, supporting console and file output with customizable levels and formatting (including colored console output and JSONL).
*   **`custom_types`**: Defines the Pydantic models used for structured data exchange, including `Plan`, `Task`, `File`, `Artifact`, `Prompt`, `Team`, and `AgentOutput`.

## Getting Started

### Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-name>
    ```

2.  **Install dependencies:**
    ```bash
    npm install
    ```

3.  **Build the TypeScript project:**
    ```bash
    npm run build
    ```

### Basic Usage

To run a workflow, use the `main.ts` entry point. You need to provide an initial task description (which serves as the high-level goal) and optionally specify the number of rounds and input files.

```bash
node dist/src/index.js "Your initial task description" -r 3 -f file1.txt file2.py
```

*   `"Your initial task description"`: The high-level goal for the workflow.
*   `-r 3`: Specifies that the workflow should run for 3 rounds.
*   `-f file1.txt file2.py`: Lists input files to be used during the task execution.

## Core Concepts

### Agents

Agents are the core actors in the system. Each agent has a name, role, goal, and can be configured with a specific LLM model and system prompt. They execute tasks based on the provided context and plan. Agents can delegate tasks to sub-teams managed by the `Orchestrator` or other specialized agents.

### Plans and Tasks

A `Plan` is a structured definition of a workflow, comprising a sequence of `Task` objects. Each `Task` has an ID, a description, a responsible role, an assigned agent, and a list of prerequisite task IDs.

### Artifacts

Artifacts are outputs generated by agents during task execution. They can be stored and retrieved using the `Session` and `ExecutionContext` objects. The `ExecutionContext.recordArtifact` method is used to save artifacts for the current step, while `ExecutionContext.recordInitial` is used for initial inputs. Artifacts are stored within the session directory.

### `AgentOutput` Model

The `AgentOutput` Pydantic model defines the structured response format for agents. It includes fields for the main `output` message, an optional `artifact` (containing files), and optional `team` updates (like new system prompts). This model ensures consistent communication between agents.

### LLM Integration

The `LLM` class and its factory pattern allow for easy instantiation and usage of different LLM providers (Gemini, Ollama, OpenAI, Mistral) based on configuration. The `generateContent` method provides a unified interface for interacting with these models.

## API Reference

### `Agent` Class (`src/agent/Agent.ts`)

*   **`constructor(name: string, role: string, goal: string, model: string, systemPrompt: string)`**: Initializes an agent instance.
*   **`updateSystemPrompt(newPrompt: string): void`**: Updates the agent's system prompt.
*   **`async executeTask(context: ExecutionContext): Promise<string | null>`**: Executes a task using the LLM based on the provided context. It prepares the prompt, including previous artifacts, and calls the LLM. Returns the agent's output as a JSON string or null on error.

### `Orchestrator` Class (`src/orchestrator/Orchestrator.ts`)

*   **`constructor(...)`**: Extends `Agent` constructor.
*   **`async generatePlan(session: Session, highLevelGoal: string, files: File[]): Promise<Plan | null>`**: Generates a structured plan for the workflow.
*   **`async executeTask(context: ExecutionContext): Promise<string | null>`**: Overrides base method to handle plan generation tasks.

### `Session` Class (`src/core/Session.ts`)

*   **`constructor(agents: Agent[], projectRoot: string)`**: Initializes a session with agents and project root.
*   **`async addArtifact(name: string, content: any): Promise<void>`**: Saves an artifact in the session directory.
*   **`async getArtifact(name: string): Promise<any | null>`**: Loads an artifact from the session directory.

### `ExecutionContext` Class (`src/core/ExecutionContext.ts`)

*   **`constructor(session: Session, plan: Plan, stepIndex?: number, roundNum?: number)`**: Initializes the execution context.
*   **`nextStep(): void`**: Advances the step index.
*   **`reset(): void`**: Resets the step index to 0.
*   **`currentStep(): Task`**: Returns the current `Task` object.
*   **`async recordArtifact(key: string, value: any, remember?: boolean): Promise<void>`**: Records an artifact for the current step and optionally remembers it in context memory.
*   **`recordInitial(key: string, value: any): void`**: Records initial artifacts provided at the start of the workflow.
*   **`getInitialFiles(): File[] | null`**: Retrieves initial files.

### `LLM` Abstract Class (`src/llm/LLM.ts`)

*   **`static factory(species: string): LLM`**: Factory method to create LLM instances (e.g., `Gemini`, `Olli`, `Opi`, `Mistral`).
*   **`abstract async generateContent(...)`**: Abstract method for content generation.

### `Logger` Class (`src/logger/Logger.ts`)

*   **`static getInstance(): Logger`**: Gets the singleton logger instance.
*   **`static setup(level: string): void`**: Configures logging level and handlers.
*   **`info(message: string): void`**, **`warn(message: string): void`**, **`error(message: string): void`**, **`debug(message: string): void`**: Logging methods.

### `custom_types` Module (`src/custom_types.ts`)

*   **`Task`**: Represents a single actionable step in a plan.
    *   `id`: Unique ID for the task.
    *   `description`: What the task is and how to execute it.
    *   `role`: Role responsible for performing this task.
    *   `agent`: Specific agent assigned to this task.
    *   `requires`: List of prerequisite task IDs.
*   **`File`**: Represents a generated file.
    *   `path`: File path or name.
    *   `content`: Full content of the file.
*   **`Artifact`**: Output artifact generated by a task.
    *   `task`: ID of the task that produced this artifact.
    *   `files`: List of `File` objects included in the artifact.
*   **`Role`**: Definition of a role in the team.
    *   `title`: Role title.
    *   `purpose`: High-level responsibilities and objectives.
*   **`Plan`**: Overall strategy with reasoning, roles, and tasks.
    *   `highLevelGoal`: The overarching objective.
    *   `reasoning`: Why this plan is structured this way.
    *   `roles`: All roles needed for execution.
    *   `tasks`: Step-by-step ordered tasks.
    *   `team`: Optional `Team` updates.
*   **`Prompt`**: System prompt assigned to an agent.
    *   `agent`: Name of the agent this prompt is for.
    *   `role`: Role context of the agent.
    *   `systemPrompt`: Full text of the system prompt.
*   **`Team`**: Metadata and coordination for the team.
    *   `notes`: General notes, reflections, or feedback.
    *   `prompts`: Updated system prompts for agents.
*   **`AgentOutput`**: Structured output returned by an agent.
    *   `output`: Message from the agent.
    *   `artifact`: Optional `Artifact` generated by the agent.
    *   `team`: Optional `Team` updates.
    *   `reasoning`: Explanation of how the agent arrived at this output.

## Advanced Usage & Tutorials

### Advanced Planning

The planning process is primarily handled by the `Orchestrator` agent. To influence plan generation, you can:

1.  **Refine the High-Level Goal:** A more specific and well-defined goal leads to a more effective plan.
2.  **Engineer Prompts:** Modifying the system prompt for the `Orchestrator` (e.g., by adjusting files in the `prompts/` directory) can guide its reasoning and task decomposition. This allows for customization of how the orchestrator approaches planning, including task dependencies and role assignments.
3.  **Provide Initial Files:** Including relevant context files during system startup can help the orchestrator create more informed plans.

### LLM Configuration

LLM providers and models are configured via `runtime.yaml`. The `LLM.factory` method uses the model identifier (e.g., `gemini-2.5-flash-latest`, `ollama:llama3`, `openai:gpt-4`) to instantiate the correct LLM client. Ensure necessary API keys and environment variables (e.g., `OPENAI_API_KEY`, `MISTRAL_API_KEY`) are set correctly.

## Contribution Guide

### Running Tests

To run the test suite, navigate to the project's root directory and execute:

```bash
npm test
```

This command will discover and run all tests defined within the project.

## Troubleshooting & FAQ

### How do I configure different LLM providers?

LLM providers are configured within the `config/runtime.yaml` file. The `LLM.factory` method uses the model name to determine which provider to use (e.g., `gemini-`, `ollama:`, `openai:`, `mistral:` prefixes). Ensure that any necessary API keys or environment variables are set correctly before running the system.

### What is the purpose of `runtime.md`?

This `runtime.md` file serves as the primary documentation for the runtime system. It's intended to be a user-friendly guide explaining the system's architecture, core concepts, API, and how to use and contribute to it. It is styled using a minimalist design aesthetic with a clean layout for optimal readability.
