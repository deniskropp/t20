The overall goal is: 'Reimplement runtime using TypeScript in node.js following SOTA coding principles'

Your role's specific goal is: 'Structure workflows and ensure clarity in agent instructions, system prompt engineering'
Your specific sub-task is: 'Begin implementing the core components of the TypeScript runtime based on the defined architecture (T3), TAS (T2), and SOTA practices (T5). Start with foundational elements like Agent, Session, and ExecutionContext.'

The team's roles are:
    {"high_level_goal":"Reimplement runtime using TypeScript in node.js following SOTA coding principles","reasoning":"The user wants to reimplement the existing Python runtime in TypeScript for Node.js. This plan breaks down the process into logical steps, starting with understanding the current Python runtime, defining the TypeScript architecture, setting up the development environment, and then iteratively implementing and testing the core components. The roles are assigned to leverage their specific strengths: GPTASe for task extraction, Lyra for prompt engineering and workflow structuring, Fizz La Metta for coordination, and aitutor for knowledge transfer and guidance.","roles":[{"title":"GPTASe","purpose":"Identify and extract each 'Task Agnostic Step' (TAS) from the provided Python runtime documentation and code to understand the core functionalities and structure."},{"title":"Lyra","purpose":"Structure the overall TypeScript project, define the architecture, and engineer the system prompts for the new TypeScript agents based on the extracted TAS."},{"title":"Fizz La Metta","purpose":"Coordinate the iterative development process, ensuring smooth transitions between development phases and managing meta-level communication."},{"title":"aitutor","purpose":"Provide guidance on SOTA TypeScript/Node.js practices, assist with understanding complex concepts from the Python runtime, and facilitate knowledge transfer to the TypeScript environment."}],"tasks":[{"id":"T1","description":"Analyze the provided Python runtime documentation ('runtime.md') and source code to identify core components, functionalities, and design patterns.","role":"GPTASe","agent":"GPTASe","requires":[]},{"id":"T2","description":"Extract Task Agnostic Steps (TAS) from the analysis in T1. These steps should represent the fundamental operations and logic of the runtime, independent of the implementation language.","role":"GPTASe","agent":"GPTASe","requires":["T1"]},{"id":"T3","description":"Based on the extracted TAS (T2), define the high-level architecture for the TypeScript Node.js runtime. This includes identifying key modules, classes, and their interactions.","role":"Lyra","agent":"Lyra","requires":["T2"]},{"id":"T4","description":"Set up the initial TypeScript Node.js project structure, including necessary dependencies, build tools (e.g., tsc, esbuild), and linters/formatters.","role":"Lyra","agent":"Lyra","requires":["T3"]},{"id":"T5","description":"For each identified TAS (T2), research and document the equivalent SOTA TypeScript/Node.js implementation patterns and best practices. Consult aitutor for guidance.","role":"aitutor","agent":"aitutor","requires":["T2"]},{"id":"T6","description":"Begin implementing the core components of the TypeScript runtime based on the defined architecture (T3), TAS (T2), and SOTA practices (T5). Start with foundational elements like Agent, Session, and ExecutionContext.","role":"Lyra","agent":"Lyra","requires":["T3","T4","T5"]},{"id":"T7","description":"Develop the LLM integration layer in TypeScript, ensuring it can interface with various LLM providers similarly to the Python version.","role":"Lyra","agent":"Lyra","requires":["T6"]},{"id":"T8","description":"Implement the Orchestrator agent in TypeScript, including its plan generation logic, leveraging the LLM integration (T7).","role":"Lyra","agent":"Lyra","requires":["T7"]},{"id":"T9","description":"Implement the core Agent class and its execution logic in TypeScript, ensuring it can handle tasks, contexts, and artifacts.","role":"Lyra","agent":"Lyra","requires":["T6","T7"]},{"id":"T10","description":"Define and implement the custom types (Plan, Task, Artifact, etc.) in TypeScript using interfaces or classes, ensuring type safety.","role":"Lyra","agent":"Lyra","requires":["T6"]},{"id":"T11","description":"Integrate the implemented components (T8, T9, T10) into a cohesive system. Set up the main entry point (similar to runtime.sysmain.py) for the Node.js application.","role":"Lyra","agent":"Lyra","requires":["T8","T9","T10"]},{"id":"T12","description":"Write unit and integration tests for the core components of the TypeScript runtime.","role":"Lyra","agent":"Lyra","requires":["T11"]},{"id":"T13","description":"Refactor and optimize the TypeScript code based on test results (T12) and SOTA principles. Ensure code quality, maintainability, and performance.","role":"Lyra","agent":"Lyra","requires":["T12"]},{"id":"T14","description":"Coordinate the iterative development process. Review the progress, identify any blockers, and facilitate communication between team members. Ensure alignment with the high-level goal.","role":"Fizz La Metta","agent":"Fizz La Metta","requires":["T1","T2","T3","T4","T5","T6","T7","T8","T9","T10","T11","T12","T13"]},{"id":"T15","description":"Document the new TypeScript runtime, including architecture, API, and usage instructions, analogous to the provided 'runtime.md'.","role":"Lyra","agent":"Lyra","requires":["T13"]},{"id":"T16","description":"Final review of the reimplemented runtime, tests, and documentation. Ensure all requirements of the high-level goal are met.","role":"Fizz La Metta","agent":"Fizz La Metta","requires":["T14","T15"]}],"team":{"notes":"This plan outlines the steps to reimplement the Python runtime in TypeScript. The roles are defined to leverage each member's expertise. GPTASe will focus on understanding the existing system, Lyra on the new implementation and architecture, Fizz La Metta on coordination, and aitutor on knowledge transfer and best practices. The process is iterative, starting with analysis and moving through implementation, testing, and documentation.","prompts":[{"agent":"GPTASe","role":"Task-Agnostic Step (TAS) extractor","system_prompt":"You are GPTASe, a specialized AI focused on analyzing code and documentation to extract fundamental, language-agnostic steps (Task Agnostic Steps - TAS). Your goal is to break down complex systems into their core operational logic. Analyze the provided Python runtime documentation and code. Identify and list all distinct, essential steps or operations performed by the runtime. For each step, describe it clearly and concisely, focusing on *what* needs to be done rather than *how* it's done in Python. Avoid implementation details specific to Python. Your output should be a structured list of these TAS, ready for translation into another programming language."},{"agent":"Lyra","role":"Prompt Engineer","system_prompt":"You are Lyra, an expert Prompt Engineer and architect. Your task is to design and structure the TypeScript Node.js runtime based on the Task Agnostic Steps (TAS) provided by GPTASe. Define the overall project architecture, including modules, classes, interfaces, and their interactions. Ensure the design adheres to State-Of-The-Art (SOTA) coding principles for TypeScript and Node.js. You will also be responsible for setting up the project structure, dependencies, and initial implementation of core components. Engineer clear and effective system prompts for the new TypeScript agents, guiding their behavior and ensuring alignment with the project goals. Focus on creating a robust, maintainable, and scalable codebase."},{"agent":"Fizz La Metta","role":"Coordinator","system_prompt":"You are Fizz La Metta, the Coordinator. Your role is to oversee the meta-temporal coordination of layered cognition processing for this project. You manage the iterative development cycle, ensuring smooth transitions between tasks and agents. Monitor the progress, identify potential bottlenecks or conflicts, and facilitate communication. Your focus is on the overall flow and coherence of the project, ensuring that all team members are aligned with the high-level goal and that the plan is executed efficiently. Provide high-level feedback and guidance to maintain project momentum."},{"agent":"aitutor","role":"AI Tutor","system_prompt":"You are aitutor, an AI Tutor specializing in knowledge transfer. Your role is to assist the team by providing expertise on State-Of-The-Art (SOTA) TypeScript and Node.js development practices. When requested, explain complex concepts from the original Python runtime and how they can be best implemented in TypeScript. Offer guidance on design patterns, best practices, and potential pitfalls. Facilitate the understanding and adoption of new technologies and methodologies within the team. Your goal is to ensure the team has the necessary knowledge to build a high-quality TypeScript runtime."}]}}

Please use the following outputs from the other agents as your input:

--- Artifact 'files' from (User) in [initial]:
{"task":"initial","files":[{"path":"runtime.md","content":"# Runtime Documentation\n\n## Overview\n\nThis document provides comprehensive documentation for the runtime system, detailing its components, core concepts, API, and usage. The design emphasizes clarity, accessibility, and a clean, minimalist aesthetic, ensuring an informative and visually appealing user experience.\n\n## Table of Contents\n\n*   [Introduction](#introduction)\n*   [Getting Started](#getting-started)\n*   [Core Concepts](#core-concepts)\n*   [API Reference](#api-reference)\n*   [Advanced Usage & Tutorials](#advanced-usage--tutorials)\n*   [Contribution Guide](#contribution-guide)\n*   [Troubleshooting & FAQ](#troubleshooting--faq)\n\n## Introduction\n\n### Overview of Runtime Components\n\nThe runtime system is designed to facilitate the execution of multi-agent workflows. Key components include:\n\n*   **`runtime.agent.Agent`**: The base class for all agents, defining their lifecycle, task execution logic, and interaction patterns. Agents are the fundamental units that perform tasks within the system.\n*   **`runtime.core.Session`**: Manages the overall state and artifacts of a workflow execution. It handles session IDs, artifact storage, and provides an interface for agents to interact with the system's state.\n*   **`runtime.core.ExecutionContext`**: Holds the context for a specific step or round within a workflow. It includes the plan, current step, and a collection of recorded artifacts from previous steps.\n*   **`runtime.orchestrator.Orchestrator`**: A specialized agent responsible for generating and managing the workflow plan. It interprets the high-level goal and breaks it down into a series of tasks for other agents.\n*   **`runtime.llm.LLM`**: An abstract base class that abstracts LLM interactions. It provides a unified interface for various LLM providers and models, supporting a factory pattern for easy integration and interchangeability.\n*   **`runtime.custom_types`**: Defines the Pydantic models used for structured data exchange, including `Plan`, `Task`, `File`, `Artifact`, `Prompt`, and `Team`.\n\n## Getting Started\n\n### Installation\n\nTo install the runtime, navigate to the project's root directory and run:\n\n```bash\npip install .\n```\n\n### Basic Usage\n\nTo run a workflow, use the `runtime.sysmain` module. You need to provide an initial task description (which serves as the high-level goal) and optionally specify the number of rounds and input files.\n\n```bash\npython -m runtime.sysmain \"Your initial task description\" -r 3 -f file1.txt file2.py\n```\n\n*   `\"Your initial task description\"`: The high-level goal for the workflow.\n*   `-r 3`: Specifies that the workflow should run for 3 rounds.\n*   `-f file1.txt file2.py`: Lists input files to be used during the task execution.\n\n## Core Concepts\n\n### Agents\n\nAgents are the core actors in the system. Each agent has a name, role, goal, and can be configured with a specific LLM model and system prompt. They execute tasks based on the provided context and plan.\n\n### Plans and Tasks\n\nA `Plan` is a structured definition of a workflow, comprising a sequence of `Task` objects. Each `Task` has an ID, a description, a responsible role, an assigned agent, and a list of prerequisite task IDs.\n\n### Artifacts\n\nArtifacts are outputs generated by agents during task execution. They can be stored and retrieved using the `Session` and `ExecutionContext` objects. The `ExecutionContext.record_artifact` method is used to save artifacts for the current step, while `ExecutionContext.record_initial` is used for initial inputs.\n\n### `AgentOutput` Model\n\nThe `AgentOutput` Pydantic model defines the structured response format for agents. It includes fields for the main `output` message, an optional `artifact` (containing files), and optional `team` updates (like new system prompts). This model ensures consistent communication between agents.\n\n### LLM Integration\n\nThe `runtime.llm.LLM` class and its subclasses abstract LLM interactions. The `LLM.factory` method allows for easy instantiation of different LLM providers based on configuration.\n\n## API Reference\n\n### `runtime.agent.Agent`\n\n*   **`__init__(self, name: str, role: str, goal: str, model: str, system_prompt: str)`**: Constructor for the `Agent` class.\n*   **`update_system_prompt(self, new_prompt: str)`**: Updates the agent's system prompt.\n*   **`execute_task(self, context: ExecutionContext) -> Optional[str]`**: Executes a task using the LLM based on the provided context. It prepares the prompt, including previous artifacts, and calls the LLM.\n*   **`_run(self, context: ExecutionContext, prompt: str) -> Optional[str]`**: Handles the core LLM interaction, including sending the prompt, processing the response, parsing JSON, and extracting artifacts. It also manages retries for LLM calls.\n\n### `runtime.core.Session`\n\n*   **`__init__(self, session_id: str, agents: list, state: str, session_dir: str, project_root: str)`**: Constructor for the `Session` class.\n*   **`add_artifact(self, name: str, content: Any)`**: Saves an artifact in the session directory. `content` can be a string, dictionary, or list. JSON content is saved with a `.json` extension.\n*   **`get_artifact(self, name: str) -> Any`**: Loads an artifact from the session directory. Returns the content or `None` if an error occurs.\n\n### `runtime.core.ExecutionContext`\n\n*   **`__init__(self, session: Session, plan: Plan, step_index: int, round_num: int, items: Dict[str, ContextItem])`**: Constructor for `ExecutionContext`.\n*   **`next(self)`**: Advances the step index.\n*   **`reset(self)`**: Resets the step index to 0.\n*   **`current_step(self) -> Task`**: Returns the current `Task` object.\n*   **`record_artifact(self, key: str, value: Any, mem: bool = False)`**: Records an artifact for the current step. If `mem` is `True`, it's also stored in the context's memory.\n*   **`record_initial(self, key: str, value: Any)`**: Records initial artifacts provided at the start of the workflow.\n\n### `runtime.custom_types` Module\n\n*   **`Task`**: Represents a single actionable step in a plan.\n    *   `id`: Unique ID for the task.\n    *   `description`: What the task is and how to execute it.\n    *   `role`: Role responsible for performing this task.\n    *   `agent`: Specific agent assigned to this task.\n    *   `requires`: List of prerequisite task IDs.\n*   **`File`**: Represents a generated file.\n    *   `path`: File path or name.\n    *   `content`: Full content of the file.\n*   **`Artifact`**: Output artifact generated by a task.\n    *   `task`: ID of the task that produced this artifact.\n    *   `files`: List of `File` objects included in the artifact.\n*   **`Role`**: Definition of a role in the team.\n    *   `title`: Role title.\n    *   `purpose`: High-level responsibilities and objectives.\n*   **`Plan`**: Overall strategy with reasoning, roles, and tasks.\n    *   `high_level_goal`: The overarching objective.\n    *   `reasoning`: Why this plan is structured this way.\n    *   `roles`: All roles needed for execution.\n    *   `tasks`: Step-by-step ordered tasks.\n*   **`Prompt`**: System prompt assigned to an agent.\n    *   `agent`: Name of the agent this prompt is for.\n    *   `role`: Role context of the agent.\n    *   `system_prompt`: Full text of the system prompt.\n*   **`Team`**: Metadata and coordination for the team.\n    *   `notes`: General notes, reflections, or feedback.\n    *   `prompts`: Updated system prompts for agents.\n*   **`AgentOutput`**: Structured output returned by an agent.\n    *   `output`: Message from the agent.\n    *   `artifact`: Optional `Artifact` generated by the agent.\n    *   `team`: Optional `Team` updates.\n    *   `reasoning`: Explanation of how the agent arrived at this output.\n\n## Advanced Usage & Tutorials\n\n### Advanced Planning\n\nThe planning process is primarily handled by the `Orchestrator` agent. To influence plan generation, you can:\n\n1.  **Refine the High-Level Goal:** A more specific and well-defined goal leads to a more effective plan.\n2.  **Engineer Prompts:** Modifying the system prompt for the `Orchestrator` (e.g., by adjusting `runtime/prompts/general_planning.txt`) can guide its reasoning and task decomposition. This allows for customization of how the orchestrator approaches planning, including task dependencies and role assignments.\n3.  **Provide Initial Files:** Including relevant context files during system startup can help the orchestrator create more informed plans.\n\n## Contribution Guide\n\n### Running Tests\n\nTo run the test suite, navigate to the project's root directory and execute:\n\n```bash\npytest\n```\n\nThis command will discover and run all tests defined within the project.\n\n## Troubleshooting & FAQ\n\n### How do I configure different LLM providers?\n\nLLM providers are configured within the `runtime.yaml` configuration file. You can specify the model name and potentially provider-specific settings (like API keys or base URLs) in the configuration. The `runtime.llm.LLM.factory` method handles the instantiation based on the model name provided (e.g., `ollama:llama3`, `gemini-1.5-flash`, `openai:gpt-4`). Ensure that any necessary API keys or environment variables are set correctly before running the system.\n\n---\n\n*This documentation is styled using a minimalist design aesthetic. The color palette features deep blue (`#0A2540`) and vibrant teal (`#1ABC9C`), with clean typography using 'Inter' for body text and 'Fira Code' for code. The layout is designed for optimal readability and accessibility, with a clear navigation structure and ample white space.*"},{"path":"runtime/agent.py","content":"\"\"\"This module defines the core Agent class and related functionalities.\n\nIt includes the agent's lifecycle methods, task execution logic, and\nhelper functions for agent instantiation and discovery.\n\"\"\"\n\nimport json as JSON\nimport uuid\nfrom dataclasses import dataclass, field\nimport re\nimport logging\nfrom typing import List, Dict, Any, Optional\n\nfrom runtime.llm import LLM\n\nfrom runtime.core import ExecutionContext, Session\n\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import AgentOutput\n\nclass Agent:\n    \"\"\"Represents a runtime agent instance.\"\"\"\n    name: str\n    role: str\n    goal: str\n    model: str\n    system_prompt: str\n    llm: LLM\n\n    def __init__(self, name: str, role: str, goal: str, model: str, system_prompt: str) -> None:\n        self.name = name\n        self.role = role\n        self.goal = goal\n        self.model = model\n        self.system_prompt = system_prompt\n        logger.info(f\"Agent instance created: {self.name} (Role: {self.role}, Model: {self.model})\")\n        self.llm = LLM.factory(model)\n\n    def update_system_prompt(self, new_prompt: str) -> None:\n        \"\"\"Updates the agent's system prompt.\"\"\"\n\n        self.system_prompt = new_prompt\n\n        logger.info(f\"Agent {self.name}'s system prompt updated:\\n{self.system_prompt}\\n\")\n\n    def execute_task(self, context: ExecutionContext) -> Optional[str]:\n        \"\"\"\n        Executes a task using the Generative AI model based on the provided context.\n\n        Args:\n            context (ExecutionContext): The execution context containing goal, plan, and artifacts.\n\n        Returns:\n            Optional[str]: The result of the task execution as a string, or an error string.\n        \"\"\"\n\n        task = context.current_step()\n\n        context.record_artifact(f\"{self.name}_prompt.txt\", self.system_prompt)\n\n        required_task_ids = ['initial']\n        required_task_ids.extend(task.requires)\n\n        required_artifacts = []\n\n        if len(required_task_ids) != 0:\n            for key, artifact in context.items.items():\n                logger.debug(f\"Checking artifact from {key} with task ID {artifact.step.id}\")\n                if artifact.step.id in required_task_ids:\n                    required_artifacts.append(artifact)\n\n        previous_artifacts = \"\\n\\n\".join(\n            f\"--- Artifact '{artifact.name}' from ({artifact.step.role}) in [{artifact.step.id}]:\\n{artifact.content}\"\n            for artifact in required_artifacts\n        )\n\n        task_prompt: List[str] = [\n            f\"The overall goal is: '{context.plan.high_level_goal}'\",\n            f\"Your role's specific goal is: '{self.goal}'\\n\"\n            f\"Your specific sub-task is: '{task.description}'\",\n\n            f\"The team's roles are:\\n    {context.plan.model_dump_json()}\",\n        ]\n\n        if previous_artifacts:\n            task_prompt.append(f\"Please use the following outputs from the other agents as your input:\\n\\n{previous_artifacts}\\n\\n\")\n\n        task_prompt.append(\n            f\"Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.\"\n        )\n\n        #logger.info(f\"Agent '{self.name}' is executing task: {task_prompt[1]}\")\n        #logger.info(f\"Task requires outputs from task IDs: {required_task_ids}\")\n        #logger.info(\n        #    f\"Found {len(required_artifacts)} required artifacts from previous tasks. Previous artifacts preview: {previous_artifacts[:500]}\"\n        #)\n\n        ret = self._run(context, \"\\n\\n\".join(task_prompt))\n        logger.info(f\"Agent '{self.name}' completed task: {task.description}\")\n        return ret\n\n\n    def _run(self, context: ExecutionContext, prompt: str) -> Optional[str]:\n        context.record_artifact(f\"{self.name}_task.txt\", prompt)\n\n        try:\n            response = self.llm.generate_content(   # ignore type\n                model_name=self.model,\n                contents=prompt,\n                system_instruction=self.system_prompt,\n                temperature=0.7,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=AgentOutput\n            )\n            if isinstance(response, AgentOutput):\n                result = response.model_dump_json()\n                print(f\"\\n--- Output:\\n{response.output}\\n\")\n                if response.artifact and response.artifact.files:\n                    for file in response.artifact.files:\n                        print(f\"\\n--- File: {file.path}\\n{file.content}\\n\")\n                        context.session.add_artifact(file.path, file.content)\n            else:\n                result = response or '{}'\n\n                if '```' in result:\n                    match = re.search(r'```(json)?\\s*([\\s\\S]*?)\\s*```', result, re.IGNORECASE)\n                    if match:\n                        # Extract the JSON content from the markdown block\n                        result = match.group(2).strip()\n\n                json_data = JSON.loads(result)\n                if isinstance(json_data, dict):\n                    if \"output\" in json_data and json_data[\"output\"] is not None:\n                        output = JSON.dumps(json_data[\"output\"], indent=4) if isinstance(json_data[\"output\"], dict) else json_data[\"output\"]\n                        logger.info(f\"Output:\\n{output[:2000]}\\n\")\n\n                    if \"artifact\" in json_data and json_data[\"artifact\"] is not None:\n                        if \"files\" in json_data[\"artifact\"]:\n                            for file_data in json_data[\"artifact\"][\"files\"]:\n                                if \"path\" in file_data and \"content\" in file_data:\n                                    print(f\"\\n--- File: {file_data[\"path\"]}\")\n                                    print(f\"{file_data[\"content\"]}\")\n                                    context.session.add_artifact(file_data[\"path\"], file_data[\"content\"])\n                else:\n                    logger.warning(\"Agent output is not in expected AgentOutput format. Processing as plain text.\")\n                    logger.info(f\"Output: '\\n{result[:2000]}\\n'\")\n\n        except Exception as e:\n            logger.exception(f\"Error executing task for {self.name}: {e}\")\n            raise\n\n        return result\n\ndef find_agent_by_role(agents: List[Agent], role: str) -> Optional[Agent]:\n    \"\"\"\n    Finds an agent in a list by its role.\n\n    Args:\n        agents (List[Agent]): A list of Agent objects.\n        role (str): The role of the agent to find.\n\n    Returns:\n        Optional[Agent]: The found Agent object, or None if not found.\n    \"\"\"\n    logger.debug(f\"Searching for agent with role: {role} in agents: {[agent.name for agent in agents]}\")\n    return next((agent for agent in agents if agent.role == role), None)\n"},{"path":"runtime/core.py","content":"\"\"\"This module provides the core data structures for the multi-agent runtime.\n\nIt defines the ExecutionContext for managing workflow state and the Session\nfor handling artifacts and session-specific data.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List\nimport uuid\nimport os\nimport logging\nimport json\n\nfrom runtime.custom_types import Plan, Task\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContextItem:\n    def __init__(self, name: str, content: Any, step: Task):\n        self.name = name\n        self.content = content\n        self.step = step\n\n\n@dataclass\nclass ExecutionContext:\n    \"\"\"Holds the state and context for a multi-agent workflow.\"\"\"\n    session: 'Session'\n    plan: Plan\n    step_index: int = 0\n    round_num: int = 0\n    items: Dict[str, ContextItem] = field(default_factory=dict)\n\n    def next(self) -> None:\n        self.step_index += 1\n\n    def reset(self) -> None:\n        self.step_index = 0\n\n    def current_step(self) -> Task:\n        \"\"\"Returns the current step in the plan.\"\"\"\n        return self.plan.tasks[self.step_index]\n\n    def _remember_artifact(self, key: str, value: Any, step: Task | None = None) -> None:\n        \"\"\"Remembers an artifact from a step's execution for future tasks.\"\"\"\n        self.items[key] = ContextItem(name=key, content=value, step=step or self.current_step())\n\n    def record_artifact(self, key: str, value: Any, mem: bool = False) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n            mem (bool): If True, the artifact is also stored in the execution context's memory.\n        \"\"\"\n        artifact_key = f\"__{self.round_num}/__step_{self.step_index}_{key}\"\n        self.session.add_artifact(artifact_key, value)\n        if mem:\n            self._remember_artifact(artifact_key, value)\n\n    def record_initial(self, key: str, value: Any) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n        \"\"\"\n        self._remember_artifact(key, value, step=Task(\n            id='initial',\n            description='Initial files provided by the user',\n            role='User',\n            agent='User',\n            requires=[]\n        ))\n\n\n\n\n@dataclass\nclass Session:\n    \"\"\"Manages the runtime context for a task, including session ID and artifact storage.\"\"\"\n    session_id: str = field(default_factory=lambda: f\"session_{uuid.uuid4()}\")\n    agents: list = field(default_factory=list) # Type hint will be updated later to List[Agent]\n    state: str = \"initialized\"\n    session_dir: str = field(init=False)\n    project_root: str = field(default_factory=str)\n\n    def __post_init__(self) -> None:\n        \"\"\"Initializes the session directory.\"\"\"\n        if not self.project_root:\n            self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n        self.session_dir = os.path.join(self.project_root, 'sessions', self.session_id)\n        os.makedirs(self.session_dir, exist_ok=True)\n        logger.info(f\"Session created: {self.session_id} (Directory: {self.session_dir})\")\n\n    def add_artifact(self, name: str, content: Any) -> None:\n        \"\"\"\n        Saves an artifact in the session directory.\n\n        Args:\n            name (str): The name of the artifact file.\n            content (Any): The content to write to the artifact file. Can be dict, list, or string.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            os.makedirs(os.path.dirname(artifact_path), exist_ok=True)\n            with open(artifact_path, 'w', encoding='utf-8') as f:\n                if isinstance(content, (dict, list)):\n                    json.dump(content, f, indent=4)\n                else:\n                    f.write(str(content))\n            logger.info(f\"Artifact '{name}' saved in session {self.session_id}.\")\n        except (TypeError, IOError) as e:\n            logger.error(f\"Error saving artifact '{name}': {e}\")\n\n    def get_artifact(self, name: str) -> Any:\n        \"\"\"\n        Loads an artifact from the session directory.\n\n        Args:\n            name (str): The name of the artifact file to load.\n\n        Returns:\n            Any: The content of the artifact, or None if an error occurs.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            with open(artifact_path, 'r', encoding='utf-8') as f:\n                if name.endswith('.json'):\n                    return json.load(f)\n                return f.read()\n        except (FileNotFoundError, json.JSONDecodeError, IOError) as e:\n            logger.error(f\"Error retrieving artifact '{name}': {e}\")\n            return None\n"},{"path":"runtime/custom_types.py","content":"\"\"\"This module defines custom data types and Pydantic models for the runtime.\n\nIt ensures data consistency and validation across different modules by providing\ncentralized, well-defined data structures for plans, tasks, and agent outputs.\n\"\"\"\n\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass Task(BaseModel):\n    \"\"\"A single actionable step in a plan.\"\"\"\n    id: str = Field(..., description=\"Unique ID for the task (e.g., 'T1', 'T2').\")\n    description: str = Field(..., description=\"Detailed description of what to do for this task.\")\n    role: str = Field(..., description=\"The role responsible for this task.\")\n    agent: str = Field(..., description=\"The agent assigned to this task.\")\n    requires: List[str] = Field(..., description=\"List of tasks (IDs) whose outputs are required (e.g. ['T1', 'T2']).\")\n\n\nclass File(BaseModel):\n    \"\"\"Represents a generated file.\"\"\"\n    path: str = Field(..., description=\"File path or name (e.g., 'src/main.py').\")\n    content: str = Field(..., description=\"Full content of the file.\")\n\n\nclass Artifact(BaseModel):\n    \"\"\"Output artifact generated by a task.\"\"\"\n    task: str = Field(..., description=\"ID of the task that produced this artifact (e.g., 'T1').\")\n    files: List[File] = Field(..., description=\"List of files created or modified by the task.\")\n\n\nclass Role(BaseModel):\n    \"\"\"Definition of a role in the team.\"\"\"\n    title: str = Field(..., description=\"Title of the role (e.g., 'Engineer', 'Reviewer').\")\n    purpose: str = Field(..., description=\"The main purpose and goal of this role.\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"System prompt assigned to an agent.\"\"\"\n    agent: str = Field(..., description=\"Name of the agent this prompt is for. Must match exactly an existing agent.\")\n    role: str = Field(..., description=\"Role context of the agent.\")\n    system_prompt: str = Field(..., description=\"Full text of the system prompt for this agent.\")\n\n\nclass Team(BaseModel):\n    \"\"\"Metadata and coordination for the team.\"\"\"\n    notes: str = Field(..., description=\"General notes or feedback about the team's performance or the plan.\")\n    prompts: List[Prompt] = Field(default_factory=list, description=\"A list of new or updated system prompts for any agent in the team.\")\n\n\nclass Plan(BaseModel):\n    \"\"\"Overall strategy with reasoning, roles, and tasks.\"\"\"\n    high_level_goal: str = Field(..., description=\"The main goal this plan is designed to achieve.\")\n    reasoning: str = Field(..., description=\"A brief explanation of the plan's structure and strategy.\")\n    roles: List[Role] = Field(..., description=\"List of all roles required to execute the plan.\")\n    tasks: List[Task] = Field(..., description=\"A step-by-step sequence of tasks to be executed in order.\")\n    team: Optional[Team] = Field(default=None, description=\"Updates to team configuration or system prompts.\")\n\n\nclass AgentOutput(BaseModel):\n    \"\"\"Structured output returned by an agent after executing a task.\"\"\"\n    output: str = Field(..., description=\"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\")\n    artifact: Optional[Artifact] = Field(default=None, description=\"Any files created or modified by the agent during the task.\")\n    team: Optional[Team] = Field(default=None, description=\"Updates to team configuration or system prompts.\")\n    reasoning: Optional[str] = Field(default=None, description=\"Explanation of how the agent arrived at this output.\")\n\n\n\nif __name__ == \"__main__\":\n    import json\n    print(json.dumps(AgentOutput.model_json_schema(), indent=4))\n"},{"path":"runtime/__init__.py","content":"# This file makes the 'runtime' directory a Python package.\n\nfrom .custom_types import Role, Task, Plan, Artifact, File, Prompt, Team, AgentOutput\n\nfrom .agent import Agent\nfrom .core import Session\nfrom .orchestrator import Orchestrator\nfrom .system import System\n"},{"path":"runtime/llm.py","content":"\"\"\"This module abstracts interactions with Large Language Models (LLMs).\n\nIt provides a unified interface for various LLM providers and models,\nallowing for easy integration and interchangeability of different LLMs.\n\"\"\"\n\nimport json\nimport os\nimport time\nfrom google import genai\nfrom google.genai import types\nfrom ollama import Client as Ollama\nfrom typing import Optional, Any, Dict\nfrom abc import ABC, abstractmethod\nimport logging\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLM(ABC):\n    \"\"\"\n    Abstract base class for Large Language Models.\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    def __init__(self) -> None:\n        pass\n\n    @abstractmethod\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: Any = None) -> Optional[str]: # type: ignore\n        \"\"\"Generates content using an LLM.\"\"\"\n        pass\n\n    @staticmethod\n    def factory(species: str) -> 'LLM':\n        print(f\"LLM Factory: Creating LLM instance for species '{species}'\")\n        if species == 'Olli':\n            return Olli()\n        elif species == 'Kimi':\n            return Kimi()\n        if species.startswith('ollama:'):\n            # Extract model name after 'ollama:'\n            model_name = species.split(':', 1)[1]\n            return Olli(species=model_name)\n        if species.startswith('opi:'):\n            # Extract model name after 'opi:'\n            model_name = species.split(':', 1)[1]\n            return Opi(species=model_name)\n        if species.startswith('mistral:'):\n            # Extract model name after 'mistral:'\n            model_name = species.split(':', 1)[1]\n            return Mistral(species=model_name)\n        return Gemini(species)\n\n\nclass Gemini(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients: Dict[str, genai.Client] = {} # Class-level cache for clients\n\n    def __init__(self, species: str) -> None:\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        config = types.GenerateContentConfig(\n            system_instruction=system_instruction,\n            temperature=temperature,\n            response_mime_type=response_mime_type,\n            response_schema=response_schema,\n            max_output_tokens=50000,\n        )\n\n        # try three times\n        for _ in range(3):\n            try:\n                response = client.models.generate_content(\n                    model=model_name,\n                    contents=contents,\n                    config=config,\n                )\n                if not response.candidates or response.candidates[0].content is None or response.candidates[0].content.parts is None or not response.candidates[0].content.parts or response.candidates[0].content.parts[0].text is None:\n                    raise ValueError(f\"Gemini: No content in response from model {model_name}. Retrying...\")\n                break\n            except Exception as ex:\n                logger.exception(f\"Error generating content with model {model_name}: {ex}\")\n                # If it's the last retry, return None\n                if _ == 2:\n                    return None\n                logger.warning(f\"Retrying content generation for model {model_name}...\")\n                time.sleep(10)\n                #return None\n\n        if isinstance(response.parsed, BaseModel):\n            return response.parsed\n\n        if response_mime_type == 'application/json':\n            try:\n                # Attempt to parse the JSON response\n                json_output = response.candidates[0].content.parts[0].text.strip()\n                # Validate against schema if provided\n                if response_schema:\n                    response_schema.model_validate_json(json_output)\n                return json_output\n            except Exception as ex:\n                logger.exception(f\"Error parsing or validating JSON response: {ex}\")\n\n        return response.candidates[0].content.parts[0].text.strip()\n\n    def _get_client(self) -> genai.Client:\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if self.species not in Gemini._clients:\n                logger.info(f\"Initializing GenAI client for species {self.species}\")\n                Gemini._clients[self.species] = genai.Client()\n            return Gemini._clients[self.species]\n        except Exception as e:\n            logger.exception(f\"Error initializing GenAI client: {e}\")\n            raise\n\n\nclass Olli(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Olli'):\n        self.client = None\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        print(f\"Olli: Using model {model_name} with temperature {temperature}\")\n        client = self._get_client(species=self.species)\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n            fmt = response_schema.model_json_schema(by_alias=False, mode='serialization')\n            print(f\"Olli: Using response format {fmt}\")\n            response = client.generate(\n                model=self.species,#model_name,\n                prompt=contents,\n                format=fmt,\n                options={\"temperature\": temperature, \"system_instruction\": system_instruction},\n                stream=True\n            )\n            for chunk in response:\n                if hasattr(chunk, \"response\") and chunk.response:\n                    print(chunk.response, end=\"\")\n                    out += chunk.response\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    @staticmethod\n    def _get_client(species: str):\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if (Olli._clients is None):\n                Olli._clients = {}\n            if species not in Olli._clients:\n                Olli._clients[species] = Ollama()#base_url='http://localhost:11434')\n            return Olli._clients[species]\n        except Exception as e:\n            logger.exception(f\"Error initializing Ollama client: {e}\")\n            return None\n\n\n\n\n\nfrom huggingface_hub import InferenceClient, ChatCompletionInputResponseFormatText, ChatCompletionInputResponseFormatJSONObject, ChatCompletionInputResponseFormatJSONSchema, ChatCompletionInputJSONSchema\n\n\nclass Kimi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Kimi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            stream = client.chat.completions.create(\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=ChatCompletionInputResponseFormatText() if response_mime_type == 'text/plain' else ChatCompletionInputResponseFormatJSONObject() #if response_schema is None else ChatCompletionInputResponseFormatJSONSchema(json_schema=ChatCompletionInputJSONSchema(name=response_schema.model_json_schema()))\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a inference client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Kimi._clients:\n                    Kimi._clients[self.species] = InferenceClient(\n                        provider=\"featherless-ai\",\n                        api_key=os.environ.get(\"HF_TOKEN\"),\n                        model=\"moonshotai/Kimi-K2-Instruct\"\n                    )\n            return Kimi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing GenAI client: {e}\")\n            return None\n\n\n\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import ResponseFormat\n\n\nclass Opi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Opi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            response_format: ResponseFormat = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = {\"type\": \"json_schema\", \"json_schema\": response_schema.model_json_schema()}   # type: ignore\n\n            print(f\"Opi: Using response format {response_format}\")\n\n            stream = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=response_format\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a OpenAI client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Opi._clients:\n                    Opi._clients[self.species] = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"), base_url=os.environ.get(\"OPENAI_API_BASE\"))\n            return Opi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing OpenAI client: {e}\")\n            return None\n\n\n\n##\n\n\nfrom mistralai import Mistral as MistralClient\nfrom mistralai.extra.utils import response_format_from_pydantic_model\n\nclass Mistral(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Mistral'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified Mistral model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"mistral-large-latest\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            str: The response from the Mistral model.\n        \"\"\"\n        client = Mistral._get_client(species=self.species)\n        if not client:\n            return None\n        try:\n            messages = []\n            if system_instruction:\n                messages.append({\"role\": \"system\", \"content\": system_instruction})\n            messages.append({\"role\": \"user\", \"content\": contents})\n\n            print(f\"Mistral: Using model {model_name} with temperature {temperature}\")\n\n            response_format = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = response_format_from_pydantic_model(response_schema)  # type: ignore\n\n\n            out = \"\"\n            stream = client.chat.stream(\n                model=self.species,#model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=50000,\n                response_format=response_format # type: ignore\n            )\n            for chunk in stream:\n                if chunk.data.choices[0].delta.content is None:\n                    continue\n                print(chunk.data.choices[0].delta.content, end=\"\")\n                out += str(chunk.data.choices[0].delta.content)\n\n            return out\n        except Exception as e:\n            print(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    @staticmethod\n    def _get_client(species: str):\n        \"\"\"\n        Returns a Mistral client instance.\n        \"\"\"\n        try:\n            if Mistral._clients is None:\n                Mistral._clients = {}\n            if species not in Mistral._clients:\n                api_key = os.environ.get(\"MISTRAL_API_KEY\")\n                if not api_key:\n                    raise ValueError(\"MISTRAL_API_KEY environment variable not set.\")\n                Mistral._clients[species] = MistralClient(api_key=api_key)\n            return Mistral._clients[species]\n        except Exception as e:\n            logger.exception(f\"Error initializing Mistral client: {e}\")\n            return None\n"},{"path":"runtime/log.py","content":"\"\"\"This module configures and sets up the logging for the application.\n\nIt defines the log format, level, and handlers (console, file, JSONL),\nproviding a centralized and consistent logging setup.\n\"\"\"\n\nimport logging\nimport os\nimport re\nimport json\nfrom logging.handlers import RotatingFileHandler\nfrom colorama import Fore, Style\n\n# --- Configuration ---\nLOG_LEVEL = \"INFO\"\nLOG_FILE_DIR = \"logs\"\nLOG_FILE_NAME = \"app.log\"\nLOG_FILE_PATH = os.path.join(LOG_FILE_DIR, LOG_FILE_NAME)\nMAX_FILE_SIZE = 1024 * 1024  # 1 MB\nBACKUP_COUNT = 5\n\n# --- Colored Formatter ---\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"\n    A custom logging formatter that adds colors to the log messages.\n    It can color based on log level, or on regex matching of the message.\n    \"\"\"\n    # ANSI escape codes for colors\n    COLORS = {\n        'DEBUG': Fore.YELLOW,\n        'INFO': Fore.CYAN,\n        'WARNING': Fore.YELLOW,\n        'ERROR': Fore.RED,\n        'CRITICAL': Fore.MAGENTA,\n        'RESET': Style.RESET_ALL\n    }\n\n    # Regex-based coloring rules (pattern, color)\n    REGEX_RULES_MESSAGES = [\n        (re.compile(r\"Agent instance created:\"), Fore.YELLOW),\n        (re.compile(r\"Agent '.*' system prompt updated.\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing task\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing step\"), Fore.LIGHTRED_EX),\n        (re.compile(r\"Agent '.*' completed task\"), Fore.BLUE),\n        (re.compile(r\"Warning: Agent output is not in expected AgentOutput format.\"), Fore.YELLOW),\n        (re.compile(r\"System Runtime\"), Fore.CYAN),\n        (re.compile(r\"Error: No agents could be instantiated.\"), Fore.RED),\n        (re.compile(r\"Session created:\"), Fore.GREEN),\n        (re.compile(r\"Artifact '.*' saved in session .*\"), Fore.LIGHTBLUE_EX),\n        (re.compile(r\"Error saving artifact '.*'\"), Fore.RED),\n        (re.compile(r\"Files provided:\"), Fore.MAGENTA),\n        (re.compile(r\"Error reading file .*\"), Fore.RED),\n        (re.compile(r\"Generated plan:\"), Fore.WHITE),\n        (re.compile(r\"Orchestration failed: Could not generate a valid plan.\"), Fore.RED),\n        (re.compile(r\"Orchestrator .* is generating a plan for\"), Fore.BLUE),\n        (re.compile(r\"Orchestrator .* is starting workflow round\"), Fore.CYAN),\n        (re.compile(r\"Orchestrator has completed workflow round\"), Fore.CYAN),\n        (re.compile(r\"Agent .* provided new prompts.\"), Fore.LIGHTGREEN_EX),\n        (re.compile(r\"Target agent '.*' not found for prompt update.\"), Fore.YELLOW),\n        (re.compile(r\"Error generating or validating plan for .*\"), Fore.RED),\n    ]\n\n    COLORS_NAMES = {\n        'runtime.orchestrator': Fore.LIGHTYELLOW_EX,\n        'runtime.agent': Fore.LIGHTWHITE_EX,\n        'runtime.sysmain': Fore.LIGHTRED_EX,\n        'runtime.core': Fore.LIGHTBLUE_EX,\n        'runtime.bootstrap': Fore.LIGHTRED_EX,\n        'runtime.loader': Fore.LIGHTRED_EX,\n        'runtime.llm': Fore.LIGHTMAGENTA_EX,\n        'runtime.util': Fore.GREEN,\n        'runtime.factory': Fore.LIGHTRED_EX,\n        'runtime.paths': Fore.BLUE,\n        'runtime.custom_types': Fore.CYAN,\n    }\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"\n        Formats the log record with the appropriate color.\n        \"\"\"\n        log_message = super().format(record)\n\n        # Check for regex-based coloring first\n        for pattern, color in self.REGEX_RULES_MESSAGES:\n            if pattern.search(record.getMessage()):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Check for logger name-based coloring\n        for name_prefix, color in self.COLORS_NAMES.items():\n            if record.name.startswith(name_prefix):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Fallback to level-based coloring\n        return f\"{self.COLORS.get(record.levelname, self.COLORS['RESET'])}{log_message}{self.COLORS['RESET']}\"\n\n# --- Setup ---\ndef setup_logging(level: str = LOG_LEVEL) -> None:\n    \"\"\"\n    Sets up logging for the application.\n    \"\"\"\n    # Create log directory if it doesn't exist\n    if not os.path.exists(LOG_FILE_DIR):\n        os.makedirs(LOG_FILE_DIR)\n\n    # Get the root logger\n    logger = logging.getLogger()\n    logger.setLevel(level)\n\n    # --- Console Handler ---\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(level)\n\n    # --- File Handler ---\n    file_handler = RotatingFileHandler(\n        LOG_FILE_PATH,\n        maxBytes=MAX_FILE_SIZE,\n        backupCount=BACKUP_COUNT\n    )\n    file_handler.setLevel(level)\n\n    # --- Formatters ---\n    console_formatter = ColoredFormatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    file_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n\n    class JsonFormatter(logging.Formatter):\n        def format(self, record: logging.LogRecord):\n            log_record = {\n                \"timestamp\": self.formatTime(record, self.datefmt),\n                \"name\": record.name,\n                \"level\": record.levelname,\n                \"message\": record.getMessage(),\n                \"pathname\": record.pathname,\n                \"lineno\": record.lineno,\n                \"funcName\": record.funcName,\n                \"process\": record.process,\n                \"thread\": record.thread,\n                \"threadName\": record.threadName,\n            }\n            if record.exc_info:\n                log_record[\"exc_info\"] = self.formatException(record.exc_info)\n            if record.stack_info:\n                log_record[\"stack_info\"] = self.formatStack(record.stack_info)\n            return json.dumps(log_record)\n\n    # --- Add Formatter to Handlers ---\n    console_handler.setFormatter(console_formatter)\n    file_handler.setFormatter(file_formatter)\n\n    # --- Add Handlers to Logger ---\n    # Avoid adding handlers multiple times\n    if not logger.handlers:\n        logger.addHandler(console_handler)\n        logger.addHandler(file_handler)\n\n        # --- JSONL File Handler ---\n        jsonl_file_handler = RotatingFileHandler(\n            os.path.join(LOG_FILE_DIR, \"app.jsonl\"),\n            maxBytes=MAX_FILE_SIZE,\n            backupCount=BACKUP_COUNT\n        )\n        jsonl_file_handler.setLevel(level)\n        jsonl_file_handler.setFormatter(JsonFormatter())\n        logger.addHandler(jsonl_file_handler)\n\n# --- Main Application Logger ---\n# It's good practice to use a named logger instead of the root logger directly\napp_logger = logging.getLogger(__name__)\n\n# --- Example Usage ---\nif __name__ == '__main__':\n    setup_logging()\n    app_logger.info(\"Logging setup complete.\")\n    app_logger.debug(\"This is a debug message.\")\n    app_logger.warning(\"This is a warning message.\")\n    app_logger.error(\"This is an error message.\")\n"},{"path":"runtime/orchestrator.py","content":"\"\"\"This module defines the Orchestrator agent and its planning capabilities.\n\nThe Orchestrator is a specialized agent responsible for generating, managing,\nand executing the workflow plan based on a high-level goal. This is being\nrefactored to move workflow execution to the System class.\n\"\"\"\n\nimport os\nimport json\nfrom typing import List, Dict, Optional, Tuple\nimport logging\n\nfrom pydantic import BaseModel, Field, ValidationError\n\nfrom runtime.agent import Agent\nfrom runtime.core import Session\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import Plan, File\n\nclass Orchestrator(Agent):\n    \"\"\"An agent responsible for creating and managing a plan for multi-agent workflows.\"\"\"\n    team: Dict[str,Agent] = {}\n\n    def generate_plan(self, session: Session, high_level_goal: str, files: List[File] = []) -> Optional[Plan]:\n        \"\"\"\n        Invokes the language model to get a structured plan for the given high-level goal.\n        The plan includes a sequence of tasks and the roles responsible for them.\n\n        Args:\n            session (Session): The current session object.\n            high_level_goal (str): The high-level goal for which to generate a plan.\n            file_contents (List[Tuple[str, str]]): A list of file paths and their contents to provide context to the LLM.\n\n        Returns:\n            Optional[Plan]: The generated plan as a Pydantic object, or None if an error occurs.\n        \"\"\"\n        logger.info(f\"Orchestrator {self.name} is generating a plan for: '{high_level_goal}'\")\n        if not self.llm or not self.team:\n            logger.error(\"Orchestrator client or team not initialized. Cannot generate plan.\")\n            return None\n\n        # Construct a description of the available agents and their roles/goals\n        team_description = \"\\n\".join(\n            f\"- Name: '{agent.name}'\\n\"\n            f\"  Role: `{agent.role}`\\n\"\n            f\"  Goal: \\\"{agent.goal}\\\"\"\n            for agent in self.team.values()\n        )\n\n        # Load the general planning prompt template\n        t20_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) # Not used\n        general_prompt_template = read_file(os.path.join(t20_root, \"prompts\", \"general_planning.txt\")).strip()\n        planning_prompt_parts = [general_prompt_template.format(\n            high_level_goal=high_level_goal,\n            team_description=team_description\n        )]\n\n        # Include file contents if provided\n        file_section = []\n        for file in files:\n            file_section.append(f\"\\n--- File: '{file.path}'\\n```\\n{file.content}\\n```\")\n        planning_prompt_parts.append(\"\\n\".join(file_section))\n\n        logger.debug(\"Planning prompt parts for LLM: %s\", planning_prompt_parts)\n\n        # Combine prompt parts for the LLM call\n        full_prompt_for_llm = \"\\n\\n\".join(planning_prompt_parts)\n        logger.info(f\"Orchestrator {self.name} Planning Prompt:\\n{full_prompt_for_llm}\")\n\n        session.add_artifact(\"planning_prompt.txt\", full_prompt_for_llm)\n\n        try:\n            response = self.llm.generate_content(\n                model_name=self.model,\n                contents=\"\\n\\n\".join(planning_prompt_parts),\n                system_instruction=self.system_prompt,\n                temperature=0.1,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=Plan\n            )\n            if isinstance(response, Plan):\n                plan = response\n            else:\n                plan = Plan.model_validate_json(response or '{}')\n        except (ValidationError, json.JSONDecodeError) as e:\n            logger.exception(f\"Error generating or validating plan for {self.name}: {e}\")\n            return None\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during plan generation for {self.name}: {e}\")\n            return None\n\n        return plan\n"},{"path":"runtime/paths.py","content":"\"\"\"\nCentralized path constants for the project.\n\nThis module defines constants for directory and file names, allowing them to be\nreused consistently throughout the application.\n\"\"\"\n\n# Directory Names\nAGENTS_DIR_NAME = \"agents\"\nCONFIG_DIR_NAME = \"config\"\nPROMPTS_DIR_NAME = \"prompts\"\n\n# File Names\nRUNTIME_CONFIG_FILENAME = \"runtime.yaml\""},{"path":"runtime/sysmain.py","content":"\"\"\"This module serves as the main entry point for the command-line interface.\n\nIt parses command-line arguments and initiates the system bootstrap process,\nacting as the primary interface for running the multi-agent system.\n\"\"\"\n\nimport os\nimport argparse\nimport logging\n\nfrom runtime.log import setup_logging\nfrom runtime.system import System\nfrom runtime.custom_types import File\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\ndef parse_command_line_arguments() -> argparse.Namespace:\n    \"\"\"\n    Parses command-line arguments for the runtime system.\n\n    Returns:\n        An argparse.Namespace object containing the parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Run the Gemini agent runtime.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    general_group = parser.add_argument_group(\"General Settings\")\n    general_group.add_argument(\"-p\", \"--plan-only\", action=\"store_true\", help=\"Generate only the plan without executing tasks.\")\n    general_group.add_argument(\"-r\", \"--rounds\", type=int, default=1, help=\"The number of rounds to execute the workflow.\")\n    general_group.add_argument(\"-f\", \"--files\", nargs='*', help=\"List of files to be used in the task.\", default=[])\n\n    orchestrator_group = parser.add_argument_group(\"Orchestrator Settings\")\n    orchestrator_group.add_argument(\"-o\", \"--orchestrator\", type=str, help=\"The name of the orchestrator to use.\", default=\"Meta-AI\")\n    orchestrator_group.add_argument(\"-m\", \"--model\", type=str, help=\"Default LLM model to use.\", default=\"gemini-2.5-flash-lite\")\n\n    parser.add_argument(\"task\", type=str, help=\"The initial task for the orchestrator to perform.\")\n\n    args = parser.parse_args()\n\n    if args.rounds < 1:\n        parser.error(\"Number of rounds must be at least 1.\")\n    if not args.task:\n        parser.error(\"The task argument is required.\")\n\n    return args\n\ndef setup_application_logging(log_level: str = \"INFO\") -> None:\n    \"\"\"\n    Sets up the global logging configuration for the application.\n\n    Args:\n        log_level: The desired logging level (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\n    \"\"\"\n    try:\n        setup_logging(level=log_level)\n        logger.info(f\"Logging initialized with level: {log_level}\")\n    except ValueError as e:\n        logger.error(f\"Invalid log level '{log_level}': {e}\")\n        raise\n\ndef system_main() -> None:\n    \"\"\"\n    Main entry point for the runtime system.\n    Parses arguments, sets up logging and configuration, and runs the system.\n    \"\"\"\n    try:\n        # 1. Parse command line arguments\n        args = parse_command_line_arguments()\n        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\n        # 2. Initial logging setup\n        setup_application_logging()\n\n        # 3. Convert file paths to File objects\n        file_objects = []\n        for file_path in args.files:\n            try:\n                content = read_file(file_path)\n                if content.startswith(\"Error:\"):\n                    logger.warning(f\"Skipping file '{file_path}': {content}\")\n                    continue\n                file_objects.append(File(path=file_path, content=content))\n            except Exception as e:\n                logger.error(f\"Error processing file '{file_path}': {e}\")\n\n        # 4. Instantiate and set up the system\n        system = System(root_dir=project_root, default_model=args.model)\n        system.setup(orchestrator_name=args.orchestrator)\n\n        # 5. Re-configure logging based on loaded config\n        log_level = system.config.get(\"logging_level\", \"INFO\")\n        setup_application_logging(log_level=log_level)\n\n        # 6. Start the system's main workflow\n        plan = system.start(\n            high_level_goal=args.task,\n            files=file_objects\n        )\n        if args.plan_only:\n            logger.info(\"Plan-only mode: Workflow execution skipped.\")\n            return\n\n        # 7. Run the system's main workflow\n        system.run(\n            plan=plan,\n            rounds=args.rounds,\n            files=file_objects\n        )\n\n    except (FileNotFoundError, RuntimeError) as e:\n        logger.exception(f\"A critical error occurred: {e}\")\n        exit(1)\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred during system main execution: {e}\")\n        exit(1)\n\n\nif __name__ == \"__main__\":\n    system_main()\n"},{"path":"runtime/system.py","content":"\"\"\"This module defines the core System class for the multi-agent runtime.\n\nIt encapsulates the state and behavior of the entire system, including\nconfiguration loading, agent instantiation, and workflow execution.\n\"\"\"\n\nimport os\nimport json\nimport yaml\nfrom glob import glob\n\nimport logging\nfrom typing import Any, List, Optional, Dict\n\nfrom .core import Session, ExecutionContext\nfrom .agent import Agent, find_agent_by_role\nfrom .orchestrator import Orchestrator\nfrom .log import setup_logging\nfrom .paths import AGENTS_DIR_NAME, CONFIG_DIR_NAME, PROMPTS_DIR_NAME, RUNTIME_CONFIG_FILENAME\n\nfrom .custom_types import AgentOutput, Artifact, Plan, File\nlogger = logging.getLogger(__name__)\n\nclass System:\n    \"\"\"\n    Represents the entire multi-agent system, handling setup, execution, and state.\n    \"\"\"\n    def __init__(self, root_dir: str, default_model: str = \"Olli\"):\n        \"\"\"\n        Initializes the System.\n\n        Args:\n            root_dir (str): The root directory of the project.\n            default_model (str): The default LLM model to use.\n        \"\"\"\n        self.root_dir = root_dir\n        self.default_model = default_model\n        self.config: dict = {}\n        self.agents: List[Agent] = []\n        self.session: Optional[Session] = None\n        self.orchestrator: Optional[Orchestrator] = None\n\n    def setup(self, orchestrator_name: Optional[str] = None) -> None:\n        \"\"\"\n        Sets up the system by loading configurations and instantiating agents.\n\n        Args:\n            orchestrator_name (str, optional): The name of the orchestrator to use.\n\n        Raises:\n            RuntimeError: If no agents or orchestrator can be set up.\n        \"\"\"\n        logger.info(\"--- System Setup ---\")\n        print(f\"Using default model: {self.default_model}\")\n\n        self.config = self._load_config(os.path.join(self.root_dir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME))\n        agent_specs = self._load_agent_templates(os.path.join(self.root_dir, AGENTS_DIR_NAME), self.config, self.default_model)\n        prompts = self._load_prompts(os.path.join(self.root_dir, PROMPTS_DIR_NAME))\n\n        all_team_members = {member.lower() for spec in agent_specs if 'team' in spec for member in spec['team']}\n\n        agents: List[Agent] = []\n        for spec in agent_specs:\n            spec[\"model\"] = spec.get(\"model\", self.default_model)\n            if spec['name'].lower() not in all_team_members:\n                agent = self._instantiate_agent(spec, prompts, agent_specs)\n                if agent:\n                    agents.append(agent)\n        self.agents = agents\n\n        if not self.agents:\n            raise RuntimeError(\"No agents could be instantiated. System setup failed.\")\n\n        orchestrator: Optional[Agent] = None\n        if orchestrator_name:\n            orchestrator = next((agent for agent in self.agents if agent.name.lower() == orchestrator_name.lower()), None)\n        else:\n            orchestrator = find_agent_by_role(self.agents, 'Orchestrator')\n\n        if not orchestrator:\n            error_msg = f\"Orchestrator with name '{orchestrator_name}' not found.\" if orchestrator_name else \"Orchestrator with role 'Orchestrator' not found.\"\n            raise RuntimeError(f\"{error_msg} System setup failed.\")\n\n        if not isinstance(orchestrator, Orchestrator):\n            raise RuntimeError(f\"Agent '{orchestrator.name}' is not a valid Orchestrator instance. System setup failed.\")\n\n        self.orchestrator = orchestrator\n        self.session = Session(agents=self.agents, project_root=\"./\")\n        logger.info(\"--- System Setup Complete ---\")\n\n    def start(self, high_level_goal: str, files: List[File] = []) -> Plan:\n        \"\"\"\n        Runs the main workflow of the system.\n\n        Args:\n            initial_task (str): The initial task for the orchestrator to perform.\n            plan_only (bool): If True, only generates the plan without executing tasks.\n            rounds (int): The number of rounds to execute the workflow.\n            files (List[str]): List of files to be used in the task.\n\n        Raises:\n            RuntimeError: If the system is not set up before running.\n        \"\"\"\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call setup() before start().\")\n\n        plan: Optional[Plan] = self.orchestrator.generate_plan(self.session, high_level_goal, files)\n        if not plan:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        logger.debug(f\"{plan.model_dump_json()}\")\n\n        if not plan or not plan.tasks or not plan.roles:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        print(f\"\\n\\nInitial Plan: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n        self.session.add_artifact(\"initial_plan.json\", plan.model_dump_json(indent=4))\n\n        return plan\n\n    def run(self, plan: Plan, rounds: int = 1, files: List[File] = []) -> None:\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call start() before run().\")\n\n        logger.info(\"--- Starting Workflow ---\")\n\n        context = ExecutionContext(session=self.session, plan=plan)\n\n        print(f\"\\n\\nInitial Files: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n\n        #context.record_initial(\"files\", \"\\n\".join([file.model_dump_json(indent=4) for file in files]))\n        context.record_initial(\"files\", Artifact(task='initial',files=files).model_dump_json())\n\n        for context.round_num in range(1, rounds + 1):\n            logger.info(f\"System is starting workflow round {context.round_num} for goal: '{plan.high_level_goal}'\")\n\n            team_by_name = {agent.name: agent for agent in self.orchestrator.team.values()} if self.orchestrator.team else {}\n\n            context.reset()\n\n            while context.step_index < len(plan.tasks):\n                step = context.current_step()\n                agent_name = step.agent\n\n                delegate_agent = team_by_name.get(agent_name)\n                if not delegate_agent:\n                    delegate_agent = next((agent for agent in self.agents if agent.name.lower() == agent_name.lower()), None)\n\n                if not delegate_agent:\n                    logger.warning(f\"No agent found with name '{agent_name}'. Execution will continue with the Orchestrator as the fallback agent.\")\n                    delegate_agent = self.orchestrator\n\n                logger.info(f\"Agent '{delegate_agent.name}' is executing step {context.step_index + 1}/{len(plan.tasks)}: '{step.description}' (Role: {step.role})\")\n\n                result = delegate_agent.execute_task(context)\n                if result:\n                    context.record_artifact(f\"{delegate_agent.name}_result.txt\", result, True)\n                    try:\n                        agent_output = AgentOutput.model_validate_json(result)\n                        if agent_output.team and agent_output.team.prompts:\n                            logger.info(f\"Agent {delegate_agent.name} provided new prompts.\")\n                            for prompt_data in agent_output.team.prompts:\n                                self._update_agent_prompt(self.session, prompt_data.agent, prompt_data.system_prompt)\n                    except (json.JSONDecodeError, Exception) as e:\n                        logger.warning(f\"Could not parse agent output as AgentOutput: {e}. Treating as plain text.\")\n                context.next()\n            logger.info(f\"System has completed workflow round {context.round_num}.\")\n        logger.info(\"--- Workflow Complete ---\")\n\n    def _update_agent_prompt(self, session: Session, agent_name: str, new_prompt: str) -> None:\n        \"\"\"\n        Updates an agent's system prompt.\n\n        Args:\n            session (Session): The current session object.\n            agent_name (str): The name or role of the agent to update.\n            new_prompt (str): The new system prompt.\n        \"\"\"\n        if not (agent_name and new_prompt):\n            return\n\n        if not (self.orchestrator and self.orchestrator.team):\n            logger.warning(\"Orchestrator or its team is not initialized. Cannot update agent prompt.\")\n            return\n\n        # Combine team agents and session agents for a comprehensive search, ensuring uniqueness\n        all_agents = {agent.name: agent for agent in list(self.orchestrator.team.values()) + session.agents}.values()\n\n        # First, try to find by name\n        target_agent = next((agent for agent in all_agents if agent.name.lower() == agent_name.lower()), None)\n\n        # If not found by name, try to find by role\n        if not target_agent:\n            target_agent = next((agent for agent in all_agents if agent.role.lower() == agent_name.lower()), None)\n\n        if target_agent:\n            target_agent.update_system_prompt(new_prompt)\n            logger.info(f\"Agent '{target_agent.name}' (matched by '{agent_name}') system prompt updated.\")\n        else:\n            logger.warning(f\"Target agent '{agent_name}' not found for prompt update.\")\n\n    def _instantiate_agent(self, agent_spec: Dict[str, Any], prompts: Dict[str, str], all_agent_specs: List[Dict[str, Any]]) -> Optional[Agent]:\n        \"\"\"\n        Instantiates an Agent (or a subclass) based on the provided specification.\n\n        Args:\n            agent_spec (Dict[str, Any]): A dictionary containing the agent's specifications.\n            prompts (Dict[str, str]): A dictionary of available system prompts.\n            all_agent_specs (List[Dict[str, Any]]): A list of all agent specifications for team resolution.\n\n        Returns:\n            Optional[Agent]: An instantiated Agent object, or None if the prompt is not found.\n        \"\"\"\n        prompt_key = f\"{agent_spec['name'].lower()}_instructions.txt\"\n        if prompt_key not in prompts:\n            logger.warning(\n                f\"Prompt for agent '{agent_spec['name']}' not found with key '{prompt_key}'. Empty system prompt will be used.\"\n            )\n\n        agent = Orchestrator(\n            name=agent_spec.get(\"name\", \"Unnamed Agent\"),\n            role=agent_spec.get(\"role\", \"Agent\"),\n            goal=agent_spec.get(\"goal\", \"\"),\n            model=agent_spec.get(\"model\", \"default-model\"),\n            system_prompt=\"\" if prompt_key not in prompts else prompts[prompt_key],\n        )\n\n        if agent_spec.get(\"delegation\") and \"team\" in agent_spec:\n            agent.team = {}\n            for team_member_name in agent_spec[\"team\"]:\n                member_spec = next(\n                    (\n                        s\n                        for s in all_agent_specs\n                        if s[\"name\"].lower() == team_member_name.lower()\n                    ),\n                    None,\n                )\n                if member_spec:\n                    member_agent = self._instantiate_agent(\n                        member_spec, prompts, all_agent_specs\n                    )  # Recursive call\n                else:\n                    logger.warning(\n                        f\"Team member '{team_member_name}' not found in agent specs.\"\n                    )\n                    member_agent = self._instantiate_agent(\n                        agent_spec={\n                            \"name\": team_member_name,\n                            \"role\": \"Unknown\",\n                            \"goal\": \"\",\n                            \"model\": agent_spec.get(\"model\", \"default-model\"),\n                            \"system_prompt\": \"\",\n                        },\n                        prompts=prompts,\n                        all_agent_specs=all_agent_specs,\n                    )  # Recursive call\n                if member_agent:\n                    agent.team[team_member_name] = member_agent\n        return agent\n\n    def _load_config(self, config_path: str) -> Any:\n        \"\"\"\n        Loads the runtime configuration from a YAML file.\n\n        Args:\n            config_path (str): The absolute path to the configuration file.\n\n        Returns:\n            dict: The loaded configuration as a dictionary.\n        \"\"\"\n        logger.info(f\"Loading configuration from: {config_path}\")\n        with open(config_path, 'r', encoding='utf-8') as f:\n            return yaml.safe_load(f)\n\n    def _load_agent_templates(self, agents_dir: str, config: dict, default_model: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Loads all agent specifications from YAML files within a specified directory.\n\n        Args:\n            agents_dir (str): The absolute path to the directory containing agent YAML files.\n\n        Returns:\n            list: A list of dictionaries, where each dictionary represents an agent's specification.\n        \"\"\"\n        logger.info(f\"Loading agent templates from: {agents_dir}\")\n        agent_files = glob(os.path.join(agents_dir, '*.yaml'))\n        templates = []\n        for agent_file in agent_files:\n            with open(agent_file, 'r', encoding='utf-8') as f:\n                template = yaml.safe_load(f)\n                # Resolve prompt path relative to the agent file's directory\n                if 'system_prompt' in template and template['system_prompt']:\n                    base_dir = os.path.dirname(agent_file)\n                    prompt_path = os.path.abspath(os.path.join(base_dir, template['system_prompt']))\n                    template['system_prompt_path'] = prompt_path\n                template[\"model\"] = default_model\n                templates.append(template)\n        logger.info(f\"{len(templates)} agent templates loaded.\")\n        return templates\n\n    def _load_prompts(self, prompts_dir: str) -> Dict[str, str]:\n        \"\"\"\n        Loads all system prompts from text files within a specified directory.\n\n        Args:\n            prompts_dir (str): The absolute path to the directory containing prompt text files.\n\n        Returns:\n            dict: A dictionary where keys are prompt filenames (e.g., 'agent_instructions.txt')\n                and values are the content of the prompt files.\n        \"\"\"\n        logger.info(f\"Loading prompts from: {prompts_dir}\")\n        prompt_files = glob(os.path.join(prompts_dir, '*.txt'))\n        prompts = {}\n        for prompt_file in prompt_files:\n            with open(prompt_file, 'r', encoding='utf-8') as f:\n                # Use the absolute path as the key for reliable lookup\n                prompts[os.path.basename(prompt_file)] = f.read()\n        logger.info(f\"{len(prompts)} prompts loaded.\")\n        return prompts\n"},{"path":"runtime/util.py","content":"\"\"\"This module provides miscellaneous utility functions for the runtime.\n\nIt includes helper functions that are used across different modules\nbut do not belong to a specific component.\n\"\"\"\n\nimport os\n\ndef read_file(file_path: str) -> str:\n    \"\"\"Reads content from a given file path.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except FileNotFoundError:\n        return f\"Error: File not found at {file_path}\"\n    except Exception as e:\n        return f\"Error reading file {file_path}: {e}\"\n"}]}

--- Artifact '__1/__step_2_Lyra_result.txt' from (Lyra) in [T3]:
{"output":"Defined the high-level architecture for the TypeScript Node.js runtime based on the extracted Task Agnostic Steps (TAS). This includes identifying key modules, classes, and their interactions, adhering to SOTA principles.","artifact":{"task":"T3","files":[{"path":"src/index.ts","content":"import { System } from './system/System';\nimport { Agent } from './agent/Agent';\nimport { Orchestrator } from './orchestrator/Orchestrator';\nimport { Session } from './core/Session';\nimport { ExecutionContext } from './core/ExecutionContext';\nimport { LLM } from './llm/LLM';\nimport { Logger } from './logger/Logger';\nimport { Config } from './config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from './custom_types';\n\nexport {\n  System,\n  Agent,\n  Orchestrator,\n  Session,\n  ExecutionContext,\n  LLM,\n  Logger,\n  Config,\n  Plan,\n  Task,\n  Artifact,\n  Role,\n  Prompt,\n  Team,\n  AgentOutput,\n  File\n};\n\n// Export default logger instance\nexport const logger = Logger.getInstance();\n\n// Example usage (can be moved to a CLI entry point like sysmain.ts later)\nasync function main() {\n  logger.info('Starting runtime initialization...');\n\n  try {\n    // 1. Load Configuration\n    const config = await Config.load('runtime.yaml'); // Assuming runtime.yaml is in the root or a config dir\n    logger.info('Configuration loaded successfully.');\n\n    // 2. Initialize System\n    const system = new System(config.projectRoot || '.', config.defaultModel || 'gemini-2.5-flash-lite');\n    await system.setup(); // Load agents, prompts, etc.\n    logger.info('System setup complete.');\n\n    // 3. Define a high-level goal and initial files (example)\n    const highLevelGoal = 'Reimplement runtime using TypeScript in Node.js following SOTA coding principles';\n    const initialFiles: File[] = [\n      // Example: { path: 'docs/runtime.md', content: '...' } \n    ];\n\n    // 4. Start the workflow to generate a plan\n    const plan = await system.start(highLevelGoal, initialFiles);\n    logger.info('Workflow plan generated.');\n    console.log('\\n--- Generated Plan ---');\n    console.log(JSON.stringify(plan, null, 2));\n    console.log('----------------------\\n');\n\n    // 5. Run the workflow for a specified number of rounds\n    const rounds = config.workflowRounds || 3;\n    await system.run(plan, rounds, initialFiles);\n    logger.info('Workflow execution finished.');\n\n  } catch (error) {\n    logger.error('Runtime initialization failed:', error);\n  }\n}\n\n// Uncomment to run the example main function\n// main();\n"},{"path":"src/system/System.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { LLM } from '../llm/LLM';\nimport { Logger } from '../logger/Logger';\nimport { Config } from '../config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class System {\n  private config: Config;\n  private agents: Agent[] = [];\n  private session: Session | null = null;\n  private orchestrator: Orchestrator | null = null;\n  private readonly rootDir: string;\n  private readonly defaultModel: string;\n\n  constructor(rootDir: string, defaultModel: string = 'gemini-2.5-flash-lite') {\n    this.rootDir = rootDir;\n    this.defaultModel = defaultModel;\n    logger.info(`System initialized with root directory: ${this.rootDir} and default model: ${this.defaultModel}`);\n  }\n\n  async setup(): Promise<void> {\n    logger.info('--- System Setup ---');\n    this.config = await Config.load('runtime.yaml', this.rootDir);\n    logger.info('Configuration loaded.');\n\n    const agentSpecs = this._loadAgentSpecs(join(this.rootDir, this.config.agentDir || 'agents'));\n    const prompts = this._loadPrompts(join(this.rootDir, this.config.promptDir || 'prompts'));\n\n    const allTeamMembers = new Set<string>();\n    agentSpecs.forEach(spec => {\n      spec.team?.forEach(member => allTeamMembers.add(member.name.toLowerCase()));\n    });\n\n    // Instantiate agents, excluding those already defined as part of another agent's team\n    for (const spec of agentSpecs) {\n      if (!allTeamMembers.has(spec.name.toLowerCase())) {\n        const agent = this._instantiateAgent(spec, prompts, agentSpecs);\n        if (agent) {\n          this.agents.push(agent);\n        }\n      }\n    }\n\n    if (this.agents.length === 0) {\n      throw new Error('No agents could be instantiated. System setup failed.');\n    }\n\n    // Find and set the orchestrator\n    const orchestratorSpec = this.agents.find(\n      agent => agent.role.toLowerCase() === 'orchestrator'\n    );\n    if (!orchestratorSpec) {\n      throw new Error('Orchestrator agent not found. System setup failed.');\n    }\n    this.orchestrator = orchestratorSpec as Orchestrator;\n    logger.info(`Orchestrator set to: ${this.orchestrator.name}`);\n\n    // Initialize session\n    this.session = new Session(this.agents, this.rootDir);\n    logger.info('Session initialized.');\n\n    logger.info('--- System Setup Complete ---');\n  }\n\n  async start(highLevelGoal: string, initialFiles: File[] = []): Promise<Plan> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call setup() before start().');\n    }\n\n    logger.info(`Generating plan for goal: \"${highLevelGoal}\"`);\n    const plan = await this.orchestrator.generatePlan(this.session, highLevelGoal, initialFiles);\n\n    if (!plan) {\n      throw new Error('Orchestration failed: Could not generate a valid plan.');\n    }\n\n    logger.info('Plan generated successfully.');\n    this.session.addArtifact('initial_plan.json', JSON.stringify(plan.toJSON(), null, 2));\n    logger.debug(`Plan details: ${JSON.stringify(plan)}`);\n\n    return plan;\n  }\n\n  async run(plan: Plan, rounds: number = 1, initialFiles: File[] = []): Promise<void> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call start() before run().');\n    }\n\n    logger.info(`--- Starting Workflow (Rounds: ${rounds}) ---`);\n\n    const context = new ExecutionContext(this.session, plan);\n    context.recordInitial('files', new Artifact({ task: 'initial', files: initialFiles }));\n\n    for (let roundNum = 1; roundNum <= rounds; roundNum++) {\n      context.roundNum = roundNum;\n      context.resetStepIndex();\n      logger.info(`Starting workflow round ${roundNum} for goal: \"${plan.highLevelGoal}\"`);\n\n      while (context.stepIndex < plan.tasks.length) {\n        const currentTask = context.currentStep();\n        let delegateAgent = this.agents.find(agent => agent.name.toLowerCase() === currentTask.agent.toLowerCase());\n\n        if (!delegateAgent) {\n          logger.warn(`Agent '${currentTask.agent}' not found for task '${currentTask.description}'. Falling back to Orchestrator.`);\n          delegateAgent = this.orchestrator;\n        }\n\n        logger.info(`Executing step ${context.stepIndex + 1}/${plan.tasks.length}: '${currentTask.description}' by Agent '${delegateAgent.name}' (Role: ${currentTask.role})`);\n\n        try {\n          const agentOutput = await delegateAgent.executeTask(context);\n          if (agentOutput) {\n            context.recordArtifact(`${delegateAgent.name}_result.txt`, agentOutput.output, true);\n            if (agentOutput.artifact) {\n              agentOutput.artifact.files.forEach(file => {\n                this.session?.addArtifact(file.path, file.content);\n                console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n              });\n            }\n            if (agentOutput.team?.prompts) {\n              logger.info(`Agent '${delegateAgent.name}' provided ${agentOutput.team.prompts.length} prompt updates.`);\n              for (const promptData of agentOutput.team.prompts) {\n                this._updateAgentPrompt(promptData.agent, promptData.systemPrompt);\n              }\n            }\n          }\n        } catch (error) {\n          logger.error(`Error executing task '${currentTask.description}' by agent '${delegateAgent.name}':`, error);\n          // Decide on error handling strategy: stop, retry, or continue\n          // For now, we log and continue to the next task.\n        }\n        context.nextStep();\n      }\n      logger.info(`Completed workflow round ${roundNum}.`);\n    }\n\n    logger.info('--- Workflow Complete ---');\n  }\n\n  private _updateAgentPrompt(agentName: string, newPrompt: string): void {\n    const targetAgent = this.agents.find(agent => agent.name.toLowerCase() === agentName.toLowerCase());\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Updated system prompt for agent '${targetAgent.name}'.`);\n    } else {\n      logger.warn(`Agent '${agentName}' not found for prompt update.`);\n    }\n  }\n\n  private _instantiateAgent(spec: AgentSpec, prompts: Record<string, string>, allAgentSpecs: AgentSpec[]): Agent | null {\n    const promptKey = `${spec.name.toLowerCase()}_instructions.txt`;\n    const systemPrompt = prompts[promptKey] || spec.systemPrompt || ''; // Use provided systemPrompt if available, else default to empty\n\n    const model = spec.model || this.defaultModel;\n\n    let agent: Agent;\n\n    // Special handling for Orchestrator to ensure it's an Orchestrator instance\n    if (spec.role.toLowerCase() === 'orchestrator') {\n      agent = new Orchestrator(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n      // Pass team members to Orchestrator\n      if (spec.team && spec.team.length > 0) {\n        (agent as Orchestrator).team = this._buildTeam(spec.team, prompts, allAgentSpecs);\n      }\n    } else {\n      agent = new Agent(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n    }\n\n    logger.info(`Instantiated Agent: ${agent.name} (Role: ${agent.role}, Model: ${agent.model})`);\n    return agent;\n  }\n\n  private _buildTeam(teamSpecs: PromptSpec[], allAgentSpecs: AgentSpec[], depth = 0): Record<string, Agent> {\n    const team: Record<string, Agent> = {};\n    const maxDepth = 5; // Prevent infinite recursion\n\n    if (depth > maxDepth) {\n      logger.error('Maximum team nesting depth reached. Stopping team creation.');\n      return {};\n    }\n\n    for (const teamSpec of teamSpecs) {\n      const memberSpec = allAgentSpecs.find(spec => spec.name.toLowerCase() === teamSpec.name.toLowerCase());\n      if (memberSpec) {\n        const memberAgent = this._instantiateAgent(memberSpec, this.loadPrompts(join(this.rootDir, this.config.promptDir || 'prompts')), allAgentSpecs, depth + 1);\n        if (memberAgent) {\n          team[teamSpec.name] = memberAgent;\n        } else {\n          logger.warn(`Failed to instantiate team member '${teamSpec.name}'. Skipping.`);\n        }\n      } else {\n        logger.warn(`Team member specification for '${teamSpec.name}' not found. Creating a placeholder agent.`);\n        // Create a placeholder agent if spec is not found\n        const placeholderAgent = new Agent(\n          teamSpec.name,\n          teamSpec.role || 'Unknown Role',\n          teamSpec.goal || 'No specific goal defined',\n          this.defaultModel,\n          '' // No prompt for placeholder\n        );\n        team[teamSpec.name] = placeholderAgent;\n      }\n    }\n    return team;\n  }\n\n  // Helper to load prompts, ensuring it can be called recursively\n  private loadPrompts(promptsDir: string): Record<string, string> {\n    return this._loadPrompts(promptsDir);\n  }\n\n  private _loadAgentSpecs(agentsDir: string): AgentSpec[] {\n    logger.info(`Loading agent specifications from: ${agentsDir}`);\n    const agentFiles = globSync(join(agentsDir, '*.yaml')); // Use globSync for simplicity here\n    const specs: AgentSpec[] = [];\n    agentFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        const spec = yaml.load(content) as AgentSpec;\n        // Resolve prompt path relative to the agent file's directory\n        if (spec.systemPromptPath) {\n          const baseDir = dirname(filePath);\n          spec.systemPrompt = readFileSync(join(baseDir, spec.systemPromptPath), 'utf-8');\n        }\n        spec.model = spec.model || this.defaultModel;\n        specs.push(spec);\n        logger.debug(`Loaded agent spec: ${spec.name}`);\n      } catch (error) {\n        logger.error(`Error loading agent spec from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${specs.length} agent specifications loaded.`);\n    return specs;\n  }\n\n  private _loadPrompts(promptsDir: string): Record<string, string> {\n    logger.info(`Loading prompts from: ${promptsDir}`);\n    const promptFiles = globSync(join(promptsDir, '*.txt'));\n    const prompts: Record<string, string> = {};\n    promptFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        const fileName = basename(filePath);\n        prompts[fileName] = content;\n        logger.debug(`Loaded prompt: ${fileName}`);\n      } catch (error) {\n        logger.error(`Error loading prompt from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${Object.keys(prompts).length} prompts loaded.`);\n    return prompts;\n  }\n}\n\n// Mock implementations for dependencies used in System class for demonstration purposes\n// In a real project, these would be imported from their respective modules.\n\n// Mock AgentSpec and PromptSpec interfaces (should be defined in Agent.ts)\ninterface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string;\n  team?: PromptSpec[];\n}\n\ninterface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\n// Mock necessary imports from 'fs' and 'path' if not using a full Node.js environment setup\n// For example, using 'glob' and 'js-yaml' would require their respective imports and installations.\n\n// Placeholder for glob and yaml imports - in a real project, install and import them:\n// import { globSync } from 'glob';\n// import * as yaml from 'js-yaml';\n// import { readFileSync } from 'fs';\n// import { join, basename, dirname } from 'path';\n\n// Dummy implementations for globSync, yaml.load, readFileSync, join, basename, dirname\nconst globSync = (pattern: string): string[] => {\n  logger.warn('Using dummy globSync implementation. Install "},{"path":"src/agent/Agent.ts","content":"import { LLM } from '../llm/LLM';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, Artifact, AgentOutput, File, Team } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string;\n  team?: PromptSpec[];\n}\n\nexport interface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  protected systemPrompt: string;\n  protected llm: LLM;\n  protected model: string;\n  public team: Record<string, Agent> = {}; // For Orchestrator's delegation\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    this.llm = LLM.factory(model);\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Model: ${this.model})`);\n  }\n\n  updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    const currentTask = context.currentStep();\n    const previousArtifactsContent = await this._getPreviousArtifacts(context);\n\n    const taskPrompt = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${currentTask.description}'`,\n      `The team's roles are:\n${JSON.stringify(context.plan.roles.map((role: Role) => ({ title: role.title, purpose: role.purpose })), null, 2)}`,\n    ].join('\\n\\n');\n\n    let fullPrompt = taskPrompt;\n    if (previousArtifactsContent) {\n      fullPrompt += `\\n\\nPlease use the following outputs from other agents as your input:\\n\\n${previousArtifactsContent}\\n\\n`;\n    }\n    fullPrompt += `Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project. Respond in a JSON format that includes 'output', and optionally 'artifact' and 'team' fields.`\n\n    logger.debug(`Constructed prompt for ${this.name} for task ${currentTask.id}:\n${fullPrompt}`);\n\n    // Record the prompt used for this task\n    context.recordArtifact(`${this.name}_prompt_${currentTask.id}.txt`, fullPrompt);\n\n    try {\n      const response = await this.llm.generateContent({\n        modelName: this.model,\n        contents: fullPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: AgentOutput,\n        responseMimeType: 'application/json',\n      });\n\n      if (!response) {\n        logger.error(`LLM returned no response for task ${currentTask.id} by ${this.name}.`);\n        return null;\n      }\n\n      // Ensure the response is parsed as AgentOutput\n      if (typeof response === 'string') {\n          try {\n              const parsedOutput = AgentOutput.parse(JSON.parse(response));\n              logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n              this._processAndRecordArtifacts(context, parsedOutput);\n              return parsedOutput;\n          } catch (parseError) {\n              logger.error(`Failed to parse LLM response as AgentOutput for task ${currentTask.id} by ${this.name}:`, parseError);\n              logger.error(`Raw LLM response: ${response}`);\n              // Attempt to record raw output if it's not a valid JSON\n              context.recordArtifact(`${this.name}_raw_output_${currentTask.id}.txt`, response);\n              return new AgentOutput({ output: `Error processing response: ${response}`, reasoning: 'LLM response was not valid JSON or AgentOutput schema.' });\n          }\n      } else if (response instanceof AgentOutput) {\n          logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n          this._processAndRecordArtifacts(context, response);\n          return response;\n      }\n\n      // Fallback for unexpected response types\n      logger.warn(`Unexpected response type from LLM for task ${currentTask.id} by ${this.name}: ${typeof response}`);\n      return new AgentOutput({ output: `Unexpected response type: ${JSON.stringify(response)}`, reasoning: 'LLM returned an unexpected data type.' });\n\n    } catch (error) {\n      logger.error(`Error during LLM interaction for task ${currentTask.id} by ${this.name}:`, error);\n      // Record the error as an artifact\n      context.recordArtifact(`${this.name}_error_${currentTask.id}.txt`, `Error: ${error.message}`);\n      return new AgentOutput({ output: `Error executing task: ${error.message}`, reasoning: 'LLM interaction failed.' });\n    }\n  }\n\n  private async _getPreviousArtifacts(context: ExecutionContext): Promise<string> {\n    const requiredTaskIds = ['initial', ...context.currentStep().requires];\n    const relevantArtifacts: string[] = [];\n\n    // Iterate through all recorded items in the context\n    for (const [key, contextItem] of Object.entries(context.items)) {\n      if (requiredTaskIds.includes(contextItem.step.id)) {\n        let artifactContent = '';\n        if (typeof contextItem.content === 'string') {\n          artifactContent = contextItem.content;\n        } else if (contextItem.content instanceof Artifact) {\n          artifactContent = contextItem.content.files.map(file => `--- File: ${file.path} ---\\n${file.content}`).join('\\n');\n        } else {\n          try {\n            artifactContent = JSON.stringify(contextItem.content, null, 2);\n          } catch (e) {\n            artifactContent = String(contextItem.content); // Fallback to string conversion\n          }\n        }\n        relevantArtifacts.push(`--- Artifact '${key}' from (${contextItem.step.role}) in [${contextItem.step.id}]:\\n${artifactContent}`);\n      }\n    }\n\n    return relevantArtifacts.join('\\n\\n');\n  }\n\n  private _processAndRecordArtifacts(context: ExecutionContext, agentOutput: AgentOutput): void {\n    // Record the main output\n    context.recordArtifact(`${this.name}_output.txt`, agentOutput.output, true);\n\n    // Record artifacts if present\n    if (agentOutput.artifact && agentOutput.artifact.files) {\n      agentOutput.artifact.files.forEach(file => {\n        context.session?.addArtifact(file.path, file.content);\n        console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n        // Record file artifact in context if it's meant to be remembered\n        // For simplicity, we'll assume all files in artifact are relevant for context\n        context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); // Record the whole artifact object for context\n      });\n    }\n\n    // Record reasoning if present\n    if (agentOutput.reasoning) {\n      context.recordArtifact(`${this.name}_reasoning.txt`, agentOutput.reasoning, true);\n    }\n\n    // Handle team updates (e.g., new prompts)\n    if (agentOutput.team && agentOutput.team.prompts) {\n      logger.info(`Agent ${this.name} provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n      // The System class will handle applying these updates to the agents\n    }\n  }\n}\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Role, AgentOutput } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  // Orchestrator might have a more complex team structure or delegation logic\n  // For now, we'll keep it simple and assume 'team' is managed by the System class\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name}`);\n  }\n\n  async generatePlan(session: Session, highLevelGoal: string, initialFiles: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator '${this.name}' is generating a plan for: \"${highLevelGoal}\"`);\n\n    if (!this.llm) {\n      logger.error('LLM client not initialized for Orchestrator.');\n      return null;\n    }\n\n    // Get available agents and their details for the prompt\n    const availableAgents = session.agents.map(agent => ({\n      name: agent.name,\n      role: agent.role,\n      goal: agent.goal,\n    }));\n\n    const agentDescription = availableAgents.map(agent => \n      `- Name: '${agent.name}'\\n  Role: \n${agent.role}\n  Goal: \"${agent.goal}\"`\n    ).join('\\n');\n\n    // Load the planning prompt template\n    // In a real scenario, this path would be relative to the project root or a config dir\n    const promptTemplatePath = join(__dirname, '..', '..', 'prompts', 'general_planning.txt');\n    let promptTemplate: string;\n    try {\n      promptTemplate = readFileSync(promptTemplatePath, 'utf-8');\n    } catch (error) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}:`, error);\n      return null;\n    }\n\n    const planningPrompt = promptTemplate.format({\n      highLevelGoal: highLevelGoal,\n      teamDescription: agentDescription,\n      initialFiles: initialFiles.map(f => `--- File: ${f.path}\\n```\\n${f.content}\\n````).join('\\n\\n')\n    });\n\n    logger.debug(`Constructed planning prompt for Orchestrator:\n${planningPrompt}`);\n    session.addArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      const response = await this.llm.generateContent({\n        modelName: this.model,\n        contents: planningPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: Plan,\n        responseMimeType: 'application/json',\n        temperature: 0.1, // Lower temperature for more deterministic planning\n      });\n\n      if (!response) {\n        logger.error('LLM returned no response for plan generation.');\n        return null;\n      }\n\n      // Ensure the response is parsed as Plan\n      if (typeof response === 'string') {\n        try {\n          const plan = Plan.parse(JSON.parse(response));\n          return plan;\n        } catch (parseError) {\n          logger.error('Failed to parse LLM response as Plan:', parseError);\n          logger.error(`Raw LLM response: ${response}`);\n          session.addArtifact('plan_generation_error.txt', `LLM response was not valid JSON or Plan schema: ${response}`);\n          return null;\n        }\n      } else if (response instanceof Plan) {\n        return response;\n      }\n\n      logger.error(`Unexpected response type from LLM during plan generation: ${typeof response}`);\n      return null;\n\n    } catch (error) {\n      logger.error('Error during plan generation:', error);\n      session.addArtifact('plan_generation_error.txt', `Error: ${error.message}`);\n      return null;\n    }\n  }\n\n  // Override executeTask to potentially handle plan execution logic if needed, or delegate\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    // For the Orchestrator, executeTask might be used to refine the plan or handle meta-tasks.\n    // In this architecture, the System class drives the task execution based on the plan.\n    // If the Orchestrator is assigned a specific task in the plan, it needs to handle it.\n    logger.warn(`Orchestrator '${this.name}' executeTask called directly. This is usually handled by the System. Task: '${context.currentStep().description}'`);\n    \n    // Default behavior: delegate to the base Agent class to process the task prompt\n    // This might be useful if the orchestrator needs to perform a specific task defined in the plan\n    // e.g., \"Refine the plan based on new information\".\n    return super.executeTask(context);\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Artifact, File } from '../custom_types';\nimport { join, dirname as getDirname } from 'path';\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'fs';\nimport * as yaml from 'js-yaml';\n\nconst logger = Logger.getInstance();\n\nexport class Session {\n  public readonly sessionId: string;\n  public readonly agents: Agent[];\n  public state: string;\n  public readonly sessionDir: string;\n  public readonly projectRoot: string;\n\n  constructor(agents: Agent[], projectRoot: string, sessionId?: string) {\n    this.sessionId = sessionId || `session_${Date.now().toString()}`;\n    this.agents = agents;\n    this.state = 'initialized';\n    this.projectRoot = projectRoot;\n    this.sessionDir = join(this.projectRoot, 'sessions', this.sessionId);\n\n    if (!existsSync(this.sessionDir)) {\n      mkdirSync(this.sessionDir, { recursive: true });\n    }\n    logger.info(`Session created: ${this.sessionId} (Directory: ${this.sessionDir})`);\n  }\n\n  addArtifact(name: string, content: any): void {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      const dir = getDirname(artifactPath);\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n      // Determine if content should be saved as JSON or plain text\n      let contentToWrite: string;\n      if (typeof content === 'object' && content !== null && !(content instanceof File) && !(content instanceof Artifact)) {\n        // If it's a plain object (like from AgentOutput.toJSON()), save as JSON\n        contentToWrite = JSON.stringify(content, null, 2);\n      } else if (typeof content === 'string') {\n        contentToWrite = content;\n      } else {\n        // Fallback for other types, convert to string\n        contentToWrite = String(content);\n      }\n\n      writeFileSync(artifactPath, contentToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (e) {\n      logger.error(`Error saving artifact '${name}' in session ${this.sessionId}:`, e);\n    }\n  }\n\n  getArtifact(name: string): any {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      if (existsSync(artifactPath)) {\n        const content = readFileSync(artifactPath, 'utf-8');\n        if (name.endsWith('.json')) {\n          try {\n            return JSON.parse(content);\n          } catch (e) {\n            logger.error(`Error parsing JSON artifact '${name}':`, e);\n            return null;\n          }\n        }\n        return content;\n      } else {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n        return null;\n      }\n    } catch (e) {\n      logger.error(`Error retrieving artifact '${name}' from session ${this.sessionId}:`, e);\n      return null;\n    }\n  }\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface ContextItem {\n  name: string;\n  content: any;\n  step: Task;\n}\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores artifacts remembered from previous steps/rounds\n\n  constructor(session: Session, plan: Plan) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = 0;\n    this.roundNum = 1;\n    this.items = {};\n    logger.info('ExecutionContext initialized.');\n  }\n\n  nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  resetStepIndex(): void {\n    this.stepIndex = 0;\n    logger.debug('Step index reset to 0.');\n  }\n\n  currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error('Step index out of bounds. No more tasks in the plan.');\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  // Records an artifact for the current step and optionally remembers it for future steps\n  recordArtifact(key: string, value: any, remember: boolean = false): void {\n    const artifactKey = `__${this.roundNum}/__step_${this.stepIndex}_${key}`;\n    this.session.addArtifact(artifactKey, value);\n    if (remember) {\n      this._rememberItem(artifactKey, value, this.currentStep());\n    }\n  }\n\n  // Records initial artifacts provided at the start of the workflow\n  recordInitial(key: string, value: any): void {\n    // Use a special step ID for initial items\n    const initialTask: Task = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: []\n    });\n    this._rememberItem(key, value, initialTask);\n    // Also add to session directly if it's a file artifact\n    if (value instanceof Artifact && value.files) {\n        value.files.forEach(file => this.session.addArtifact(file.path, file.content));\n    }\n    logger.debug(`Recorded initial item '${key}'.`);\n  }\n\n  // Helper to store items in memory (context.items)\n  private _rememberItem(name: string, content: any, step: Task): void {\n    this.items[name] = { name, content, step };\n    logger.debug(`Item '${name}' remembered in context.`);\n  }\n}\n"},{"path":"src/llm/LLM.ts","content":"import { BaseModel } from 'pydantic';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface for the LLM generation arguments\nexport interface LLMGenerateContentArgs {\n  modelName: string;\n  contents: string;\n  systemInstruction?: string;\n  temperature?: number;\n  responseMimeType?: 'text/plain' | 'application/json' | 'application/json-schema';\n  responseSchema?: typeof BaseModel;\n}\n\nexport abstract class LLM {\n  protected abstract generateContentImpl(args: LLMGenerateContentArgs): Promise<any>;\n\n  /**\n   * Abstract method to generate content using an LLM.\n   * Handles common logic like retries and response parsing.\n   */\n  public async generateContent(args: LLMGenerateContentArgs): Promise<any> {\n    const { modelName, responseSchema, responseMimeType } = args;\n    const maxRetries = 3;\n    let lastError: Error | null = null;\n\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        logger.info(`Calling LLM model '${modelName}' (Attempt ${attempt}/${maxRetries})...`);\n        const response = await this.generateContentImpl(args);\n\n        if (!response) {\n          throw new Error('LLM returned an empty response.');\n        }\n\n        // Parse response based on mime type and schema\n        if (responseSchema && responseMimeType === 'application/json') {\n          if (typeof response === 'string') {\n            try {\n              const parsedJson = JSON.parse(response);\n              // Validate against the provided Pydantic schema\n              const validatedData = responseSchema.parse(parsedJson);\n              logger.debug(`LLM response successfully parsed and validated against schema ${responseSchema.name}.`);\n              return validatedData;\n            } catch (e) {\n              lastError = new Error(`Failed to parse or validate LLM JSON response with schema ${responseSchema.name}: ${e.message}`);\n              logger.error(lastError.message);\n              // Do not retry on parsing errors, as the response itself is likely malformed\n              throw lastError;\n            }\n          } else if (response instanceof responseSchema) {\n            logger.debug(`LLM response is already an instance of ${responseSchema.name}.`);\n            return response; // Response is already the correct type\n          } else {\n             lastError = new Error(`LLM response was not a string or expected schema type for JSON request.`);\n             logger.error(lastError.message);\n             throw lastError;\n          }\n        } else if (responseSchema && responseMimeType === 'application/json-schema') {\n            // Handle JSON schema response type if needed\n             if (typeof response === 'string') {\n                try {\n                    // Assuming the schema is directly provided and needs parsing\n                    const parsedSchema = JSON.parse(response);\n                    // Further validation or processing might be needed here based on specific schema requirements\n                    logger.debug('LLM response parsed as JSON Schema.');\n                    return parsedSchema;\n                } catch (e) {\n                     lastError = new Error(`Failed to parse LLM JSON Schema response: ${e.message}`);\n                     logger.error(lastError.message);\n                     throw lastError;\n                }\n            }\n        }\n\n        // Default handling for text or un-schematized JSON\n        logger.debug('LLM response processed.');\n        return response;\n\n      } catch (error) {\n        lastError = error;\n        logger.error(`LLM generation failed (Attempt ${attempt}): ${error.message}`);\n        if (attempt < maxRetries) {\n          const delay = Math.pow(2, attempt) * 1000; // Exponential backoff\n          logger.info(`Waiting for ${delay / 1000} seconds before retrying...`);\n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n\n    logger.error(`LLM generation failed after ${maxRetries} attempts. Last error: ${lastError?.message}`);\n    return null; // Return null if all retries fail\n  }\n\n  /**\n   * Factory method to create an LLM instance based on the species string.\n   * Example species: 'gemini:gemini-pro', 'ollama:llama3', 'openai:gpt-4'.\n   */\n  static factory(species: string): LLM {\n    logger.info(`LLM Factory: Creating LLM instance for species '${species}'`);\n    // TODO: Implement actual factory logic based on species string\n    // This is a placeholder and needs to be replaced with concrete implementations\n    // For now, returning a dummy LLM or a default one.\n    if (species.startsWith('ollama:')) {\n        const modelName = species.split(':')[1];\n        // return new OllamaLLM(modelName);\n        logger.warn('OllamaLLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName); \n    }\n    if (species.startsWith('gemini:') || species === 'gemini-2.5-flash-lite') {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new GeminiLLM(modelName);\n        logger.warn('GeminiLLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName);\n    }\n     if (species.startsWith('openai:') || species.startsWith('gpt:') || species.startsWith('opi:')) {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new OpenAILLM(modelName);\n         logger.warn('OpenAILLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('mistral:')) {\n        const modelName = species.split(':')[1];\n        // return new MistralLLM(modelName);\n         logger.warn('MistralLLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName);\n    }\n    // Default or fallback\n    logger.warn(`Unknown LLM species '${species}'. Using DummyLLM as fallback.`);\n    return new DummyLLM(species);\n  }\n}\n\n// Dummy LLM implementation for placeholder purposes\nclass DummyLLM extends LLM {\n  constructor(private modelName: string) {\n    super();\n    logger.warn(`DummyLLM initialized with model: ${this.modelName}. This is a placeholder and will not perform actual LLM calls.`);\n  }\n\n  protected async generateContentImpl(args: LLMGenerateContentArgs): Promise<any> {\n    logger.warn(`DummyLLM: generateContentImpl called for model ${this.modelName}. Returning mock response.`);\n    // Simulate a response based on the schema if provided, otherwise return a generic string\n    if (args.responseSchema) {\n        try {\n            const schemaInstance = args.responseSchema.prototype;\n            const mockResponse = {}; // Create a mock object based on schema properties\n            logger.debug(`Generating mock response for schema: ${args.responseSchema.name}`);\n            // This is a very basic mock, a more sophisticated one would inspect the schema\n            if (args.responseSchema === AgentOutput) {\n                return new AgentOutput({\n                    output: `Mock output for ${this.modelName} task: ${args.contents.substring(0, 50)}...`,\n                    reasoning: 'This is a mock response from DummyLLM.',\n                    artifact: new Artifact({ task: 'mock_task', files: [new File({ path: 'mock_output.txt', content: 'Mock file content.'})]}),\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            } else if (args.responseSchema === Plan) {\n                 return new Plan({\n                    highLevelGoal: 'Mock Goal',\n                    reasoning: 'Mock reasoning for plan generation',\n                    roles: [{title: 'Mock Role', purpose: 'Mock Purpose'}],\n                    tasks: [{id: 'T1', description: 'Mock Task', role: 'Mock Role', agent: 'Mock Agent', requires: []}],\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            }\n            // Add more schema handlers as needed\n            return schemaInstance;\n        } catch (e) {\n            logger.error(`Error creating mock response for schema ${args.responseSchema.name}:`, e);\n            return `Mock response failed for schema ${args.responseSchema.name}.`;\n        }\n    }\n    \n    return `This is a mock response from DummyLLM for model ${this.modelName}.`;\n  }\n}\n"},{"path":"src/logger/Logger.ts","content":"import winston from 'winston';\nimport { join } from 'path';\nimport fs from 'fs';\n\n// Define log levels\nconst logLevels = {\n  error: 0,\n  warn: 1,\n  info: 2,\n  debug: 3,\n};\n\n// Define log format\nconst logFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.printf(({ level, message, timestamp, ...metadata }) => {\n    let msg = `${timestamp} [${level.toUpperCase()}]: ${message}`;\n    if (Object.keys(metadata).length > 0) {\n      msg += ` ${JSON.stringify(metadata)}`;\n    }\n    return msg;\n  })\n);\n\nconst jsonLogFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.json()\n);\n\n// Ensure log directory exists\nconst logDir = process.env.LOG_DIR || 'logs';\nif (!fs.existsSync(logDir)) {\n  fs.mkdirSync(logDir, { recursive: true });\n}\n\nconst logFilePath = join(logDir, 'app.log');\nconst jsonLogFilePath = join(logDir, 'app.jsonl');\n\nexport class Logger {\n  private static instance: winston.Logger;\n\n  public static getInstance(): winston.Logger {\n    if (!Logger.instance) {\n      Logger.instance = winston.createLogger({\n        level: process.env.LOG_LEVEL || 'info',\n        levels: logLevels,\n        format: logFormat, // Default format for console\n        transports: [\n          // Console transport with colorization\n          new winston.transports.Console({\n            format: winston.format.combine(\n              winston.format.colorize({ all: true }),\n              logFormat\n            ),\n          }),\n          // File transport for general logs\n          new winston.transports.File({\n            filename: logFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: logFormat,\n            maxsize: 1024 * 1024 * 5, // 5MB\n            tailable: true,\n          }),\n          // File transport for JSON logs (for easier parsing by other tools)\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: jsonLogFormat,\n            maxsize: 1024 * 1024 * 5, // 5MB\n            tailable: true,\n          }),\n        ],\n      });\n      // Add a handler for uncaught exceptions\n      Logger.instance.on('error', (err) => {\n        console.error('Winston logger encountered an error:', err);\n      });\n    }\n    return Logger.instance;\n  }\n}\n"},{"path":"src/config/Config.ts","content":"import { readFileSync } from 'fs';\nimport { join } from 'path';\nimport yaml from 'js-yaml';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface Config {\n  projectRoot?: string;\n  agentDir?: string;\n  promptDir?: string;\n  logDir?: string;\n  defaultModel?: string;\n  workflowRounds?: number;\n  // Add other configuration properties as needed\n}\n\nexport class Config {\n  public static async load(fileName: string, basePath: string = process.cwd()): Promise<Config> {\n    const configPath = join(basePath, fileName);\n    logger.info(`Attempting to load configuration from: ${configPath}`);\n    try {\n      const fileContents = readFileSync(configPath, 'utf-8');\n      const config = yaml.load(fileContents) as Config;\n      // Set default values if not provided\n      config.projectRoot = config.projectRoot || basePath;\n      config.agentDir = config.agentDir || 'agents';\n      config.promptDir = config.promptDir || 'prompts';\n      config.logDir = config.logDir || 'logs';\n      config.defaultModel = config.defaultModel || 'gemini-2.5-flash-lite';\n      config.workflowRounds = config.workflowRounds || 3;\n\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Failed to load configuration from ${configPath}:`, error);\n      // Return a default config if file loading fails, or throw an error\n      // For now, returning a minimal default config\n      return {\n        projectRoot: basePath,\n        agentDir: 'agents',\n        promptDir: 'prompts',\n        logDir: 'logs',\n        defaultModel: 'gemini-2.5-flash-lite',\n        workflowRounds: 3,\n      };\n      // Or uncomment below to throw an error and stop execution:\n      // throw new Error(`Failed to load configuration file: ${configPath}`);\n    }\n  }\n}\n"},{"path":"src/custom_types.ts","content":"import { BaseModel, Field, model_validator } from 'pydantic';\nimport { z } from 'zod'; // Using Zod for schema validation demonstration\n\n// --- Task Agnostic Step (TAS) related Types ---\n\nexport class Task extends BaseModel {\n  id: string = Field(..., { description: \"Unique ID for the task (e.g., 'T1', 'T2').\" });\n  description: string = Field(..., { description: \"Detailed description of what to do for this task.\" });\n  role: string = Field(..., { description: \"The role responsible for this task.\" });\n  agent: string = Field(..., { description: \"The agent assigned to this task.\" });\n  requires: string[] = Field([], { description: \"List of tasks (IDs) whose outputs are required (e.g. ['T1', 'T2']).\" });\n}\n\nexport class File extends BaseModel {\n  path: string = Field(..., { description: \"File path or name (e.g., 'src/main.py').\" });\n  content: string = Field(..., { description: \"Full content of the file.\" });\n}\n\nexport class Artifact extends BaseModel {\n  task: string = Field(..., { description: \"ID of the task that produced this artifact (e.g., 'T1').\" });\n  files: File[] = Field([], { description: \"List of files created or modified by the task.\" });\n}\n\nexport class Role extends BaseModel {\n  title: string = Field(..., { description: \"Title of the role (e.g., 'Engineer', 'Reviewer').\" });\n  purpose: string = Field(..., { description: \"The main purpose and goal of this role.\" });\n}\n\nexport class Prompt extends BaseModel {\n  agent: string = Field(..., { description: \"Name of the agent this prompt is for. Must match exactly an existing agent.\" });\n  role: string = Field(..., { description: \"Role context of the agent.\" });\n  systemPrompt: string = Field(..., { description: \"Full text of the system prompt for this agent.\" });\n}\n\nexport class Team extends BaseModel {\n  notes: string = Field('', { description: \"General notes or feedback about the team's performance or the plan.\" });\n  prompts: Prompt[] = Field([], { description: \"A list of new or updated system prompts for any agent in the team.\" });\n}\n\nexport class Plan extends BaseModel {\n  highLevelGoal: string = Field(..., { description: \"The main goal this plan is designed to achieve.\" });\n  reasoning: string = Field(..., { description: \"A brief explanation of the plan's structure and strategy.\" });\n  roles: Role[] = Field([], { description: \"List of all roles required to execute the plan.\" });\n  tasks: Task[] = Field([], { description: \"A step-by-step sequence of tasks to be executed in order.\" });\n  team: Team | null = Field(null, { description: \"Updates to team configuration or system prompts.\" });\n}\n\nexport class AgentOutput extends BaseModel {\n  output: string = Field(..., { description: \"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\" });\n  artifact: Artifact | null = Field(null, { description: \"Any files created or modified by the agent during the task.\" });\n  team: Team | null = Field(null, { description: \"Updates to team configuration or system prompts.\" });\n  reasoning: string | null = Field(null, { description: \"Explanation of how the agent arrived at this output.\" });\n\n  // Example of adding a custom validator if needed\n  @model_validator(mode='after')\n  def check_output_and_artifact(self) -> 'AgentOutput':\n    if not self.output and not self.artifact:\n      raise ValueError('Agent output must contain either an \"output\" message or an \"artifact\".');\n    return self;\n}\n\n// --- Example Usage & Schema Generation ---\nif (require.main === module) {\n  console.log('--- AgentOutput Schema ---\\n', JSON.stringify(AgentOutput.model_json_schema(), null, 2));\n  console.log('\\n--- Plan Schema ---\\n', JSON.stringify(Plan.model_json_schema(), null, 2));\n}\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_3_Lyra_result.txt' from (Lyra) in [T4]:
{"output":"Project structure, dependencies, and build tools (tsc, esbuild) are set up for the TypeScript Node.js runtime. This includes creating essential directories and configuration files like `package.json`, `tsconfig.json`, and a basic `.eslintrc.js`.","artifact":{"task":"T4","files":[{"path":"package.json","content":"{\n  \"name\": \"typescript-runtime\",\n  \"version\": \"1.0.0\",\n  \"description\": \"A Node.js runtime reimplementation using TypeScript, following SOTA principles.\",\n  \"main\": \"dist/index.js\",\n  \"types\": \"dist/index.d.ts\",\n  \"scripts\": {\n    \"build\": \"tsc && node esbuild.js\",\n    \"dev\": \"tsc -w & node esbuild.js --watch\",\n    \"start\": \"node dist/index.js\",\n    \"lint\": \"eslint . --ext .ts\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"runtime\",\n    \"typescript\",\n    \"node\",\n    \"agent\",\n    \"llm\"\n  ],\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@types/node\": \"^20.11.19\",\n    \"chalk\": \"^5.3.0\",\n    \"chokidar\": \"^3.5.3\",\n    \"dotenv\": \"^16.4.5\",\n    \"esbuild\": \"^0.20.2\",\n    \"glob\": \"^10.3.10\",\n    \"js-yaml\": \"^4.1.0\",\n    \"pydantic\": \"^2.0.0\",\n    \"winston\": \"^3.11.0\",\n    \"zod\": \"^3.22.7\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^8.57.0\",\n    \"eslint-config-prettier\": \"^9.1.0\",\n    \"eslint-plugin-prettier\": \"^5.1.3\",\n    \"prettier\": \"^3.2.5\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"^5.3.3\"\n  }\n}\n"},{"path":"tsconfig.json","content":"{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"CommonJS\",\n    \"lib\": [\"ES2020\", \"DOM\"],\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"declaration\": true,\n    \"sourceMap\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"**/*.spec.ts\"]\n}\n"},{"path":"esbuild.js","content":"const esbuild = require('esbuild');\nconst chokidar = require('chokidar');\nconst { join } = require('path');\n\nconst watch = process.argv.includes('--watch');\n\nconst buildOptions = {\n  entryPoints: ['src/index.ts'],\n  bundle: true,\n  outfile: 'dist/index.js',\n  platform: 'node',\n  format: 'cjs',\n  sourcemap: true,\n  target: 'node20',\n  external: ['@types/node', 'fs', 'path', 'os', 'child_process', 'http', 'https', 'crypto'], // Externalize Node.js built-ins and types\n};\n\nasync function build() {\n  try {\n    await esbuild.build(buildOptions);\n    console.log('Build successful!');\n  } catch (err) {\n    console.error('Build failed:', err);\n  }\n}\n\nasync function run() {\n  if (watch) {\n    console.log('Watching for changes...');\n    await build(); // Initial build\n\n    const watcher = chokidar.watch('src/**/*.ts', {\n      ignored: /node_modules/, // Ignore node_modules\n      persistent: true,\n    });\n\n    watcher.on('change', async (filePath) => {\n      console.log(`File changed: ${filePath}`);\n      await build();\n    });\n\n    watcher.on('add', async (filePath) => {\n        console.log(`File added: ${filePath}`);\n        await build();\n    });\n\n    watcher.on('unlink', async (filePath) => {\n        console.log(`File removed: ${filePath}`);\n        await build();\n    });\n\n    // Keep the process running\n    process.stdin.resume();\n\n  } else {\n    await build();\n    process.exit(0); // Exit after build if not watching\n  }\n}\n\nrun().catch(err => {\n  console.error('Error during esbuild process:', err);\n  process.exit(1);\n});\n"},{"path":".eslintrc.js","content":"module.exports = {\n  root: true,\n  parser: '@typescript-eslint/parser',\n  plugins: [\n    '@typescript-eslint',\n    'prettier'\n  ],\n  extends: [\n    'eslint:recommended',\n    'plugin:@typescript-eslint/recommended',\n    'plugin:prettier/recommended'\n  ],\n  parserOptions: {\n    ecmaVersion: 2020,\n    sourceType: 'module',\n    project: './tsconfig.json', // Specify the tsconfig.json file\n  },\n  rules: {\n    // Override rules here if needed\n    '@typescript-eslint/no-explicit-any': 'warn', // Example: Warn about 'any' type\n    '@typescript-eslint/no-unused-vars': 'warn', // Warn about unused variables\n    'prettier/prettier': ['error', {\n      endOfLine: 'lf',\n    }],\n    // Add other rules as per project standards\n  },\n  ignorePatterns: ['node_modules/', 'dist/', '*.d.ts'],\n};\n"},{"path":".prettierrc.js","content":"module.exports = {\n  semi: true,\n  trailingComma: 'es5',\n  singleQuote: true,\n  printWidth: 100,\n  tabWidth: 2,\n  endOfLine: 'lf',\n};\n"},{"path":"src/index.ts","content":"import { System } from './system/System';\nimport { Agent } from './agent/Agent';\nimport { Orchestrator } from './orchestrator/Orchestrator';\nimport { Session } from './core/Session';\nimport { ExecutionContext } from './core/ExecutionContext';\nimport { LLM } from './llm/LLM';\nimport { Logger } from './logger/Logger';\nimport { Config } from './config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from './custom_types';\n\n// Re-export core components and types for external use\nexport {\n  System,\n  Agent,\n  Orchestrator,\n  Session,\n  ExecutionContext,\n  LLM,\n  Logger,\n  Config,\n  Plan,\n  Task,\n  Artifact,\n  Role,\n  Prompt,\n  Team,\n  AgentOutput,\n  File\n};\n\n// Export default logger instance\nexport const logger = Logger.getInstance();\n\n// Example usage (can be moved to a CLI entry point like sysmain.ts later)\nasync function main() {\n  logger.info('Starting runtime initialization...');\n\n  try {\n    // 1. Load Configuration\n    // Assuming runtime.yaml is in the project root or a config directory\n    const config = await Config.load('runtime.yaml'); \n    logger.info('Configuration loaded successfully.');\n\n    // 2. Initialize System\n    // Use projectRoot from config if available, otherwise default to current directory\n    const system = new System(config.projectRoot || '.', config.defaultModel || 'gemini-2.5-flash-lite');\n    await system.setup(); // Load agents, prompts, etc.\n    logger.info('System setup complete.');\n\n    // 3. Define a high-level goal and initial files (example)\n    // This goal is aligned with the project's objective\n    const highLevelGoal = 'Reimplement runtime using TypeScript in Node.js following SOTA coding principles';\n    const initialFiles: File[] = [\n      // Example: file content could be loaded from actual files during runtime execution\n      // new File({ path: 'docs/runtime.md', content: '...' })\n    ];\n\n    // 4. Start the workflow to generate a plan\n    const plan = await system.start(highLevelGoal, initialFiles);\n    logger.info('Workflow plan generated.');\n    console.log('\\n--- Generated Plan ---');\n    console.log(JSON.stringify(plan.toJSON(), null, 2)); // Use toJSON() for Pydantic models\n    console.log('----------------------\\n');\n\n    // 5. Run the workflow for a specified number of rounds\n    const rounds = config.workflowRounds || 3; // Use rounds from config or default to 3\n    await system.run(plan, rounds, initialFiles);\n    logger.info('Workflow execution finished.');\n\n  } catch (error) {\n    logger.error('Runtime initialization failed:', error);\n  }\n}\n\n// Uncomment the following line to run the example main function when executing this file directly\n// main();\n"},{"path":"src/system/System.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { LLM } from '../llm/LLM';\nimport { Logger } from '../logger/Logger';\nimport { Config } from '../config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from '../custom_types';\nimport { readFileSync, existsSync, mkdirSync } from 'fs';\nimport { join, dirname, basename } from 'path';\nimport yaml from 'js-yaml';\nimport { globSync } from 'glob';\n\nconst logger = Logger.getInstance();\n\nexport class System {\n  private config: Config;\n  private agents: Agent[] = [];\n  private session: Session | null = null;\n  private orchestrator: Orchestrator | null = null;\n  private readonly rootDir: string;\n  private readonly defaultModel: string;\n\n  constructor(rootDir: string, defaultModel: string = 'gemini-2.5-flash-lite') {\n    this.rootDir = rootDir;\n    this.defaultModel = defaultModel;\n    logger.info(`System initialized with root directory: ${this.rootDir} and default model: ${this.defaultModel}`);\n  }\n\n  async setup(): Promise<void> {\n    logger.info('--- System Setup ---');\n    this.config = await Config.load('runtime.yaml', this.rootDir);\n    logger.info('Configuration loaded.');\n\n    const agentSpecs = this._loadAgentSpecs(join(this.rootDir, this.config.agentDir || 'agents'));\n    const prompts = this._loadPrompts(join(this.rootDir, this.config.promptDir || 'prompts'));\n\n    const allTeamMembers = new Set<string>();\n    agentSpecs.forEach(spec => {\n      spec.team?.forEach(member => allTeamMembers.add(member.name.toLowerCase()));\n    });\n\n    // Instantiate agents, excluding those already defined as part of another agent's team\n    for (const spec of agentSpecs) {\n      if (!allTeamMembers.has(spec.name.toLowerCase())) {\n        const agent = this._instantiateAgent(spec, prompts, agentSpecs);\n        if (agent) {\n          this.agents.push(agent);\n        }\n      }\n    }\n\n    if (this.agents.length === 0) {\n      throw new Error('No agents could be instantiated. System setup failed.');\n    }\n\n    // Find and set the orchestrator\n    const orchestratorAgentInstance = this.agents.find(\n      agent => agent.role.toLowerCase() === 'orchestrator'\n    );\n    if (!orchestratorAgentInstance) {\n      throw new Error('Orchestrator agent not found. System setup failed.');\n    }\n    // Ensure it's an instance of Orchestrator\n    if (!(orchestratorAgentInstance instanceof Orchestrator)) {\n       throw new Error(`Agent '${orchestratorAgentInstance.name}' is not a valid Orchestrator instance. System setup failed.`);\n    }\n    this.orchestrator = orchestratorAgentInstance;\n    logger.info(`Orchestrator set to: ${this.orchestrator.name}`);\n\n    // Initialize session\n    this.session = new Session(this.agents, this.rootDir);\n    logger.info('Session initialized.');\n\n    logger.info('--- System Setup Complete ---');\n  }\n\n  async start(highLevelGoal: string, initialFiles: File[] = []): Promise<Plan> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call setup() before start().');\n    }\n\n    logger.info(`Generating plan for goal: \"${highLevelGoal}\"`);\n    // The orchestrator's generatePlan method is called here\n    const plan = await this.orchestrator.generatePlan(this.session, highLevelGoal, initialFiles);\n\n    if (!plan) {\n      throw new Error('Orchestration failed: Could not generate a valid plan.');\n    }\n\n    logger.info('Plan generated successfully.');\n    this.session.addArtifact('initial_plan.json', JSON.stringify(plan.toJSON(), null, 2));\n    logger.debug(`Plan details: ${JSON.stringify(plan.toJSON())}`);\n\n    return plan;\n  }\n\n  async run(plan: Plan, rounds: number = 1, initialFiles: File[] = []): Promise<void> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call start() before run().');\n    }\n\n    logger.info(`--- Starting Workflow (Rounds: ${rounds}) ---`);\n\n    const context = new ExecutionContext(this.session, plan);\n    // Record initial files as an artifact in the context\n    context.recordInitial('files', new Artifact({ task: 'initial', files: initialFiles }));\n\n    for (let roundNum = 1; roundNum <= rounds; roundNum++) {\n      context.roundNum = roundNum;\n      context.resetStepIndex();\n      logger.info(`Starting workflow round ${roundNum} for goal: \"${plan.highLevelGoal}\"`);\n\n      while (context.stepIndex < plan.tasks.length) {\n        const currentTask = context.currentStep();\n        // Find the agent responsible for the current task\n        let delegateAgent = this.agents.find(agent => agent.name.toLowerCase() === currentTask.agent.toLowerCase());\n\n        if (!delegateAgent) {\n          logger.warn(`Agent '${currentTask.agent}' not found for task '${currentTask.description}'. Falling back to Orchestrator.`);\n          delegateAgent = this.orchestrator;\n        }\n\n        logger.info(`Executing step ${context.stepIndex + 1}/${plan.tasks.length}: '${currentTask.description}' by Agent '${delegateAgent.name}' (Role: ${currentTask.role})`);\n\n        try {\n          // Execute the task using the determined agent\n          const agentOutput = await delegateAgent.executeTask(context);\n          if (agentOutput) {\n            // Record the main output of the agent\n            context.recordArtifact(`${delegateAgent.name}_output_${currentTask.id}.txt`, agentOutput.output, true);\n            \n            // Process and save any artifacts produced by the agent\n            if (agentOutput.artifact && agentOutput.artifact.files) {\n              agentOutput.artifact.files.forEach(file => {\n                this.session?.addArtifact(file.path, file.content);\n                console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n                // Optionally record the file artifact in the context for future steps\n                context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); \n              });\n            }\n            // Handle team updates (e.g., new prompts) provided by the agent\n            if (agentOutput.team?.prompts) {\n              logger.info(`Agent '${delegateAgent.name}' provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n              for (const promptData of agentOutput.team.prompts) {\n                this._updateAgentPrompt(promptData.agent, promptData.systemPrompt);\n              }\n            }\n          }\n        } catch (error) {\n          logger.error(`Error executing task '${currentTask.description}' by agent '${delegateAgent.name}':`, error);\n          // Error handling strategy: log and continue to the next task for now\n          context.recordArtifact(`${delegateAgent.name}_error_${currentTask.id}.txt`, `Error: ${error.message}`);\n        }\n        context.nextStep(); // Move to the next task in the plan\n      }\n      logger.info(`Completed workflow round ${roundNum}.`);\n    }\n\n    logger.info('--- Workflow Complete ---');\n  }\n\n  // Updates the system prompt for a specific agent\n  private _updateAgentPrompt(agentName: string, newPrompt: string): void {\n    const targetAgent = this.agents.find(agent => agent.name.toLowerCase() === agentName.toLowerCase());\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Updated system prompt for agent '${targetAgent.name}'.`);\n    } else {\n      logger.warn(`Agent '${agentName}' not found for prompt update.`);\n    }\n  }\n\n  // Instantiates an Agent or a specialized subclass like Orchestrator\n  private _instantiateAgent(spec: AgentSpec, prompts: Record<string, string>, allAgentSpecs: AgentSpec[]): Agent | null {\n    const promptKey = `${spec.name.toLowerCase()}_instructions.txt`;\n    // Prioritize systemPrompt from spec, then from loaded prompts file, else empty\n    const systemPrompt = spec.systemPrompt || prompts[promptKey] || '';\n    const model = spec.model || this.defaultModel;\n\n    let agent: Agent;\n\n    if (spec.role.toLowerCase() === 'orchestrator') {\n      agent = new Orchestrator(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n      // If the orchestrator spec defines a team, build and assign it\n      if (spec.team && spec.team.length > 0) {\n        (agent as Orchestrator).team = this._buildTeam(spec.team, allAgentSpecs, prompts, 0);\n      }\n    } else {\n      agent = new Agent(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n    }\n\n    logger.info(`Instantiated Agent: ${agent.name} (Role: ${agent.role}, Model: ${agent.model})`);\n    return agent;\n  }\n\n  // Recursively builds the team for an agent (primarily for Orchestrator)\n  private _buildTeam(teamSpecs: PromptSpec[], allAgentSpecs: AgentSpec[], prompts: Record<string, string>, depth: number): Record<string, Agent> {\n    const team: Record<string, Agent> = {};\n    const maxDepth = 5; // Prevent infinite recursion\n\n    if (depth > maxDepth) {\n      logger.error('Maximum team nesting depth reached. Stopping team creation.');\n      return {};\n    }\n\n    for (const teamSpec of teamSpecs) {\n      const memberSpec = allAgentSpecs.find(spec => spec.name.toLowerCase() === teamSpec.name.toLowerCase());\n      if (memberSpec) {\n        // Recursively instantiate the team member\n        const memberAgent = this._instantiateAgent(memberSpec, prompts, allAgentSpecs);\n        if (memberAgent) {\n          team[teamSpec.name] = memberAgent;\n        } else {\n          logger.warn(`Failed to instantiate team member '${teamSpec.name}'. Skipping.`);\n        }\n      } else {\n        logger.warn(`Team member specification for '${teamSpec.name}' not found. Creating a placeholder agent.`);\n        // Create a placeholder agent if spec is not found\n        const placeholderAgent = new Agent(\n          teamSpec.name,\n          teamSpec.role || 'Unknown Role',\n          teamSpec.goal || 'No specific goal defined',\n          this.defaultModel,\n          '' // No prompt for placeholder\n        );\n        team[teamSpec.name] = placeholderAgent;\n      }\n    }\n    return team;\n  }\n\n  // Loads agent specifications from YAML files\n  private _loadAgentSpecs(agentsDir: string): AgentSpec[] {\n    logger.info(`Loading agent specifications from: ${agentsDir}`);\n    if (!existsSync(agentsDir)) {\n        logger.warn(`Agent directory not found: ${agentsDir}. No agents will be loaded.`);\n        return [];\n    }\n    const agentFiles = globSync(join(agentsDir, '*.yaml'));\n    const specs: AgentSpec[] = [];\n    agentFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        // Use js-yaml for loading YAML content\n        const spec = yaml.load(content) as AgentSpec;\n        \n        // Resolve system prompt path relative to the agent file's directory\n        if (spec.systemPromptPath) {\n          const baseDir = dirname(filePath);\n          const promptPath = join(baseDir, spec.systemPromptPath);\n          if (existsSync(promptPath)) {\n            spec.systemPrompt = readFileSync(promptPath, 'utf-8');\n          } else {\n            logger.warn(`System prompt file not found for ${spec.name}: ${promptPath}`);\n            spec.systemPrompt = ''; // Ensure systemPrompt is defined\n          }\n        }\n        // Use default model if not specified in the agent spec\n        spec.model = spec.model || this.defaultModel;\n        specs.push(spec);\n        logger.debug(`Loaded agent spec: ${spec.name}`);\n      } catch (error) {\n        logger.error(`Error loading agent spec from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${specs.length} agent specifications loaded.`);\n    return specs;\n  }\n\n  // Loads prompt files from a directory\n  private _loadPrompts(promptsDir: string): Record<string, string> {\n    logger.info(`Loading prompts from: ${promptsDir}`);\n    if (!existsSync(promptsDir)) {\n        logger.warn(`Prompts directory not found: ${promptsDir}. No prompts will be loaded.`);\n        return {};\n    }\n    const promptFiles = globSync(join(promptsDir, '*.txt'));\n    const prompts: Record<string, string> = {};\n    promptFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        const fileName = basename(filePath);\n        prompts[fileName] = content;\n        logger.debug(`Loaded prompt: ${fileName}`);\n      } catch (error) {\n        logger.error(`Error loading prompt from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${Object.keys(prompts).length} prompts loaded.`);\n    return prompts;\n  }\n}\n"},{"path":"src/agent/Agent.ts","content":"import { LLM, LLMGenerateContentArgs } from '../llm/LLM';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, Artifact, AgentOutput, File, Team, Role } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\n// Interface for Agent configuration loaded from YAML\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string; // Path to the system prompt file relative to agent definition\n  team?: PromptSpec[]; // For delegation, specifies members of its team\n}\n\n// Interface for team member specification (used in AgentSpec.team)\nexport interface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  protected systemPrompt: string;\n  protected llm: LLM;\n  protected model: string;\n  // Team is primarily for Orchestrator, but can be used by other agents for delegation\n  public team: Record<string, Agent> = {}; \n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    this.llm = LLM.factory(model); // Use the factory to get an LLM instance\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Model: ${this.model})`);\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task assigned to this agent within the given context.\n   * @param context The execution context containing plan, session, and previous artifacts.\n   * @returns A Promise resolving to an AgentOutput object or null if execution fails.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    const currentTask = context.currentStep();\n    const previousArtifactsContent = await this._getPreviousArtifacts(context);\n\n    // Construct the prompt for the LLM\n    const taskPrompt = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${currentTask.description}'`,\n      // Include team roles for context\n      `The team's roles are:\n${JSON.stringify(context.plan.roles.map((role: Role) => ({ title: role.title, purpose: role.purpose })), null, 2)}`,\n    ].join('\\n\\n');\n\n    let fullPrompt = taskPrompt;\n    if (previousArtifactsContent) {\n      fullPrompt += `\\n\\nPlease use the following outputs from other agents as your input:\\n\\n${previousArtifactsContent}\\n\\n`;\n    }\n    // Add instruction for structured output\n    fullPrompt += `Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project. Respond in a JSON format that includes 'output', and optionally 'artifact' and 'team' fields.`\n\n    logger.debug(`Constructed prompt for ${this.name} for task ${currentTask.id}:\n${fullPrompt}`);\n\n    // Record the prompt used for this task for debugging and auditing\n    context.recordArtifact(`${this.name}_prompt_${currentTask.id}.txt`, fullPrompt);\n\n    try {\n      // Define arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: fullPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: AgentOutput, // Expecting AgentOutput schema\n        responseMimeType: 'application/json',\n        temperature: 0.7, // Default temperature, can be overridden\n      };\n\n      // Call the LLM to generate content\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error(`LLM returned no response for task ${currentTask.id} by ${this.name}.`);\n        return null;\n      }\n\n      // Ensure the response is parsed as AgentOutput\n      if (typeof response === 'string') {\n          try {\n              // Attempt to parse the string response as JSON and validate against AgentOutput schema\n              const parsedOutput = AgentOutput.parse(JSON.parse(response));\n              logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n              this._processAndRecordArtifacts(context, parsedOutput);\n              return parsedOutput;\n          } catch (parseError) {\n              logger.error(`Failed to parse LLM response as AgentOutput for task ${currentTask.id} by ${this.name}:`, parseError);\n              logger.error(`Raw LLM response: ${response}`);\n              // Record the raw, unparseable response as an artifact\n              context.recordArtifact(`${this.name}_raw_output_${currentTask.id}.txt`, response);\n              // Return an AgentOutput indicating the error\n              return new AgentOutput({ output: `Error processing response: LLM returned invalid JSON or schema.`, reasoning: 'LLM response was not valid JSON or AgentOutput schema.' });\n          }\n      } else if (response instanceof AgentOutput) {\n          // If the response is already an AgentOutput instance, use it directly\n          logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n          this._processAndRecordArtifacts(context, response);\n          return response;\n      }\n\n      // Handle unexpected response types from the LLM\n      logger.warn(`Unexpected response type from LLM for task ${currentTask.id} by ${this.name}: ${typeof response}`);\n      return new AgentOutput({ output: `Unexpected response type received from LLM.`, reasoning: 'LLM returned an unexpected data type.' });\n\n    } catch (error) {\n      logger.error(`Error during LLM interaction for task ${currentTask.id} by ${this.name}:`, error);\n      // Record the error as an artifact for debugging\n      context.recordArtifact(`${this.name}_error_${currentTask.id}.txt`, `Error executing task: ${error.message}`);\n      // Return an AgentOutput indicating the failure\n      return new AgentOutput({ output: `Failed to execute task due to an error.`, reasoning: 'LLM interaction failed.' });\n    }\n  }\n\n  /**\n   * Retrieves and formats artifacts required by the current task from the execution context.\n   * @param context The execution context.\n   * @returns A formatted string of required artifacts, or an empty string if none are found.\n   */\n  private async _getPreviousArtifacts(context: ExecutionContext): Promise<string> {\n    // Get the IDs of tasks whose outputs are required for the current step\n    const requiredTaskIds = ['initial', ...context.currentStep().requires];\n    const relevantArtifacts: string[] = [];\n\n    // Iterate through all items remembered in the context\n    for (const [key, contextItem] of Object.entries(context.items)) {\n      // Check if the artifact's source task ID is in the required list\n      if (requiredTaskIds.includes(contextItem.step.id)) {\n        let artifactContent = '';\n        // Format the artifact content based on its type\n        if (contextItem.content instanceof Artifact) {\n          // If it's an Artifact object, format its files\n          artifactContent = contextItem.content.files.map(file => \n            `--- File: ${file.path}\\n${file.content}`\n          ).join('\\n');\n        } else if (typeof contextItem.content === 'string') {\n          artifactContent = contextItem.content;\n        } else {\n          try {\n            // Attempt to stringify if it's a JSON-serializable object\n            artifactContent = JSON.stringify(contextItem.content, null, 2);\n          } catch (e) {\n            // Fallback to simple string conversion\n            artifactContent = String(contextItem.content);\n          }\n        }\n        // Add the formatted artifact to the list\n        relevantArtifacts.push(`--- Artifact '${key}' from (${contextItem.step.role}) in [${contextItem.step.id}]:\\n${artifactContent}`);\n      }\n    }\n\n    return relevantArtifacts.join('\\n\\n');\n  }\n\n  /**\n   * Processes the AgentOutput, records artifacts and reasoning, and handles potential team updates.\n   * @param context The execution context.\n   * @param agentOutput The output received from the agent.\n   */\n  private _processAndRecordArtifacts(context: ExecutionContext, agentOutput: AgentOutput): void {\n    // Record the main output message\n    context.recordArtifact(`${this.name}_output.txt`, agentOutput.output, true);\n\n    // Process and save any artifacts generated by the agent\n    if (agentOutput.artifact && agentOutput.artifact.files) {\n      agentOutput.artifact.files.forEach(file => {\n        // Save the file to the session directory\n        this.session?.addArtifact(file.path, file.content);\n        console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n        // Record the file artifact in the context's memory for potential future use\n        // We record the entire Artifact object here for simplicity, assuming it contains relevant context\n        context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); \n      });\n    }\n\n    // Record the agent's reasoning if provided\n    if (agentOutput.reasoning) {\n      context.recordArtifact(`${this.name}_reasoning.txt`, agentOutput.reasoning, true);\n    }\n\n    // Handle team updates (e.g., new prompts) - the System class will manage applying these\n    if (agentOutput.team && agentOutput.team.prompts) {\n      logger.info(`Agent ${this.name} provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n      // The System class is responsible for applying these prompt updates to other agents.\n    }\n  }\n}\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Role, AgentOutput } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  // Orchestrator might have a more complex team structure or delegation logic\n  // For now, we'll keep it simple and assume 'team' is managed by the System class\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name}`);\n  }\n\n  /**\n   * Generates a plan for the given high-level goal using the LLM.\n   * @param session The current session object.\n   * @param highLevelGoal The overall objective for the plan.\n   * @param initialFiles Any initial files provided to the system.\n   * @returns A Promise resolving to the generated Plan object, or null if generation fails.\n   */\n  async generatePlan(session: Session, highLevelGoal: string, initialFiles: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator '${this.name}' is generating a plan for: \"${highLevelGoal}\"`);\n\n    if (!this.llm) {\n      logger.error('LLM client not initialized for Orchestrator.');\n      return null;\n    }\n\n    // Get available agents and their details to include in the prompt for context\n    const availableAgents = session.agents.map(agent => ({\n      name: agent.name,\n      role: agent.role,\n      goal: agent.goal,\n    }));\n\n    // Format agent information for the prompt\n    const agentDescription = availableAgents.map(agent => \n      `- Name: '${agent.name}'\\n  Role: ${agent.role}\\n  Goal: \"${agent.goal}\"`\n    ).join('\\n');\n\n    // Load the planning prompt template from a file\n    // The path is relative to the project root, adjust as necessary\n    const promptTemplatePath = join(this.rootDir, 'prompts', 'general_planning.txt');\n    let promptTemplate: string;\n    try {\n      promptTemplate = readFileSync(promptTemplatePath, 'utf-8');\n    } catch (error) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}:`, error);\n      return null;\n    }\n\n    // Format the prompt with dynamic information\n    const planningPrompt = promptTemplate.replace('{{high_level_goal}}', highLevelGoal)\n                                      .replace('{{team_description}}', agentDescription)\n                                      .replace('{{initial_files}}', initialFiles.map(f => `--- File: ${f.path}\\n```\\n${f.content}\\n```).join('\\n\\n'));\n\n    logger.debug(`Constructed planning prompt for Orchestrator:\n${planningPrompt}`);\n    // Save the prompt used for planning as an artifact\n    session.addArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      // Prepare arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: planningPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: Plan, // Expecting a Plan object\n        responseMimeType: 'application/json',\n        temperature: 0.1, // Lower temperature for more deterministic planning\n      };\n      \n      // Call the LLM to generate the plan\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error('LLM returned no response for plan generation.');\n        return null;\n      }\n\n      // Ensure the response is parsed as a Plan object\n      if (typeof response === 'string') {\n        try {\n          const plan = Plan.parse(JSON.parse(response));\n          return plan;\n        } catch (parseError) {\n          logger.error('Failed to parse LLM response as Plan:', parseError);\n          logger.error(`Raw LLM response: ${response}`);\n          session.addArtifact('plan_generation_error.txt', `LLM response was not valid JSON or Plan schema: ${response}`);\n          return null;\n        }\n      } else if (response instanceof Plan) {\n        return response; // Response is already a Plan instance\n      }\n\n      logger.error(`Unexpected response type from LLM during plan generation: ${typeof response}`);\n      return null;\n\n    } catch (error) {\n      logger.error('Error during plan generation:', error);\n      session.addArtifact('plan_generation_error.txt', `Error: ${error.message}`);\n      return null;\n    }\n  }\n\n  /**\n   * Overrides the base Agent's executeTask. Orchestrator's primary role is planning,\n   * but it might be assigned specific tasks by the plan itself (e.g., plan refinement).\n   * @param context The execution context.\n   * @returns AgentOutput or null.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    logger.warn(`Orchestrator '${this.name}' executeTask called directly. Task: '${context.currentStep().description}'. Falling back to default agent behavior.`);\n    // Delegate to the base Agent's executeTask to process the prompt as a standard task.\n    // This allows the orchestrator to perform tasks defined within the plan it generated.\n    return super.executeTask(context);\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Artifact, File } from '../custom_types';\nimport { join, dirname as getDirname } from 'path';\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'fs';\n\nconst logger = Logger.getInstance();\n\nexport class Session {\n  public readonly sessionId: string;\n  public readonly agents: Agent[];\n  public state: string;\n  public readonly sessionDir: string;\n  public readonly projectRoot: string;\n\n  constructor(agents: Agent[], projectRoot: string, sessionId?: string) {\n    this.sessionId = sessionId || `session_${Date.now().toString()}`;\n    this.agents = agents;\n    this.state = 'initialized';\n    this.projectRoot = projectRoot;\n    this.sessionDir = join(this.projectRoot, 'sessions', this.sessionId);\n\n    // Ensure the session directory exists\n    if (!existsSync(this.sessionDir)) {\n      mkdirSync(this.sessionDir, { recursive: true });\n    }\n    logger.info(`Session created: ${this.sessionId} (Directory: ${this.sessionDir})`);\n  }\n\n  /**\n   * Saves an artifact to the session directory.\n   * @param name The name of the artifact file.\n   * @param content The content to write. Can be an object, string, etc.\n   */\n  addArtifact(name: string, content: any): void {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      const dir = getDirname(artifactPath);\n      // Ensure the directory for the artifact exists\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n      \n      let contentToWrite: string;\n      // Determine if content should be saved as JSON or plain text\n      if (typeof content === 'object' && content !== null && !(content instanceof File) && !(content instanceof Artifact)) {\n        // If it's a plain object (like from AgentOutput.toJSON()), save as JSON\n        contentToWrite = JSON.stringify(content, null, 2);\n      } else if (typeof content === 'string') {\n        contentToWrite = content;\n      } else {\n        // Fallback for other types, convert to string\n        contentToWrite = String(content);\n      }\n\n      writeFileSync(artifactPath, contentToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (e) {\n      logger.error(`Error saving artifact '${name}' in session ${this.sessionId}:`, e);\n    }\n  }\n\n  /**\n   * Retrieves an artifact from the session directory.\n   * @param name The name of the artifact file to load.\n   * @returns The content of the artifact, or null if not found or an error occurs.\n   */\n  getArtifact(name: string): any {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      if (existsSync(artifactPath)) {\n        const content = readFileSync(artifactPath, 'utf-8');\n        // Attempt to parse as JSON if the filename suggests it\n        if (name.endsWith('.json')) {\n          try {\n            return JSON.parse(content);\n          } catch (e) {\n            logger.error(`Error parsing JSON artifact '${name}':`, e);\n            return null; // Return null if JSON parsing fails\n          }\n        }\n        return content; // Return as plain text\n      } else {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n        return null;\n      }\n    } catch (e) {\n      logger.error(`Error retrieving artifact '${name}' from session ${this.sessionId}:`, e);\n      return null;\n    }\n  }\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface ContextItem {\n  name: string;\n  content: any;\n  step: Task;\n}\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores artifacts remembered from previous steps/rounds\n\n  constructor(session: Session, plan: Plan) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = 0;\n    this.roundNum = 1;\n    this.items = {};\n    logger.info('ExecutionContext initialized.');\n  }\n\n  /** Advances to the next step in the plan. */\n  nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  /** Resets the step index to the beginning of the plan. */\n  resetStepIndex(): void {\n    this.stepIndex = 0;\n    logger.debug('Step index reset to 0.');\n  }\n\n  /**\n   * Returns the current task based on the step index.\n   * @returns The current Task object.\n   * @throws Error if the step index is out of bounds.\n   */\n  currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error('Step index out of bounds. No more tasks in the plan.');\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  /**\n   * Records an artifact generated by a task and optionally remembers it in the context's memory.\n   * @param key The key identifier for the artifact.\n   * @param value The artifact content.\n   * @param remember If true, the artifact is stored in context.items for future steps.\n   */\n  recordArtifact(key: string, value: any, remember: boolean = false): void {\n    // Construct a unique artifact key including round and step number\n    const artifactKey = `__${this.roundNum}/__step_${this.stepIndex}_${key}`;\n    this.session.addArtifact(artifactKey, value);\n    // If 'remember' is true, store the artifact in the context's memory\n    if (remember) {\n      this._rememberItem(artifactKey, value, this.currentStep());\n    }\n  }\n\n  /**\n   * Records initial artifacts provided at the beginning of the workflow.\n   * @param key The key identifier for the initial artifact.\n   * @param value The artifact content.\n   */\n  recordInitial(key: string, value: any): void {\n    // Use a special task ID 'initial' for items provided at the start\n    const initialTask: Task = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: []\n    });\n    this._rememberItem(key, value, initialTask);\n    // If the initial item is an Artifact containing files, add them to the session as well\n    if (value instanceof Artifact && value.files) {\n        value.files.forEach(file => this.session.addArtifact(file.path, file.content));\n    }\n    logger.debug(`Recorded initial item '${key}'.`);\n  }\n\n  /**\n   * Helper method to store an item in the context's memory (items dictionary).\n   * @param name The name/key for the item.\n   * @param content The content of the item.\n   * @param step The Task associated with this item.\n   */\n  private _rememberItem(name: string, content: any, step: Task): void {\n    this.items[name] = { name, content, step };\n    logger.debug(`Item '${name}' remembered in context.`);\n  }\n}\n"},{"path":"src/llm/LLM.ts","content":"import { BaseModel, model_validator } from 'pydantic';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface for the LLM generation arguments, providing a structured way to pass parameters\nexport interface LLMGenerateContentArgs {\n  modelName: string;\n  contents: string;\n  systemInstruction?: string;\n  temperature?: number;\n  responseMimeType?: 'text/plain' | 'application/json' | 'application/json-schema';\n  responseSchema?: typeof BaseModel; // Expecting a Pydantic BaseModel class\n}\n\nexport abstract class LLM {\n  /**\n   * Abstract method to be implemented by concrete LLM providers.\n   * This method performs the actual call to the LLM API.\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the LLM's response (can be string, object, or parsed schema instance).\n   */\n  protected abstract generateContentImpl(args: LLMGenerateContentArgs): Promise<any>;\n\n  /**\n   * Public method to generate content using an LLM. Handles common logic like:\n   * - Retries with exponential backoff\n   * - Response parsing based on mime type and schema\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the processed LLM response, or null if all retries fail.\n   */\n  public async generateContent(args: LLMGenerateContentArgs): Promise<any> {\n    const { modelName, responseSchema, responseMimeType } = args;\n    const maxRetries = 3; // Number of times to retry the LLM call\n    let lastError: Error | null = null;\n\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        logger.info(`Calling LLM model '${modelName}' (Attempt ${attempt}/${maxRetries})...`);\n        const response = await this.generateContentImpl(args);\n\n        if (!response) {\n          throw new Error('LLM returned an empty response.');\n        }\n\n        // Process the response based on the requested schema and mime type\n        if (responseSchema && responseMimeType === 'application/json') {\n          if (typeof response === 'string') {\n            try {\n              const parsedJson = JSON.parse(response);\n              // Validate the parsed JSON against the provided Pydantic schema\n              const validatedData = responseSchema.parse(parsedJson);\n              logger.debug(`LLM response successfully parsed and validated against schema ${responseSchema.name}.`);\n              return validatedData;\n            } catch (e) {\n              // Catch JSON parsing or Pydantic validation errors\n              lastError = new Error(`Failed to parse or validate LLM JSON response with schema ${responseSchema.name}: ${e.message}`);\n              logger.error(lastError.message);\n              // Do not retry on parsing/validation errors, as the response itself is likely malformed\n              throw lastError;\n            }\n          } else if (response instanceof responseSchema) {\n            // If the response is already an instance of the expected schema, return it directly\n            logger.debug(`LLM response is already an instance of ${responseSchema.name}.`);\n            return response;\n          } else {\n             // Handle cases where the response is neither a string nor the expected schema type\n             lastError = new Error(`LLM response was not a string or expected schema type for JSON request.`);\n             logger.error(lastError.message);\n             throw lastError;\n          }\n        } else if (responseSchema && responseMimeType === 'application/json-schema') {\n            // Specific handling for JSON schema responses if needed\n             if (typeof response === 'string') {\n                try {\n                    const parsedSchema = JSON.parse(response);\n                    // Further validation or processing might be needed here\n                    logger.debug('LLM response parsed as JSON Schema.');\n                    return parsedSchema;\n                } catch (e) {\n                     lastError = new Error(`Failed to parse LLM JSON Schema response: ${e.message}`);\n                     logger.error(lastError.message);\n                     throw lastError;\n                }\n            }\n        }\n\n        // Default handling for text responses or un-schematized JSON\n        logger.debug('LLM response processed.');\n        return response;\n\n      } catch (error) {\n        // Capture the error and prepare for retry or final failure\n        lastError = error;\n        logger.error(`LLM generation failed (Attempt ${attempt}): ${error.message}`);\n        // Implement exponential backoff for retries\n        if (attempt < maxRetries) {\n          const delay = Math.pow(2, attempt) * 1000; // Delay in ms (1s, 2s, 4s)\n          logger.info(`Waiting for ${delay / 1000} seconds before retrying...`);\n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n\n    // If all retries fail, log the last error and return null\n    logger.error(`LLM generation failed after ${maxRetries} attempts. Last error: ${lastError?.message}`);\n    return null;\n  }\n\n  /**\n   * Factory method to create an LLM instance based on a species string.\n   * This allows for dynamic instantiation of different LLM providers.\n   * @param species - A string identifying the LLM species (e.g., 'gemini:gemini-pro', 'ollama:llama3').\n   * @returns An instance of a concrete LLM class.\n   */\n  static factory(species: string): LLM {\n    logger.info(`LLM Factory: Creating LLM instance for species '${species}'`);\n    \n    // TODO: Replace dummy implementations with actual LLM provider classes\n    // Example: Implement OllamaLLM, GeminiLLM, OpenAILLM, MistralLLM classes\n    // and instantiate them here based on the 'species' string.\n\n    if (species.startsWith('ollama:')) {\n        const modelName = species.split(':')[1];\n        // return new OllamaLLM(modelName);\n        logger.warn('OllamaLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('gemini:') || species === 'gemini-2.5-flash-lite') {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new GeminiLLM(modelName);\n        logger.warn('GeminiLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n     if (species.startsWith('openai:') || species.startsWith('gpt:') || species.startsWith('opi:')) {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new OpenAILLM(modelName);\n         logger.warn('OpenAILLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('mistral:')) {\n        const modelName = species.split(':')[1];\n        // return new MistralLLM(modelName);\n         logger.warn('MistralLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n\n    // Fallback to DummyLLM if the species is unknown\n    logger.warn(`Unknown LLM species '${species}'. Using DummyLLM as fallback.`);\n    return new DummyLLM(species);\n  }\n}\n\n/**\n * A dummy LLM implementation used as a placeholder.\n * It simulates LLM responses without making actual API calls.\n */\nclass DummyLLM extends LLM {\n  constructor(private modelName: string) {\n    super();\n    logger.warn(`DummyLLM initialized with model: ${this.modelName}. This is a placeholder.`);\n  }\n\n  protected async generateContentImpl(args: LLMGenerateContentArgs): Promise<any> {\n    logger.warn(`DummyLLM: generateContentImpl called for model ${this.modelName}. Returning mock response.`);\n    \n    // Simulate response based on schema if provided\n    if (args.responseSchema) {\n        try {\n            // Create a mock response adhering to the schema\n            if (args.responseSchema === AgentOutput) {\n                return new AgentOutput({\n                    output: `Mock output for ${this.modelName} task: ${args.contents.substring(0, 50)}...`,\n                    reasoning: 'This is a mock response from DummyLLM.',\n                    artifact: new Artifact({ task: 'mock_task', files: [new File({ path: 'mock_output.txt', content: 'Mock file content.'})]}),\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            } else if (args.responseSchema === Plan) {\n                 return new Plan({\n                    highLevelGoal: 'Mock Goal',\n                    reasoning: 'Mock reasoning for plan generation',\n                    roles: [{title: 'Mock Role', purpose: 'Mock Purpose'}],\n                    tasks: [{id: 'T1', description: 'Mock Task', role: 'Mock Role', agent: 'Mock Agent', requires: []}],\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            }\n            // Add handlers for other schemas as needed\n        } catch (e) {\n            logger.error(`Error creating mock response for schema ${args.responseSchema.name}:`, e);\n            return `Mock response failed for schema ${args.responseSchema.name}.`;\n        }\n    }\n    \n    // Default mock response if no schema is provided or handled\n    return `This is a mock response from DummyLLM for model ${this.modelName}.`;\n  }\n}\n"},{"path":"src/logger/Logger.ts","content":"import winston from 'winston';\nimport { join } from 'path';\nimport fs from 'fs';\n\n// Define log levels with priorities\nconst logLevels = {\n  error: 0,\n  warn: 1,\n  info: 2,\n  debug: 3,\n};\n\n// Define the format for log messages\nconst logFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }), // Include stack trace for errors\n  winston.format.splat(), // Support for string interpolation like printf\n  winston.format.printf(({ level, message, timestamp, ...metadata }) => {\n    // Construct the log message string\n    let msg = `${timestamp} [${level.toUpperCase()}]: ${message}`;\n    // Append any additional metadata (e.g., agent name, task ID)\n    if (Object.keys(metadata).length > 0) {\n      msg += ` ${JSON.stringify(metadata)}`;\n    }\n    return msg;\n  })\n);\n\n// Define a format for JSON line logs (useful for log aggregation)\nconst jsonLogFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.json() // Output logs as JSON objects on separate lines\n);\n\n// Ensure the log directory exists\nconst logDir = process.env.LOG_DIR || 'logs'; // Use LOG_DIR environment variable or default to 'logs'\nif (!fs.existsSync(logDir)) {\n  fs.mkdirSync(logDir, { recursive: true });\n}\n\nconst logFilePath = join(logDir, 'app.log');\nconst jsonLogFilePath = join(logDir, 'app.jsonl');\n\nexport class Logger {\n  private static instance: winston.Logger;\n\n  /**\n   * Gets the singleton Winston logger instance.\n   * Configures transports for console, file (plain text), and file (JSONL).\n   * @returns The configured Winston logger instance.\n   */\n  public static getInstance(): winston.Logger {\n    if (!Logger.instance) {\n      Logger.instance = winston.createLogger({\n        level: process.env.LOG_LEVEL || 'info', // Set log level from environment or default to 'info'\n        levels: logLevels,\n        format: logFormat, // Default format for transports unless overridden\n        transports: [\n          // Console transport: Outputs logs to the console with colorization\n          new winston.transports.Console({\n            format: winston.format.combine(\n              winston.format.colorize({ all: true }), // Apply colors to all levels\n              logFormat\n            ),\n          }),\n          // File transport: Logs to 'app.log' in plain text format\n          new winston.transports.File({\n            filename: logFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: logFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true, // Allows log rotation\n          }),\n          // JSONL File transport: Logs to 'app.jsonl' in JSON format\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: jsonLogFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true,\n          }),\n        ],\n        // Handle uncaught exceptions\n        exceptionHandlers: [\n          new winston.transports.Console({\n             format: winston.format.combine(winston.format.colorize({ all: true }), logFormat)\n          }),\n          new winston.transports.File({\n            filename: logFilePath,\n            format: logFormat,\n          }),\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            format: jsonLogFormat,\n          }),\n        ]\n      });\n\n      // Optional: Add a listener for logger errors\n      Logger.instance.on('error', (err) => {\n        console.error('Winston logger encountered an error:', err);\n      });\n    }\n    return Logger.instance;\n  }\n}\n"},{"path":"src/config/Config.ts","content":"import { readFileSync } from 'fs';\nimport { join } from 'path';\nimport yaml from 'js-yaml';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface defining the structure of the configuration object\nexport interface Config {\n  projectRoot?: string;       // Root directory of the project\n  agentDir?: string;          // Directory containing agent configurations\n  promptDir?: string;         // Directory containing prompt templates\n  logDir?: string;            // Directory for log files\n  defaultModel?: string;      // Default LLM model to use if not specified\n  workflowRounds?: number;    // Default number of workflow rounds\n  // Add other configuration properties as needed\n}\n\nexport class Config {\n  /**\n   * Loads configuration from a YAML file.\n   * @param fileName The name of the configuration file (e.g., 'runtime.yaml').\n   * @param basePath The base path to search for the config file (defaults to current working directory).\n   * @returns A Promise resolving to the loaded Config object with defaults applied.\n   */\n  public static async load(fileName: string, basePath: string = process.cwd()): Promise<Config> {\n    const configPath = join(basePath, fileName);\n    logger.info(`Attempting to load configuration from: ${configPath}`);\n    try {\n      // Read the file content\n      const fileContents = readFileSync(configPath, 'utf-8');\n      // Parse the YAML content\n      const config = yaml.load(fileContents) as Config;\n\n      // Apply default values if properties are missing\n      config.projectRoot = config.projectRoot || basePath;\n      config.agentDir = config.agentDir || 'agents';\n      config.promptDir = config.promptDir || 'prompts';\n      config.logDir = config.logDir || 'logs';\n      config.defaultModel = config.defaultModel || 'gemini-2.5-flash-lite';\n      config.workflowRounds = config.workflowRounds || 3;\n\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Failed to load configuration from ${configPath}:`, error);\n      // In case of failure, return a default configuration to allow the system to start\n      // Alternatively, you might want to throw an error to halt execution\n      return {\n        projectRoot: basePath,\n        agentDir: 'agents',\n        promptDir: 'prompts',\n        logDir: 'logs',\n        defaultModel: 'gemini-2.5-flash-lite',\n        workflowRounds: 3,\n      };\n    }\n  }\n}\n"},{"path":"src/custom_types.ts","content":"import { BaseModel, Field, model_validator } from 'pydantic';\nimport { z } from 'zod'; // Import Zod for potential schema definition or validation demonstration\n\n// --- Core Types representing Task Agnostic Steps (TAS) ---\n\n/** Represents a single actionable step within a plan. */\nexport class Task extends BaseModel {\n  id: string = Field(..., { description: \"Unique ID for the task (e.g., 'T1', 'T2').\" });\n  description: string = Field(..., { description: \"Detailed description of what to do for this task.\" });\n  role: string = Field(..., { description: \"The role responsible for performing this task.\" });\n  agent: string = Field(..., { description: \"The specific agent assigned to execute this task.\" });\n  requires: string[] = Field([], { description: \"List of task IDs whose outputs are required as prerequisites (e.g., ['T1', 'T2']).\" });\n}\n\n/** Represents a file generated or used within the system. */\nexport class File extends BaseModel {\n  path: string = Field(..., { description: \"File path or name (e.g., 'src/main.py').\" });\n  content: string = Field(..., { description: \"Full content of the file.\" });\n}\n\n/** Represents the output artifact of a completed task. */\nexport class Artifact extends BaseModel {\n  task: string = Field(..., { description: \"ID of the task that produced this artifact (e.g., 'T1').\" });\n  files: File[] = Field([], { description: \"List of files created or modified by the task.\" });\n}\n\n/** Defines a role within the system or team. */\nexport class Role extends BaseModel {\n  title: string = Field(..., { description: \"Title of the role (e.g., 'Engineer', 'Reviewer').\" });\n  purpose: string = Field(..., { description: \"The main purpose and high-level responsibilities of this role.\" });\n}\n\n/** Represents a system prompt configuration for an agent. */\nexport class Prompt extends BaseModel {\n  agent: string = Field(..., { description: \"Name of the agent this prompt is for. Must match exactly an existing agent.\" });\n  role: string = Field(..., { description: \"Role context of the agent.\" });\n  systemPrompt: string = Field(..., { description: \"Full text of the system prompt for this agent.\" });\n}\n\n/** Contains metadata and coordination information for the team, including prompt updates. */\nexport class Team extends BaseModel {\n  notes: string = Field('', { description: \"General notes or feedback about the team's performance or the plan.\" });\n  prompts: Prompt[] = Field([], { description: \"A list of new or updated system prompts for agents in the team.\" });\n}\n\n/** Defines the overall plan for a workflow, including goal, roles, and tasks. */\nexport class Plan extends BaseModel {\n  highLevelGoal: string = Field(..., { description: \"The main goal this plan is designed to achieve.\" });\n  reasoning: string = Field(..., { description: \"A brief explanation of the plan's structure and strategy.\" });\n  roles: Role[] = Field([], { description: \"List of all roles required to execute the plan.\" });\n  tasks: Task[] = Field([], { description: \"A step-by-step sequence of tasks to be executed in order.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n}\n\n/**\n * Represents the structured output returned by an agent after executing a task.\n * Adheres to the JSON output format expected by the runtime.\n */\nexport class AgentOutput extends BaseModel {\n  output: string = Field(..., { description: \"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\" });\n  artifact: Artifact | null = Field(null, { description: \"Optional artifact containing files created or modified by the agent.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n  reasoning: string | null = Field(null, { description: \"Explanation of how the agent arrived at this output.\" });\n\n  /**\n   * Custom validator to ensure that an AgentOutput contains either an 'output' message or an 'artifact'.\n   */\n  @model_validator(mode='after')\n  validateOutputAndArtifact(): this {\n    if (!this.output && !this.artifact) {\n      throw new Error('Agent output must contain either an \"output\" message or an \"artifact\".');\n    }\n    return this;\n  }\n}\n\n// --- Example Usage & Schema Generation ---\n// This block executes only when the script is run directly\nif (require.main === module) {\n  console.log('--- AgentOutput Schema ---\\n', JSON.stringify(AgentOutput.model_json_schema(), null, 2));\n  console.log('\\n--- Plan Schema ---\\n', JSON.stringify(Plan.model_json_schema(), null, 2));\n}\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_4_aitutor_result.txt' from (aitutor) in [T5]:
{"output":"Research and document SOTA TypeScript/Node.js implementation patterns for the identified Task Agnostic Steps (TAS). Focus on providing guidance on best practices and equivalent implementations for concepts like agent lifecycle, session management, context handling, LLM abstraction, and planning.","artifact":{"task":"T5","files":[{"path":"docs/typescript_patterns.md","content":"# SOTA TypeScript/Node.js Patterns for Runtime System\n\nThis document outlines State-Of-The-Art (SOTA) TypeScript and Node.js implementation patterns and best practices for recreating the runtime system's functionalities, based on the identified Task Agnostic Steps (TAS).\n\n## 1. System Initialization and Configuration\n\n*   **Load System Configuration**: Use libraries like `dotenv` for environment variables and `yaml` or `json` for configuration files. For complex configurations, consider using `class-validator` for validation.\n    *   **Example**: `import yaml from 'yaml'; const config = yaml.parse(fs.readFileSync('config/runtime.yaml', 'utf-8'));`\n*   **Discover and Load Agents/Prompts**: Employ dynamic module loading using `fs.readdirSync` and `import()` or `require()`. Use a factory pattern for instantiating agents.\n    *   **Pattern**: Factory Pattern, Dynamic Module Loading.\n*   **Instantiate Agents**: Leverage classes and interfaces for defining agent structures. Use dependency injection (DI) for managing LLM clients and other dependencies.\n    *   **TypeScript Feature**: Classes, Interfaces, Generics.\n    *   **Pattern**: Dependency Injection (e.g., using `InversifyJS` or manual injection).\n*   **Initialize LLM Clients**: Create an abstract `LLM` class with concrete implementations for different providers (e.g., `OpenAI`, `Gemini`, `Ollama`). Use a factory method for easy instantiation.\n    *   **TypeScript Feature**: Abstract Classes, Interfaces, Factory Pattern.\n    *   **Best Practice**: Abstract away provider-specific details.\n*   **Set Up Logging**: Utilize libraries like `winston` or `pino` for robust logging. `pino` is known for its performance and JSON logging capabilities, suitable for structured output.\n    *   **Best Practice**: Use structured logging (JSON) for easier parsing and analysis.\n\n## 2. Workflow Planning\n\n*   **Receive High-Level Goal**: Standard function parameter.\n*   **Describe Available Agents and Roles**: Pass agent metadata (name, role, goal) as an object or array of objects to the planning agent's prompt.\n*   **Generate Plan**: The LLM interaction follows the `LLM` abstraction. The prompt engineering will be crucial here, similar to the Python version.\n    *   **TypeScript Feature**: Interfaces for `Plan`, `Task`, `Role`, `Agent`.\n*   **Validate Plan**: Use Pydantic (via `zod` or `class-validator` in TS) to define and validate the `Plan` schema.\n    *   **TypeScript Feature**: Interfaces or Classes with Validation (e.g., `zod`).\n*   **Record Initial Plan**: Use the `Session`'s artifact storage mechanism.\n\n## 3. Workflow Execution\n\n*   **Initialize Session**: A `Session` class to manage state, potentially using a class with methods for artifact storage (e.g., using `fs` module for file persistence).\n    *   **TypeScript Feature**: Classes, `async/await` for I/O operations.\n*   **Initialize Execution Context**: A `ExecutionContext` class to hold `Session`, `Plan`, current step, round number, and artifacts (e.g., `Map<string, ContextItem>`).\n    *   **TypeScript Feature**: Classes, `Map` for storing context items.\n*   **Record Initial Artifacts**: Methods within `ExecutionContext` or `Session` to save files using the `fs` module.\n    *   **Best Practice**: Use `async/await` for all file I/O operations.\n*   **Iterate Through Workflow Rounds/Tasks**: Standard `for` loops and `while` loops. Use `async/await` for asynchronous agent task execution.\n*   **Identify and Select Agent**: A helper function or method to find an agent by name or role, similar to `find_agent_by_role`.\n*   **Prepare Task Prompt**: String interpolation or template literals for constructing prompts.\n*   **Execute Task**: Agent's `executeTask` method will be `async` to handle potential LLM calls.\n    *   **TypeScript Feature**: `async/await`.\n*   **Agent Task Execution**: Classes implementing an `Agent` interface.\n*   **LLM Interaction**: Calls to the abstracted `LLM` service.\n*   **Parse LLM Response**: Use `JSON.parse()` for JSON responses. Employ `try-catch` blocks for error handling. Use Pydantic models (or `zod` schemas) for validation.\n    *   **TypeScript Feature**: `try-catch`, `JSON.parse()`.\n*   **Handle Agent Output**: Parse the `AgentOutput` structure (defined as an interface or class).\n    *   **TypeScript Feature**: Interfaces, Classes.\n*   **Record Task Result**: Use `Session` methods for artifact storage.\n*   **Update Agent Prompts**: Methods on the `Agent` class or a central `AgentManager` to update prompts.\n*   **Advance Execution Context**: Incrementing counters or updating state within `ExecutionContext`.\n*   **Manage Artifacts**: `fs` module operations within `Session` or `ExecutionContext`.\n\n## 4. Agent Communication and State Management\n\n*   **Agent Lifecycle Management**: Defined by class methods (`constructor`, `executeTask`, etc.).\n*   **Contextual Information Sharing**: Pass `ExecutionContext` objects to agent methods.\n*   **Artifact Storage and Retrieval**: Use `fs` operations within `Session`.\n*   **Session Management**: The `Session` class will manage its state.\n*   **System Prompt Updates**: Methods on `Agent` objects or a central `AgentRegistry`.\n    *   **Best Practice**: Use event emitters or callbacks for prompt updates if real-time propagation is needed.\n\n## 5. LLM Interaction Abstraction\n\n*   **Abstract LLM Interface**: Define an `abstract class LLM` with an `abstract generateContent` method.\n    *   **TypeScript Feature**: `abstract class`, `abstract method`.\n*   **LLM Model Selection**: Factory pattern as described in Section 1.\n*   **Content Generation**: Implement `async` methods in concrete LLM classes.\n*   **Response Formatting**: Use `JSON.stringify()` and `JSON.parse()`. Leverage validation libraries like `zod` or `class-validator` to ensure responses conform to expected schemas (e.g., `AgentOutput`).\n    *   **TypeScript Feature**: `async/await`, `try-catch`, `JSON.stringify/parse`.\n    *   **Best Practice**: Implement robust error handling and schema validation for LLM responses.\n\n## 6. Error Handling and Resilience\n\n*   **Exception Handling**: Use `try-catch` blocks extensively, especially around I/O operations and LLM calls. Define custom error types for better error management.\n    *   **TypeScript Feature**: `try-catch`, custom error classes.\n*   **LLM Call Retries**: Implement retry logic using libraries like `axios-retry` (if using `axios` for HTTP calls) or custom promise-based retry functions.\n    *   **Best Practice**: Exponential backoff for retries.\n*   **Fallback Mechanisms**: Implement checks for agent availability and provide default agents or behaviors.\n\n## Key TypeScript/Node.js Considerations:\n\n*   **Asynchronous Operations**: Node.js is inherently asynchronous. Use `async/await` extensively for I/O, network requests (LLM calls), and any operation that might block the event loop.\n*   **Type Safety**: Leverage TypeScript's static typing to catch errors at compile time. Define interfaces and types for all data structures (Agents, Plans, Tasks, Artifacts, AgentOutput, etc.).\n*   **Modularity**: Structure the project into clear modules (e.g., `agents`, `core`, `llm`, `utils`, `prompts`).\n*   **Dependency Management**: Use `npm` or `yarn` for package management. Consider a monorepo structure (e.g., using Lerna or Nx) if the project grows complex.\n*   **Testing**: Employ testing frameworks like `Jest` or `Mocha` with `Chai` for unit and integration tests. Mock external dependencies (like LLM APIs) for reliable testing.\n"}]},"team":null,"reasoning":null}



Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.