The overall goal is: 'Reimplement runtime using TypeScript in node.js following SOTA coding principles'

Your role's specific goal is: 'Structure workflows and ensure clarity in agent instructions, system prompt engineering'
Your specific sub-task is: 'Define and implement the custom types (Plan, Task, Artifact, etc.) in TypeScript using interfaces or classes, ensuring type safety.'

The team's roles are:
    {"high_level_goal":"Reimplement runtime using TypeScript in node.js following SOTA coding principles","reasoning":"The user wants to reimplement the existing Python runtime in TypeScript for Node.js. This plan breaks down the process into logical steps, starting with understanding the current Python runtime, defining the TypeScript architecture, setting up the development environment, and then iteratively implementing and testing the core components. The roles are assigned to leverage their specific strengths: GPTASe for task extraction, Lyra for prompt engineering and workflow structuring, Fizz La Metta for coordination, and aitutor for knowledge transfer and guidance.","roles":[{"title":"GPTASe","purpose":"Identify and extract each 'Task Agnostic Step' (TAS) from the provided Python runtime documentation and code to understand the core functionalities and structure."},{"title":"Lyra","purpose":"Structure the overall TypeScript project, define the architecture, and engineer the system prompts for the new TypeScript agents based on the extracted TAS."},{"title":"Fizz La Metta","purpose":"Coordinate the iterative development process, ensuring smooth transitions between development phases and managing meta-level communication."},{"title":"aitutor","purpose":"Provide guidance on SOTA TypeScript/Node.js practices, assist with understanding complex concepts from the Python runtime, and facilitate knowledge transfer to the TypeScript environment."}],"tasks":[{"id":"T1","description":"Analyze the provided Python runtime documentation ('runtime.md') and source code to identify core components, functionalities, and design patterns.","role":"GPTASe","agent":"GPTASe","requires":[]},{"id":"T2","description":"Extract Task Agnostic Steps (TAS) from the analysis in T1. These steps should represent the fundamental operations and logic of the runtime, independent of the implementation language.","role":"GPTASe","agent":"GPTASe","requires":["T1"]},{"id":"T3","description":"Based on the extracted TAS (T2), define the high-level architecture for the TypeScript Node.js runtime. This includes identifying key modules, classes, and their interactions.","role":"Lyra","agent":"Lyra","requires":["T2"]},{"id":"T4","description":"Set up the initial TypeScript Node.js project structure, including necessary dependencies, build tools (e.g., tsc, esbuild), and linters/formatters.","role":"Lyra","agent":"Lyra","requires":["T3"]},{"id":"T5","description":"For each identified TAS (T2), research and document the equivalent SOTA TypeScript/Node.js implementation patterns and best practices. Consult aitutor for guidance.","role":"aitutor","agent":"aitutor","requires":["T2"]},{"id":"T6","description":"Begin implementing the core components of the TypeScript runtime based on the defined architecture (T3), TAS (T2), and SOTA practices (T5). Start with foundational elements like Agent, Session, and ExecutionContext.","role":"Lyra","agent":"Lyra","requires":["T3","T4","T5"]},{"id":"T7","description":"Develop the LLM integration layer in TypeScript, ensuring it can interface with various LLM providers similarly to the Python version.","role":"Lyra","agent":"Lyra","requires":["T6"]},{"id":"T8","description":"Implement the Orchestrator agent in TypeScript, including its plan generation logic, leveraging the LLM integration (T7).","role":"Lyra","agent":"Lyra","requires":["T7"]},{"id":"T9","description":"Implement the core Agent class and its execution logic in TypeScript, ensuring it can handle tasks, contexts, and artifacts.","role":"Lyra","agent":"Lyra","requires":["T6","T7"]},{"id":"T10","description":"Define and implement the custom types (Plan, Task, Artifact, etc.) in TypeScript using interfaces or classes, ensuring type safety.","role":"Lyra","agent":"Lyra","requires":["T6"]},{"id":"T11","description":"Integrate the implemented components (T8, T9, T10) into a cohesive system. Set up the main entry point (similar to runtime.sysmain.py) for the Node.js application.","role":"Lyra","agent":"Lyra","requires":["T8","T9","T10"]},{"id":"T12","description":"Write unit and integration tests for the core components of the TypeScript runtime.","role":"Lyra","agent":"Lyra","requires":["T11"]},{"id":"T13","description":"Refactor and optimize the TypeScript code based on test results (T12) and SOTA principles. Ensure code quality, maintainability, and performance.","role":"Lyra","agent":"Lyra","requires":["T12"]},{"id":"T14","description":"Coordinate the iterative development process. Review the progress, identify any blockers, and facilitate communication between team members. Ensure alignment with the high-level goal.","role":"Fizz La Metta","agent":"Fizz La Metta","requires":["T1","T2","T3","T4","T5","T6","T7","T8","T9","T10","T11","T12","T13"]},{"id":"T15","description":"Document the new TypeScript runtime, including architecture, API, and usage instructions, analogous to the provided 'runtime.md'.","role":"Lyra","agent":"Lyra","requires":["T13"]},{"id":"T16","description":"Final review of the reimplemented runtime, tests, and documentation. Ensure all requirements of the high-level goal are met.","role":"Fizz La Metta","agent":"Fizz La Metta","requires":["T14","T15"]}],"team":{"notes":"This plan outlines the steps to reimplement the Python runtime in TypeScript. The roles are defined to leverage each member's expertise. GPTASe will focus on understanding the existing system, Lyra on the new implementation and architecture, Fizz La Metta on coordination, and aitutor on knowledge transfer and best practices. The process is iterative, starting with analysis and moving through implementation, testing, and documentation.","prompts":[{"agent":"GPTASe","role":"Task-Agnostic Step (TAS) extractor","system_prompt":"You are GPTASe, a specialized AI focused on analyzing code and documentation to extract fundamental, language-agnostic steps (Task Agnostic Steps - TAS). Your goal is to break down complex systems into their core operational logic. Analyze the provided Python runtime documentation and code. Identify and list all distinct, essential steps or operations performed by the runtime. For each step, describe it clearly and concisely, focusing on *what* needs to be done rather than *how* it's done in Python. Avoid implementation details specific to Python. Your output should be a structured list of these TAS, ready for translation into another programming language."},{"agent":"Lyra","role":"Prompt Engineer","system_prompt":"You are Lyra, an expert Prompt Engineer and architect. Your task is to design and structure the TypeScript Node.js runtime based on the Task Agnostic Steps (TAS) provided by GPTASe. Define the overall project architecture, including modules, classes, interfaces, and their interactions. Ensure the design adheres to State-Of-The-Art (SOTA) coding principles for TypeScript and Node.js. You will also be responsible for setting up the project structure, dependencies, and initial implementation of core components. Engineer clear and effective system prompts for the new TypeScript agents, guiding their behavior and ensuring alignment with the project goals. Focus on creating a robust, maintainable, and scalable codebase."},{"agent":"Fizz La Metta","role":"Coordinator","system_prompt":"You are Fizz La Metta, the Coordinator. Your role is to oversee the meta-temporal coordination of layered cognition processing for this project. You manage the iterative development cycle, ensuring smooth transitions between tasks and agents. Monitor the progress, identify potential bottlenecks or conflicts, and facilitate communication. Your focus is on the overall flow and coherence of the project, ensuring that all team members are aligned with the high-level goal and that the plan is executed efficiently. Provide high-level feedback and guidance to maintain project momentum."},{"agent":"aitutor","role":"AI Tutor","system_prompt":"You are aitutor, an AI Tutor specializing in knowledge transfer. Your role is to assist the team by providing expertise on State-Of-The-Art (SOTA) TypeScript and Node.js development practices. When requested, explain complex concepts from the original Python runtime and how they can be best implemented in TypeScript. Offer guidance on design patterns, best practices, and potential pitfalls. Facilitate the understanding and adoption of new technologies and methodologies within the team. Your goal is to ensure the team has the necessary knowledge to build a high-quality TypeScript runtime."}]}}

Please use the following outputs from the other agents as your input:

--- Artifact 'files' from (User) in [initial]:
{"task":"initial","files":[{"path":"runtime.md","content":"# Runtime Documentation\n\n## Overview\n\nThis document provides comprehensive documentation for the runtime system, detailing its components, core concepts, API, and usage. The design emphasizes clarity, accessibility, and a clean, minimalist aesthetic, ensuring an informative and visually appealing user experience.\n\n## Table of Contents\n\n*   [Introduction](#introduction)\n*   [Getting Started](#getting-started)\n*   [Core Concepts](#core-concepts)\n*   [API Reference](#api-reference)\n*   [Advanced Usage & Tutorials](#advanced-usage--tutorials)\n*   [Contribution Guide](#contribution-guide)\n*   [Troubleshooting & FAQ](#troubleshooting--faq)\n\n## Introduction\n\n### Overview of Runtime Components\n\nThe runtime system is designed to facilitate the execution of multi-agent workflows. Key components include:\n\n*   **`runtime.agent.Agent`**: The base class for all agents, defining their lifecycle, task execution logic, and interaction patterns. Agents are the fundamental units that perform tasks within the system.\n*   **`runtime.core.Session`**: Manages the overall state and artifacts of a workflow execution. It handles session IDs, artifact storage, and provides an interface for agents to interact with the system's state.\n*   **`runtime.core.ExecutionContext`**: Holds the context for a specific step or round within a workflow. It includes the plan, current step, and a collection of recorded artifacts from previous steps.\n*   **`runtime.orchestrator.Orchestrator`**: A specialized agent responsible for generating and managing the workflow plan. It interprets the high-level goal and breaks it down into a series of tasks for other agents.\n*   **`runtime.llm.LLM`**: An abstract base class that abstracts LLM interactions. It provides a unified interface for various LLM providers and models, supporting a factory pattern for easy integration and interchangeability.\n*   **`runtime.custom_types`**: Defines the Pydantic models used for structured data exchange, including `Plan`, `Task`, `File`, `Artifact`, `Prompt`, and `Team`.\n\n## Getting Started\n\n### Installation\n\nTo install the runtime, navigate to the project's root directory and run:\n\n```bash\npip install .\n```\n\n### Basic Usage\n\nTo run a workflow, use the `runtime.sysmain` module. You need to provide an initial task description (which serves as the high-level goal) and optionally specify the number of rounds and input files.\n\n```bash\npython -m runtime.sysmain \"Your initial task description\" -r 3 -f file1.txt file2.py\n```\n\n*   `\"Your initial task description\"`: The high-level goal for the workflow.\n*   `-r 3`: Specifies that the workflow should run for 3 rounds.\n*   `-f file1.txt file2.py`: Lists input files to be used during the task execution.\n\n## Core Concepts\n\n### Agents\n\nAgents are the core actors in the system. Each agent has a name, role, goal, and can be configured with a specific LLM model and system prompt. They execute tasks based on the provided context and plan.\n\n### Plans and Tasks\n\nA `Plan` is a structured definition of a workflow, comprising a sequence of `Task` objects. Each `Task` has an ID, a description, a responsible role, an assigned agent, and a list of prerequisite task IDs.\n\n### Artifacts\n\nArtifacts are outputs generated by agents during task execution. They can be stored and retrieved using the `Session` and `ExecutionContext` objects. The `ExecutionContext.record_artifact` method is used to save artifacts for the current step, while `ExecutionContext.record_initial` is used for initial inputs.\n\n### `AgentOutput` Model\n\nThe `AgentOutput` Pydantic model defines the structured response format for agents. It includes fields for the main `output` message, an optional `artifact` (containing files), and optional `team` updates (like new system prompts). This model ensures consistent communication between agents.\n\n### LLM Integration\n\nThe `runtime.llm.LLM` class and its subclasses abstract LLM interactions. The `LLM.factory` method allows for easy instantiation of different LLM providers based on configuration.\n\n## API Reference\n\n### `runtime.agent.Agent`\n\n*   **`__init__(self, name: str, role: str, goal: str, model: str, system_prompt: str)`**: Constructor for the `Agent` class.\n*   **`update_system_prompt(self, new_prompt: str)`**: Updates the agent's system prompt.\n*   **`execute_task(self, context: ExecutionContext) -> Optional[str]`**: Executes a task using the LLM based on the provided context. It prepares the prompt, including previous artifacts, and calls the LLM.\n*   **`_run(self, context: ExecutionContext, prompt: str) -> Optional[str]`**: Handles the core LLM interaction, including sending the prompt, processing the response, parsing JSON, and extracting artifacts. It also manages retries for LLM calls.\n\n### `runtime.core.Session`\n\n*   **`__init__(self, session_id: str, agents: list, state: str, session_dir: str, project_root: str)`**: Constructor for the `Session` class.\n*   **`add_artifact(self, name: str, content: Any)`**: Saves an artifact in the session directory. `content` can be a string, dictionary, or list. JSON content is saved with a `.json` extension.\n*   **`get_artifact(self, name: str) -> Any`**: Loads an artifact from the session directory. Returns the content or `None` if an error occurs.\n\n### `runtime.core.ExecutionContext`\n\n*   **`__init__(self, session: Session, plan: Plan, step_index: int, round_num: int, items: Dict[str, ContextItem])`**: Constructor for `ExecutionContext`.\n*   **`next(self)`**: Advances the step index.\n*   **`reset(self)`**: Resets the step index to 0.\n*   **`current_step(self) -> Task`**: Returns the current `Task` object.\n*   **`record_artifact(self, key: str, value: Any, mem: bool = False)`**: Records an artifact for the current step. If `mem` is `True`, it's also stored in the context's memory.\n*   **`record_initial(self, key: str, value: Any)`**: Records initial artifacts provided at the start of the workflow.\n\n### `runtime.custom_types` Module\n\n*   **`Task`**: Represents a single actionable step in a plan.\n    *   `id`: Unique ID for the task.\n    *   `description`: What the task is and how to execute it.\n    *   `role`: Role responsible for performing this task.\n    *   `agent`: Specific agent assigned to this task.\n    *   `requires`: List of prerequisite task IDs.\n*   **`File`**: Represents a generated file.\n    *   `path`: File path or name.\n    *   `content`: Full content of the file.\n*   **`Artifact`**: Output artifact generated by a task.\n    *   `task`: ID of the task that produced this artifact.\n    *   `files`: List of `File` objects included in the artifact.\n*   **`Role`**: Definition of a role in the team.\n    *   `title`: Role title.\n    *   `purpose`: High-level responsibilities and objectives.\n*   **`Plan`**: Overall strategy with reasoning, roles, and tasks.\n    *   `high_level_goal`: The overarching objective.\n    *   `reasoning`: Why this plan is structured this way.\n    *   `roles`: All roles needed for execution.\n    *   `tasks`: Step-by-step ordered tasks.\n*   **`Prompt`**: System prompt assigned to an agent.\n    *   `agent`: Name of the agent this prompt is for.\n    *   `role`: Role context of the agent.\n    *   `system_prompt`: Full text of the system prompt.\n*   **`Team`**: Metadata and coordination for the team.\n    *   `notes`: General notes, reflections, or feedback.\n    *   `prompts`: Updated system prompts for agents.\n*   **`AgentOutput`**: Structured output returned by an agent.\n    *   `output`: Message from the agent.\n    *   `artifact`: Optional `Artifact` generated by the agent.\n    *   `team`: Optional `Team` updates.\n    *   `reasoning`: Explanation of how the agent arrived at this output.\n\n## Advanced Usage & Tutorials\n\n### Advanced Planning\n\nThe planning process is primarily handled by the `Orchestrator` agent. To influence plan generation, you can:\n\n1.  **Refine the High-Level Goal:** A more specific and well-defined goal leads to a more effective plan.\n2.  **Engineer Prompts:** Modifying the system prompt for the `Orchestrator` (e.g., by adjusting `runtime/prompts/general_planning.txt`) can guide its reasoning and task decomposition. This allows for customization of how the orchestrator approaches planning, including task dependencies and role assignments.\n3.  **Provide Initial Files:** Including relevant context files during system startup can help the orchestrator create more informed plans.\n\n## Contribution Guide\n\n### Running Tests\n\nTo run the test suite, navigate to the project's root directory and execute:\n\n```bash\npytest\n```\n\nThis command will discover and run all tests defined within the project.\n\n## Troubleshooting & FAQ\n\n### How do I configure different LLM providers?\n\nLLM providers are configured within the `runtime.yaml` configuration file. You can specify the model name and potentially provider-specific settings (like API keys or base URLs) in the configuration. The `runtime.llm.LLM.factory` method handles the instantiation based on the model name provided (e.g., `ollama:llama3`, `gemini-1.5-flash`, `openai:gpt-4`). Ensure that any necessary API keys or environment variables are set correctly before running the system.\n\n---\n\n*This documentation is styled using a minimalist design aesthetic. The color palette features deep blue (`#0A2540`) and vibrant teal (`#1ABC9C`), with clean typography using 'Inter' for body text and 'Fira Code' for code. The layout is designed for optimal readability and accessibility, with a clear navigation structure and ample white space.*"},{"path":"runtime/agent.py","content":"\"\"\"This module defines the core Agent class and related functionalities.\n\nIt includes the agent's lifecycle methods, task execution logic, and\nhelper functions for agent instantiation and discovery.\n\"\"\"\n\nimport json as JSON\nimport uuid\nfrom dataclasses import dataclass, field\nimport re\nimport logging\nfrom typing import List, Dict, Any, Optional\n\nfrom runtime.llm import LLM\n\nfrom runtime.core import ExecutionContext, Session\n\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import AgentOutput\n\nclass Agent:\n    \"\"\"Represents a runtime agent instance.\"\"\"\n    name: str\n    role: str\n    goal: str\n    model: str\n    system_prompt: str\n    llm: LLM\n\n    def __init__(self, name: str, role: str, goal: str, model: str, system_prompt: str) -> None:\n        self.name = name\n        self.role = role\n        self.goal = goal\n        self.model = model\n        self.system_prompt = system_prompt\n        logger.info(f\"Agent instance created: {self.name} (Role: {self.role}, Model: {self.model})\")\n        self.llm = LLM.factory(model)\n\n    def update_system_prompt(self, new_prompt: str) -> None:\n        \"\"\"Updates the agent's system prompt.\"\"\"\n\n        self.system_prompt = new_prompt\n\n        logger.info(f\"Agent {self.name}'s system prompt updated:\\n{self.system_prompt}\\n\")\n\n    def execute_task(self, context: ExecutionContext) -> Optional[str]:\n        \"\"\"\n        Executes a task using the Generative AI model based on the provided context.\n\n        Args:\n            context (ExecutionContext): The execution context containing goal, plan, and artifacts.\n\n        Returns:\n            Optional[str]: The result of the task execution as a string, or an error string.\n        \"\"\"\n\n        task = context.current_step()\n\n        context.record_artifact(f\"{self.name}_prompt.txt\", self.system_prompt)\n\n        required_task_ids = ['initial']\n        required_task_ids.extend(task.requires)\n\n        required_artifacts = []\n\n        if len(required_task_ids) != 0:\n            for key, artifact in context.items.items():\n                logger.debug(f\"Checking artifact from {key} with task ID {artifact.step.id}\")\n                if artifact.step.id in required_task_ids:\n                    required_artifacts.append(artifact)\n\n        previous_artifacts = \"\\n\\n\".join(\n            f\"--- Artifact '{artifact.name}' from ({artifact.step.role}) in [{artifact.step.id}]:\\n{artifact.content}\"\n            for artifact in required_artifacts\n        )\n\n        task_prompt: List[str] = [\n            f\"The overall goal is: '{context.plan.high_level_goal}'\",\n            f\"Your role's specific goal is: '{self.goal}'\\n\"\n            f\"Your specific sub-task is: '{task.description}'\",\n\n            f\"The team's roles are:\\n    {context.plan.model_dump_json()}\",\n        ]\n\n        if previous_artifacts:\n            task_prompt.append(f\"Please use the following outputs from the other agents as your input:\\n\\n{previous_artifacts}\\n\\n\")\n\n        task_prompt.append(\n            f\"Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.\"\n        )\n\n        #logger.info(f\"Agent '{self.name}' is executing task: {task_prompt[1]}\")\n        #logger.info(f\"Task requires outputs from task IDs: {required_task_ids}\")\n        #logger.info(\n        #    f\"Found {len(required_artifacts)} required artifacts from previous tasks. Previous artifacts preview: {previous_artifacts[:500]}\"\n        #)\n\n        ret = self._run(context, \"\\n\\n\".join(task_prompt))\n        logger.info(f\"Agent '{self.name}' completed task: {task.description}\")\n        return ret\n\n\n    def _run(self, context: ExecutionContext, prompt: str) -> Optional[str]:\n        context.record_artifact(f\"{self.name}_task.txt\", prompt)\n\n        try:\n            response = self.llm.generate_content(   # ignore type\n                model_name=self.model,\n                contents=prompt,\n                system_instruction=self.system_prompt,\n                temperature=0.7,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=AgentOutput\n            )\n            if isinstance(response, AgentOutput):\n                result = response.model_dump_json()\n                print(f\"\\n--- Output:\\n{response.output}\\n\")\n                if response.artifact and response.artifact.files:\n                    for file in response.artifact.files:\n                        print(f\"\\n--- File: {file.path}\\n{file.content}\\n\")\n                        context.session.add_artifact(file.path, file.content)\n            else:\n                result = response or '{}'\n\n                if '```' in result:\n                    match = re.search(r'```(json)?\\s*([\\s\\S]*?)\\s*```', result, re.IGNORECASE)\n                    if match:\n                        # Extract the JSON content from the markdown block\n                        result = match.group(2).strip()\n\n                json_data = JSON.loads(result)\n                if isinstance(json_data, dict):\n                    if \"output\" in json_data and json_data[\"output\"] is not None:\n                        output = JSON.dumps(json_data[\"output\"], indent=4) if isinstance(json_data[\"output\"], dict) else json_data[\"output\"]\n                        logger.info(f\"Output:\\n{output[:2000]}\\n\")\n\n                    if \"artifact\" in json_data and json_data[\"artifact\"] is not None:\n                        if \"files\" in json_data[\"artifact\"]:\n                            for file_data in json_data[\"artifact\"][\"files\"]:\n                                if \"path\" in file_data and \"content\" in file_data:\n                                    print(f\"\\n--- File: {file_data[\"path\"]}\")\n                                    print(f\"{file_data[\"content\"]}\")\n                                    context.session.add_artifact(file_data[\"path\"], file_data[\"content\"])\n                else:\n                    logger.warning(\"Agent output is not in expected AgentOutput format. Processing as plain text.\")\n                    logger.info(f\"Output: '\\n{result[:2000]}\\n'\")\n\n        except Exception as e:\n            logger.exception(f\"Error executing task for {self.name}: {e}\")\n            raise\n\n        return result\n\ndef find_agent_by_role(agents: List[Agent], role: str) -> Optional[Agent]:\n    \"\"\"\n    Finds an agent in a list by its role.\n\n    Args:\n        agents (List[Agent]): A list of Agent objects.\n        role (str): The role of the agent to find.\n\n    Returns:\n        Optional[Agent]: The found Agent object, or None if not found.\n    \"\"\"\n    logger.debug(f\"Searching for agent with role: {role} in agents: {[agent.name for agent in agents]}\")\n    return next((agent for agent in agents if agent.role == role), None)\n"},{"path":"runtime/core.py","content":"\"\"\"This module provides the core data structures for the multi-agent runtime.\n\nIt defines the ExecutionContext for managing workflow state and the Session\nfor handling artifacts and session-specific data.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List\nimport uuid\nimport os\nimport logging\nimport json\n\nfrom runtime.custom_types import Plan, Task\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContextItem:\n    def __init__(self, name: str, content: Any, step: Task):\n        self.name = name\n        self.content = content\n        self.step = step\n\n\n@dataclass\nclass ExecutionContext:\n    \"\"\"Holds the state and context for a multi-agent workflow.\"\"\"\n    session: 'Session'\n    plan: Plan\n    step_index: int = 0\n    round_num: int = 0\n    items: Dict[str, ContextItem] = field(default_factory=dict)\n\n    def next(self) -> None:\n        self.step_index += 1\n\n    def reset(self) -> None:\n        self.step_index = 0\n\n    def current_step(self) -> Task:\n        \"\"\"Returns the current step in the plan.\"\"\"\n        return self.plan.tasks[self.step_index]\n\n    def _remember_artifact(self, key: str, value: Any, step: Task | None = None) -> None:\n        \"\"\"Remembers an artifact from a step's execution for future tasks.\"\"\"\n        self.items[key] = ContextItem(name=key, content=value, step=step or self.current_step())\n\n    def record_artifact(self, key: str, value: Any, mem: bool = False) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n            mem (bool): If True, the artifact is also stored in the execution context's memory.\n        \"\"\"\n        artifact_key = f\"__{self.round_num}/__step_{self.step_index}_{key}\"\n        self.session.add_artifact(artifact_key, value)\n        if mem:\n            self._remember_artifact(artifact_key, value)\n\n    def record_initial(self, key: str, value: Any) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n        \"\"\"\n        self._remember_artifact(key, value, step=Task(\n            id='initial',\n            description='Initial files provided by the user',\n            role='User',\n            agent='User',\n            requires=[]\n        ))\n\n\n\n\n@dataclass\nclass Session:\n    \"\"\"Manages the runtime context for a task, including session ID and artifact storage.\"\"\"\n    session_id: str = field(default_factory=lambda: f\"session_{uuid.uuid4()}\")\n    agents: list = field(default_factory=list) # Type hint will be updated later to List[Agent]\n    state: str = \"initialized\"\n    session_dir: str = field(init=False)\n    project_root: str = field(default_factory=str)\n\n    def __post_init__(self) -> None:\n        \"\"\"Initializes the session directory.\"\"\"\n        if not self.project_root:\n            self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n        self.session_dir = os.path.join(self.project_root, 'sessions', self.session_id)\n        os.makedirs(self.session_dir, exist_ok=True)\n        logger.info(f\"Session created: {self.session_id} (Directory: {self.session_dir})\")\n\n    def add_artifact(self, name: str, content: Any) -> None:\n        \"\"\"\n        Saves an artifact in the session directory.\n\n        Args:\n            name (str): The name of the artifact file.\n            content (Any): The content to write to the artifact file. Can be dict, list, or string.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            os.makedirs(os.path.dirname(artifact_path), exist_ok=True)\n            with open(artifact_path, 'w', encoding='utf-8') as f:\n                if isinstance(content, (dict, list)):\n                    json.dump(content, f, indent=4)\n                else:\n                    f.write(str(content))\n            logger.info(f\"Artifact '{name}' saved in session {self.session_id}.\")\n        except (TypeError, IOError) as e:\n            logger.error(f\"Error saving artifact '{name}': {e}\")\n\n    def get_artifact(self, name: str) -> Any:\n        \"\"\"\n        Loads an artifact from the session directory.\n\n        Args:\n            name (str): The name of the artifact file to load.\n\n        Returns:\n            Any: The content of the artifact, or None if an error occurs.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            with open(artifact_path, 'r', encoding='utf-8') as f:\n                if name.endswith('.json'):\n                    return json.load(f)\n                return f.read()\n        except (FileNotFoundError, json.JSONDecodeError, IOError) as e:\n            logger.error(f\"Error retrieving artifact '{name}': {e}\")\n            return None\n"},{"path":"runtime/custom_types.py","content":"\"\"\"This module defines custom data types and Pydantic models for the runtime.\n\nIt ensures data consistency and validation across different modules by providing\ncentralized, well-defined data structures for plans, tasks, and agent outputs.\n\"\"\"\n\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass Task(BaseModel):\n    \"\"\"A single actionable step in a plan.\"\"\"\n    id: str = Field(..., description=\"Unique ID for the task (e.g., 'T1', 'T2').\")\n    description: str = Field(..., description=\"Detailed description of what to do for this task.\")\n    role: str = Field(..., description=\"The role responsible for this task.\")\n    agent: str = Field(..., description=\"The agent assigned to this task.\")\n    requires: List[str] = Field(..., description=\"List of tasks (IDs) whose outputs are required (e.g. ['T1', 'T2']).\")\n\n\nclass File(BaseModel):\n    \"\"\"Represents a generated file.\"\"\"\n    path: str = Field(..., description=\"File path or name (e.g., 'src/main.py').\")\n    content: str = Field(..., description=\"Full content of the file.\")\n\n\nclass Artifact(BaseModel):\n    \"\"\"Output artifact generated by a task.\"\"\"\n    task: str = Field(..., description=\"ID of the task that produced this artifact (e.g., 'T1').\")\n    files: List[File] = Field(..., description=\"List of files created or modified by the task.\")\n\n\nclass Role(BaseModel):\n    \"\"\"Definition of a role in the team.\"\"\"\n    title: str = Field(..., description=\"Title of the role (e.g., 'Engineer', 'Reviewer').\")\n    purpose: str = Field(..., description=\"The main purpose and goal of this role.\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"System prompt assigned to an agent.\"\"\"\n    agent: str = Field(..., description=\"Name of the agent this prompt is for. Must match exactly an existing agent.\")\n    role: str = Field(..., description=\"Role context of the agent.\")\n    system_prompt: str = Field(..., description=\"Full text of the system prompt for this agent.\")\n\n\nclass Team(BaseModel):\n    \"\"\"Metadata and coordination for the team.\"\"\"\n    notes: str = Field(..., description=\"General notes or feedback about the team's performance or the plan.\")\n    prompts: List[Prompt] = Field(default_factory=list, description=\"A list of new or updated system prompts for any agent in the team.\")\n\n\nclass Plan(BaseModel):\n    \"\"\"Overall strategy with reasoning, roles, and tasks.\"\"\"\n    high_level_goal: str = Field(..., description=\"The main goal this plan is designed to achieve.\")\n    reasoning: str = Field(..., description=\"A brief explanation of the plan's structure and strategy.\")\n    roles: List[Role] = Field(..., description=\"List of all roles required to execute the plan.\")\n    tasks: List[Task] = Field(..., description=\"A step-by-step sequence of tasks to be executed in order.\")\n    team: Optional[Team] = Field(default=None, description=\"Updates to team configuration or system prompts.\")\n\n\nclass AgentOutput(BaseModel):\n    \"\"\"Structured output returned by an agent after executing a task.\"\"\"\n    output: str = Field(..., description=\"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\")\n    artifact: Optional[Artifact] = Field(default=None, description=\"Any files created or modified by the agent during the task.\")\n    team: Optional[Team] = Field(default=None, description=\"Updates to team configuration or system prompts.\")\n    reasoning: Optional[str] = Field(default=None, description=\"Explanation of how the agent arrived at this output.\")\n\n\n\nif __name__ == \"__main__\":\n    import json\n    print(json.dumps(AgentOutput.model_json_schema(), indent=4))\n"},{"path":"runtime/__init__.py","content":"# This file makes the 'runtime' directory a Python package.\n\nfrom .custom_types import Role, Task, Plan, Artifact, File, Prompt, Team, AgentOutput\n\nfrom .agent import Agent\nfrom .core import Session\nfrom .orchestrator import Orchestrator\nfrom .system import System\n"},{"path":"runtime/llm.py","content":"\"\"\"This module abstracts interactions with Large Language Models (LLMs).\n\nIt provides a unified interface for various LLM providers and models,\nallowing for easy integration and interchangeability of different LLMs.\n\"\"\"\n\nimport json\nimport os\nimport time\nfrom google import genai\nfrom google.genai import types\nfrom ollama import Client as Ollama\nfrom typing import Optional, Any, Dict\nfrom abc import ABC, abstractmethod\nimport logging\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLM(ABC):\n    \"\"\"\n    Abstract base class for Large Language Models.\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    def __init__(self) -> None:\n        pass\n\n    @abstractmethod\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: Any = None) -> Optional[str]: # type: ignore\n        \"\"\"Generates content using an LLM.\"\"\"\n        pass\n\n    @staticmethod\n    def factory(species: str) -> 'LLM':\n        print(f\"LLM Factory: Creating LLM instance for species '{species}'\")\n        if species == 'Olli':\n            return Olli()\n        elif species == 'Kimi':\n            return Kimi()\n        if species.startswith('ollama:'):\n            # Extract model name after 'ollama:'\n            model_name = species.split(':', 1)[1]\n            return Olli(species=model_name)\n        if species.startswith('opi:'):\n            # Extract model name after 'opi:'\n            model_name = species.split(':', 1)[1]\n            return Opi(species=model_name)\n        if species.startswith('mistral:'):\n            # Extract model name after 'mistral:'\n            model_name = species.split(':', 1)[1]\n            return Mistral(species=model_name)\n        return Gemini(species)\n\n\nclass Gemini(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients: Dict[str, genai.Client] = {} # Class-level cache for clients\n\n    def __init__(self, species: str) -> None:\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        config = types.GenerateContentConfig(\n            system_instruction=system_instruction,\n            temperature=temperature,\n            response_mime_type=response_mime_type,\n            response_schema=response_schema,\n            max_output_tokens=50000,\n        )\n\n        # try three times\n        for _ in range(3):\n            try:\n                response = client.models.generate_content(\n                    model=model_name,\n                    contents=contents,\n                    config=config,\n                )\n                if not response.candidates or response.candidates[0].content is None or response.candidates[0].content.parts is None or not response.candidates[0].content.parts or response.candidates[0].content.parts[0].text is None:\n                    raise ValueError(f\"Gemini: No content in response from model {model_name}. Retrying...\")\n                break\n            except Exception as ex:\n                logger.exception(f\"Error generating content with model {model_name}: {ex}\")\n                # If it's the last retry, return None\n                if _ == 2:\n                    return None\n                logger.warning(f\"Retrying content generation for model {model_name}...\")\n                time.sleep(10)\n                #return None\n\n        if isinstance(response.parsed, BaseModel):\n            return response.parsed\n\n        if response_mime_type == 'application/json':\n            try:\n                # Attempt to parse the JSON response\n                json_output = response.candidates[0].content.parts[0].text.strip()\n                # Validate against schema if provided\n                if response_schema:\n                    response_schema.model_validate_json(json_output)\n                return json_output\n            except Exception as ex:\n                logger.exception(f\"Error parsing or validating JSON response: {ex}\")\n\n        return response.candidates[0].content.parts[0].text.strip()\n\n    def _get_client(self) -> genai.Client:\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if self.species not in Gemini._clients:\n                logger.info(f\"Initializing GenAI client for species {self.species}\")\n                Gemini._clients[self.species] = genai.Client()\n            return Gemini._clients[self.species]\n        except Exception as e:\n            logger.exception(f\"Error initializing GenAI client: {e}\")\n            raise\n\n\nclass Olli(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Olli'):\n        self.client = None\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        print(f\"Olli: Using model {model_name} with temperature {temperature}\")\n        client = self._get_client(species=self.species)\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n            fmt = response_schema.model_json_schema(by_alias=False, mode='serialization')\n            print(f\"Olli: Using response format {fmt}\")\n            response = client.generate(\n                model=self.species,#model_name,\n                prompt=contents,\n                format=fmt,\n                options={\"temperature\": temperature, \"system_instruction\": system_instruction},\n                stream=True\n            )\n            for chunk in response:\n                if hasattr(chunk, \"response\") and chunk.response:\n                    print(chunk.response, end=\"\")\n                    out += chunk.response\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    @staticmethod\n    def _get_client(species: str):\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if (Olli._clients is None):\n                Olli._clients = {}\n            if species not in Olli._clients:\n                Olli._clients[species] = Ollama()#base_url='http://localhost:11434')\n            return Olli._clients[species]\n        except Exception as e:\n            logger.exception(f\"Error initializing Ollama client: {e}\")\n            return None\n\n\n\n\n\nfrom huggingface_hub import InferenceClient, ChatCompletionInputResponseFormatText, ChatCompletionInputResponseFormatJSONObject, ChatCompletionInputResponseFormatJSONSchema, ChatCompletionInputJSONSchema\n\n\nclass Kimi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Kimi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            stream = client.chat.completions.create(\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=ChatCompletionInputResponseFormatText() if response_mime_type == 'text/plain' else ChatCompletionInputResponseFormatJSONObject() #if response_schema is None else ChatCompletionInputResponseFormatJSONSchema(json_schema=ChatCompletionInputJSONSchema(name=response_schema.model_json_schema()))\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a inference client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Kimi._clients:\n                    Kimi._clients[self.species] = InferenceClient(\n                        provider=\"featherless-ai\",\n                        api_key=os.environ.get(\"HF_TOKEN\"),\n                        model=\"moonshotai/Kimi-K2-Instruct\"\n                    )\n            return Kimi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing GenAI client: {e}\")\n            return None\n\n\n\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import ResponseFormat\n\n\nclass Opi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Opi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            response_format: ResponseFormat = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = {\"type\": \"json_schema\", \"json_schema\": response_schema.model_json_schema()}   # type: ignore\n\n            print(f\"Opi: Using response format {response_format}\")\n\n            stream = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=response_format\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a OpenAI client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Opi._clients:\n                    Opi._clients[self.species] = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"), base_url=os.environ.get(\"OPENAI_API_BASE\"))\n            return Opi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing OpenAI client: {e}\")\n            return None\n\n\n\n##\n\n\nfrom mistralai import Mistral as MistralClient\nfrom mistralai.extra.utils import response_format_from_pydantic_model\n\nclass Mistral(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Mistral'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified Mistral model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"mistral-large-latest\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            str: The response from the Mistral model.\n        \"\"\"\n        client = Mistral._get_client(species=self.species)\n        if not client:\n            return None\n        try:\n            messages = []\n            if system_instruction:\n                messages.append({\"role\": \"system\", \"content\": system_instruction})\n            messages.append({\"role\": \"user\", \"content\": contents})\n\n            print(f\"Mistral: Using model {model_name} with temperature {temperature}\")\n\n            response_format = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = response_format_from_pydantic_model(response_schema)  # type: ignore\n\n\n            out = \"\"\n            stream = client.chat.stream(\n                model=self.species,#model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=50000,\n                response_format=response_format # type: ignore\n            )\n            for chunk in stream:\n                if chunk.data.choices[0].delta.content is None:\n                    continue\n                print(chunk.data.choices[0].delta.content, end=\"\")\n                out += str(chunk.data.choices[0].delta.content)\n\n            return out\n        except Exception as e:\n            print(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    @staticmethod\n    def _get_client(species: str):\n        \"\"\"\n        Returns a Mistral client instance.\n        \"\"\"\n        try:\n            if Mistral._clients is None:\n                Mistral._clients = {}\n            if species not in Mistral._clients:\n                api_key = os.environ.get(\"MISTRAL_API_KEY\")\n                if not api_key:\n                    raise ValueError(\"MISTRAL_API_KEY environment variable not set.\")\n                Mistral._clients[species] = MistralClient(api_key=api_key)\n            return Mistral._clients[species]\n        except Exception as e:\n            logger.exception(f\"Error initializing Mistral client: {e}\")\n            return None\n"},{"path":"runtime/log.py","content":"\"\"\"This module configures and sets up the logging for the application.\n\nIt defines the log format, level, and handlers (console, file, JSONL),\nproviding a centralized and consistent logging setup.\n\"\"\"\n\nimport logging\nimport os\nimport re\nimport json\nfrom logging.handlers import RotatingFileHandler\nfrom colorama import Fore, Style\n\n# --- Configuration ---\nLOG_LEVEL = \"INFO\"\nLOG_FILE_DIR = \"logs\"\nLOG_FILE_NAME = \"app.log\"\nLOG_FILE_PATH = os.path.join(LOG_FILE_DIR, LOG_FILE_NAME)\nMAX_FILE_SIZE = 1024 * 1024  # 1 MB\nBACKUP_COUNT = 5\n\n# --- Colored Formatter ---\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"\n    A custom logging formatter that adds colors to the log messages.\n    It can color based on log level, or on regex matching of the message.\n    \"\"\"\n    # ANSI escape codes for colors\n    COLORS = {\n        'DEBUG': Fore.YELLOW,\n        'INFO': Fore.CYAN,\n        'WARNING': Fore.YELLOW,\n        'ERROR': Fore.RED,\n        'CRITICAL': Fore.MAGENTA,\n        'RESET': Style.RESET_ALL\n    }\n\n    # Regex-based coloring rules (pattern, color)\n    REGEX_RULES_MESSAGES = [\n        (re.compile(r\"Agent instance created:\"), Fore.YELLOW),\n        (re.compile(r\"Agent '.*' system prompt updated.\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing task\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing step\"), Fore.LIGHTRED_EX),\n        (re.compile(r\"Agent '.*' completed task\"), Fore.BLUE),\n        (re.compile(r\"Warning: Agent output is not in expected AgentOutput format.\"), Fore.YELLOW),\n        (re.compile(r\"System Runtime\"), Fore.CYAN),\n        (re.compile(r\"Error: No agents could be instantiated.\"), Fore.RED),\n        (re.compile(r\"Session created:\"), Fore.GREEN),\n        (re.compile(r\"Artifact '.*' saved in session .*\"), Fore.LIGHTBLUE_EX),\n        (re.compile(r\"Error saving artifact '.*'\"), Fore.RED),\n        (re.compile(r\"Files provided:\"), Fore.MAGENTA),\n        (re.compile(r\"Error reading file .*\"), Fore.RED),\n        (re.compile(r\"Generated plan:\"), Fore.WHITE),\n        (re.compile(r\"Orchestration failed: Could not generate a valid plan.\"), Fore.RED),\n        (re.compile(r\"Orchestrator .* is generating a plan for\"), Fore.BLUE),\n        (re.compile(r\"Orchestrator .* is starting workflow round\"), Fore.CYAN),\n        (re.compile(r\"Orchestrator has completed workflow round\"), Fore.CYAN),\n        (re.compile(r\"Agent .* provided new prompts.\"), Fore.LIGHTGREEN_EX),\n        (re.compile(r\"Target agent '.*' not found for prompt update.\"), Fore.YELLOW),\n        (re.compile(r\"Error generating or validating plan for .*\"), Fore.RED),\n    ]\n\n    COLORS_NAMES = {\n        'runtime.orchestrator': Fore.LIGHTYELLOW_EX,\n        'runtime.agent': Fore.LIGHTWHITE_EX,\n        'runtime.sysmain': Fore.LIGHTRED_EX,\n        'runtime.core': Fore.LIGHTBLUE_EX,\n        'runtime.bootstrap': Fore.LIGHTRED_EX,\n        'runtime.loader': Fore.LIGHTRED_EX,\n        'runtime.llm': Fore.LIGHTMAGENTA_EX,\n        'runtime.util': Fore.GREEN,\n        'runtime.factory': Fore.LIGHTRED_EX,\n        'runtime.paths': Fore.BLUE,\n        'runtime.custom_types': Fore.CYAN,\n    }\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"\n        Formats the log record with the appropriate color.\n        \"\"\"\n        log_message = super().format(record)\n\n        # Check for regex-based coloring first\n        for pattern, color in self.REGEX_RULES_MESSAGES:\n            if pattern.search(record.getMessage()):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Check for logger name-based coloring\n        for name_prefix, color in self.COLORS_NAMES.items():\n            if record.name.startswith(name_prefix):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Fallback to level-based coloring\n        return f\"{self.COLORS.get(record.levelname, self.COLORS['RESET'])}{log_message}{self.COLORS['RESET']}\"\n\n# --- Setup ---\ndef setup_logging(level: str = LOG_LEVEL) -> None:\n    \"\"\"\n    Sets up logging for the application.\n    \"\"\"\n    # Create log directory if it doesn't exist\n    if not os.path.exists(LOG_FILE_DIR):\n        os.makedirs(LOG_FILE_DIR)\n\n    # Get the root logger\n    logger = logging.getLogger()\n    logger.setLevel(level)\n\n    # --- Console Handler ---\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(level)\n\n    # --- File Handler ---\n    file_handler = RotatingFileHandler(\n        LOG_FILE_PATH,\n        maxBytes=MAX_FILE_SIZE,\n        backupCount=BACKUP_COUNT\n    )\n    file_handler.setLevel(level)\n\n    # --- Formatters ---\n    console_formatter = ColoredFormatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    file_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n\n    class JsonFormatter(logging.Formatter):\n        def format(self, record: logging.LogRecord):\n            log_record = {\n                \"timestamp\": self.formatTime(record, self.datefmt),\n                \"name\": record.name,\n                \"level\": record.levelname,\n                \"message\": record.getMessage(),\n                \"pathname\": record.pathname,\n                \"lineno\": record.lineno,\n                \"funcName\": record.funcName,\n                \"process\": record.process,\n                \"thread\": record.thread,\n                \"threadName\": record.threadName,\n            }\n            if record.exc_info:\n                log_record[\"exc_info\"] = self.formatException(record.exc_info)\n            if record.stack_info:\n                log_record[\"stack_info\"] = self.formatStack(record.stack_info)\n            return json.dumps(log_record)\n\n    # --- Add Formatter to Handlers ---\n    console_handler.setFormatter(console_formatter)\n    file_handler.setFormatter(file_formatter)\n\n    # --- Add Handlers to Logger ---\n    # Avoid adding handlers multiple times\n    if not logger.handlers:\n        logger.addHandler(console_handler)\n        logger.addHandler(file_handler)\n\n        # --- JSONL File Handler ---\n        jsonl_file_handler = RotatingFileHandler(\n            os.path.join(LOG_FILE_DIR, \"app.jsonl\"),\n            maxBytes=MAX_FILE_SIZE,\n            backupCount=BACKUP_COUNT\n        )\n        jsonl_file_handler.setLevel(level)\n        jsonl_file_handler.setFormatter(JsonFormatter())\n        logger.addHandler(jsonl_file_handler)\n\n# --- Main Application Logger ---\n# It's good practice to use a named logger instead of the root logger directly\napp_logger = logging.getLogger(__name__)\n\n# --- Example Usage ---\nif __name__ == '__main__':\n    setup_logging()\n    app_logger.info(\"Logging setup complete.\")\n    app_logger.debug(\"This is a debug message.\")\n    app_logger.warning(\"This is a warning message.\")\n    app_logger.error(\"This is an error message.\")\n"},{"path":"runtime/orchestrator.py","content":"\"\"\"This module defines the Orchestrator agent and its planning capabilities.\n\nThe Orchestrator is a specialized agent responsible for generating, managing,\nand executing the workflow plan based on a high-level goal. This is being\nrefactored to move workflow execution to the System class.\n\"\"\"\n\nimport os\nimport json\nfrom typing import List, Dict, Optional, Tuple\nimport logging\n\nfrom pydantic import BaseModel, Field, ValidationError\n\nfrom runtime.agent import Agent\nfrom runtime.core import Session\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import Plan, File\n\nclass Orchestrator(Agent):\n    \"\"\"An agent responsible for creating and managing a plan for multi-agent workflows.\"\"\"\n    team: Dict[str,Agent] = {}\n\n    def generate_plan(self, session: Session, high_level_goal: str, files: List[File] = []) -> Optional[Plan]:\n        \"\"\"\n        Invokes the language model to get a structured plan for the given high-level goal.\n        The plan includes a sequence of tasks and the roles responsible for them.\n\n        Args:\n            session (Session): The current session object.\n            high_level_goal (str): The high-level goal for which to generate a plan.\n            file_contents (List[Tuple[str, str]]): A list of file paths and their contents to provide context to the LLM.\n\n        Returns:\n            Optional[Plan]: The generated plan as a Pydantic object, or None if an error occurs.\n        \"\"\"\n        logger.info(f\"Orchestrator {self.name} is generating a plan for: '{high_level_goal}'\")\n        if not self.llm or not self.team:\n            logger.error(\"Orchestrator client or team not initialized. Cannot generate plan.\")\n            return None\n\n        # Construct a description of the available agents and their roles/goals\n        team_description = \"\\n\".join(\n            f\"- Name: '{agent.name}'\\n\"\n            f\"  Role: `{agent.role}`\\n\"\n            f\"  Goal: \\\"{agent.goal}\\\"\"\n            for agent in self.team.values()\n        )\n\n        # Load the general planning prompt template\n        t20_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) # Not used\n        general_prompt_template = read_file(os.path.join(t20_root, \"prompts\", \"general_planning.txt\")).strip()\n        planning_prompt_parts = [general_prompt_template.format(\n            high_level_goal=high_level_goal,\n            team_description=team_description\n        )]\n\n        # Include file contents if provided\n        file_section = []\n        for file in files:\n            file_section.append(f\"\\n--- File: '{file.path}'\\n```\\n{file.content}\\n```\")\n        planning_prompt_parts.append(\"\\n\".join(file_section))\n\n        logger.debug(\"Planning prompt parts for LLM: %s\", planning_prompt_parts)\n\n        # Combine prompt parts for the LLM call\n        full_prompt_for_llm = \"\\n\\n\".join(planning_prompt_parts)\n        logger.info(f\"Orchestrator {self.name} Planning Prompt:\\n{full_prompt_for_llm}\")\n\n        session.add_artifact(\"planning_prompt.txt\", full_prompt_for_llm)\n\n        try:\n            response = self.llm.generate_content(\n                model_name=self.model,\n                contents=\"\\n\\n\".join(planning_prompt_parts),\n                system_instruction=self.system_prompt,\n                temperature=0.1,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=Plan\n            )\n            if isinstance(response, Plan):\n                plan = response\n            else:\n                plan = Plan.model_validate_json(response or '{}')\n        except (ValidationError, json.JSONDecodeError) as e:\n            logger.exception(f\"Error generating or validating plan for {self.name}: {e}\")\n            return None\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during plan generation for {self.name}: {e}\")\n            return None\n\n        return plan\n"},{"path":"runtime/paths.py","content":"\"\"\"\nCentralized path constants for the project.\n\nThis module defines constants for directory and file names, allowing them to be\nreused consistently throughout the application.\n\"\"\"\n\n# Directory Names\nAGENTS_DIR_NAME = \"agents\"\nCONFIG_DIR_NAME = \"config\"\nPROMPTS_DIR_NAME = \"prompts\"\n\n# File Names\nRUNTIME_CONFIG_FILENAME = \"runtime.yaml\""},{"path":"runtime/sysmain.py","content":"\"\"\"This module serves as the main entry point for the command-line interface.\n\nIt parses command-line arguments and initiates the system bootstrap process,\nacting as the primary interface for running the multi-agent system.\n\"\"\"\n\nimport os\nimport argparse\nimport logging\n\nfrom runtime.log import setup_logging\nfrom runtime.system import System\nfrom runtime.custom_types import File\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\ndef parse_command_line_arguments() -> argparse.Namespace:\n    \"\"\"\n    Parses command-line arguments for the runtime system.\n\n    Returns:\n        An argparse.Namespace object containing the parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Run the Gemini agent runtime.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    general_group = parser.add_argument_group(\"General Settings\")\n    general_group.add_argument(\"-p\", \"--plan-only\", action=\"store_true\", help=\"Generate only the plan without executing tasks.\")\n    general_group.add_argument(\"-r\", \"--rounds\", type=int, default=1, help=\"The number of rounds to execute the workflow.\")\n    general_group.add_argument(\"-f\", \"--files\", nargs='*', help=\"List of files to be used in the task.\", default=[])\n\n    orchestrator_group = parser.add_argument_group(\"Orchestrator Settings\")\n    orchestrator_group.add_argument(\"-o\", \"--orchestrator\", type=str, help=\"The name of the orchestrator to use.\", default=\"Meta-AI\")\n    orchestrator_group.add_argument(\"-m\", \"--model\", type=str, help=\"Default LLM model to use.\", default=\"gemini-2.5-flash-lite\")\n\n    parser.add_argument(\"task\", type=str, help=\"The initial task for the orchestrator to perform.\")\n\n    args = parser.parse_args()\n\n    if args.rounds < 1:\n        parser.error(\"Number of rounds must be at least 1.\")\n    if not args.task:\n        parser.error(\"The task argument is required.\")\n\n    return args\n\ndef setup_application_logging(log_level: str = \"INFO\") -> None:\n    \"\"\"\n    Sets up the global logging configuration for the application.\n\n    Args:\n        log_level: The desired logging level (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\n    \"\"\"\n    try:\n        setup_logging(level=log_level)\n        logger.info(f\"Logging initialized with level: {log_level}\")\n    except ValueError as e:\n        logger.error(f\"Invalid log level '{log_level}': {e}\")\n        raise\n\ndef system_main() -> None:\n    \"\"\"\n    Main entry point for the runtime system.\n    Parses arguments, sets up logging and configuration, and runs the system.\n    \"\"\"\n    try:\n        # 1. Parse command line arguments\n        args = parse_command_line_arguments()\n        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\n        # 2. Initial logging setup\n        setup_application_logging()\n\n        # 3. Convert file paths to File objects\n        file_objects = []\n        for file_path in args.files:\n            try:\n                content = read_file(file_path)\n                if content.startswith(\"Error:\"):\n                    logger.warning(f\"Skipping file '{file_path}': {content}\")\n                    continue\n                file_objects.append(File(path=file_path, content=content))\n            except Exception as e:\n                logger.error(f\"Error processing file '{file_path}': {e}\")\n\n        # 4. Instantiate and set up the system\n        system = System(root_dir=project_root, default_model=args.model)\n        system.setup(orchestrator_name=args.orchestrator)\n\n        # 5. Re-configure logging based on loaded config\n        log_level = system.config.get(\"logging_level\", \"INFO\")\n        setup_application_logging(log_level=log_level)\n\n        # 6. Start the system's main workflow\n        plan = system.start(\n            high_level_goal=args.task,\n            files=file_objects\n        )\n        if args.plan_only:\n            logger.info(\"Plan-only mode: Workflow execution skipped.\")\n            return\n\n        # 7. Run the system's main workflow\n        system.run(\n            plan=plan,\n            rounds=args.rounds,\n            files=file_objects\n        )\n\n    except (FileNotFoundError, RuntimeError) as e:\n        logger.exception(f\"A critical error occurred: {e}\")\n        exit(1)\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred during system main execution: {e}\")\n        exit(1)\n\n\nif __name__ == \"__main__\":\n    system_main()\n"},{"path":"runtime/system.py","content":"\"\"\"This module defines the core System class for the multi-agent runtime.\n\nIt encapsulates the state and behavior of the entire system, including\nconfiguration loading, agent instantiation, and workflow execution.\n\"\"\"\n\nimport os\nimport json\nimport yaml\nfrom glob import glob\n\nimport logging\nfrom typing import Any, List, Optional, Dict\n\nfrom .core import Session, ExecutionContext\nfrom .agent import Agent, find_agent_by_role\nfrom .orchestrator import Orchestrator\nfrom .log import setup_logging\nfrom .paths import AGENTS_DIR_NAME, CONFIG_DIR_NAME, PROMPTS_DIR_NAME, RUNTIME_CONFIG_FILENAME\n\nfrom .custom_types import AgentOutput, Artifact, Plan, File\nlogger = logging.getLogger(__name__)\n\nclass System:\n    \"\"\"\n    Represents the entire multi-agent system, handling setup, execution, and state.\n    \"\"\"\n    def __init__(self, root_dir: str, default_model: str = \"Olli\"):\n        \"\"\"\n        Initializes the System.\n\n        Args:\n            root_dir (str): The root directory of the project.\n            default_model (str): The default LLM model to use.\n        \"\"\"\n        self.root_dir = root_dir\n        self.default_model = default_model\n        self.config: dict = {}\n        self.agents: List[Agent] = []\n        self.session: Optional[Session] = None\n        self.orchestrator: Optional[Orchestrator] = None\n\n    def setup(self, orchestrator_name: Optional[str] = None) -> None:\n        \"\"\"\n        Sets up the system by loading configurations and instantiating agents.\n\n        Args:\n            orchestrator_name (str, optional): The name of the orchestrator to use.\n\n        Raises:\n            RuntimeError: If no agents or orchestrator can be set up.\n        \"\"\"\n        logger.info(\"--- System Setup ---\")\n        print(f\"Using default model: {self.default_model}\")\n\n        self.config = self._load_config(os.path.join(self.root_dir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME))\n        agent_specs = self._load_agent_templates(os.path.join(self.root_dir, AGENTS_DIR_NAME), self.config, self.default_model)\n        prompts = self._load_prompts(os.path.join(self.root_dir, PROMPTS_DIR_NAME))\n\n        all_team_members = {member.lower() for spec in agent_specs if 'team' in spec for member in spec['team']}\n\n        agents: List[Agent] = []\n        for spec in agent_specs:\n            spec[\"model\"] = spec.get(\"model\", self.default_model)\n            if spec['name'].lower() not in all_team_members:\n                agent = self._instantiate_agent(spec, prompts, agent_specs)\n                if agent:\n                    agents.append(agent)\n        self.agents = agents\n\n        if not self.agents:\n            raise RuntimeError(\"No agents could be instantiated. System setup failed.\")\n\n        orchestrator: Optional[Agent] = None\n        if orchestrator_name:\n            orchestrator = next((agent for agent in self.agents if agent.name.lower() == orchestrator_name.lower()), None)\n        else:\n            orchestrator = find_agent_by_role(self.agents, 'Orchestrator')\n\n        if not orchestrator:\n            error_msg = f\"Orchestrator with name '{orchestrator_name}' not found.\" if orchestrator_name else \"Orchestrator with role 'Orchestrator' not found.\"\n            raise RuntimeError(f\"{error_msg} System setup failed.\")\n\n        if not isinstance(orchestrator, Orchestrator):\n            raise RuntimeError(f\"Agent '{orchestrator.name}' is not a valid Orchestrator instance. System setup failed.\")\n\n        self.orchestrator = orchestrator\n        self.session = Session(agents=self.agents, project_root=\"./\")\n        logger.info(\"--- System Setup Complete ---\")\n\n    def start(self, high_level_goal: str, files: List[File] = []) -> Plan:\n        \"\"\"\n        Runs the main workflow of the system.\n\n        Args:\n            initial_task (str): The initial task for the orchestrator to perform.\n            plan_only (bool): If True, only generates the plan without executing tasks.\n            rounds (int): The number of rounds to execute the workflow.\n            files (List[str]): List of files to be used in the task.\n\n        Raises:\n            RuntimeError: If the system is not set up before running.\n        \"\"\"\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call setup() before start().\")\n\n        plan: Optional[Plan] = self.orchestrator.generate_plan(self.session, high_level_goal, files)\n        if not plan:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        logger.debug(f\"{plan.model_dump_json()}\")\n\n        if not plan or not plan.tasks or not plan.roles:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        print(f\"\\n\\nInitial Plan: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n        self.session.add_artifact(\"initial_plan.json\", plan.model_dump_json(indent=4))\n\n        return plan\n\n    def run(self, plan: Plan, rounds: int = 1, files: List[File] = []) -> None:\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call start() before run().\")\n\n        logger.info(\"--- Starting Workflow ---\")\n\n        context = ExecutionContext(session=self.session, plan=plan)\n\n        print(f\"\\n\\nInitial Files: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n\n        #context.record_initial(\"files\", \"\\n\".join([file.model_dump_json(indent=4) for file in files]))\n        context.record_initial(\"files\", Artifact(task='initial',files=files).model_dump_json())\n\n        for context.round_num in range(1, rounds + 1):\n            logger.info(f\"System is starting workflow round {context.round_num} for goal: '{plan.high_level_goal}'\")\n\n            team_by_name = {agent.name: agent for agent in self.orchestrator.team.values()} if self.orchestrator.team else {}\n\n            context.reset()\n\n            while context.step_index < len(plan.tasks):\n                step = context.current_step()\n                agent_name = step.agent\n\n                delegate_agent = team_by_name.get(agent_name)\n                if not delegate_agent:\n                    delegate_agent = next((agent for agent in self.agents if agent.name.lower() == agent_name.lower()), None)\n\n                if not delegate_agent:\n                    logger.warning(f\"No agent found with name '{agent_name}'. Execution will continue with the Orchestrator as the fallback agent.\")\n                    delegate_agent = self.orchestrator\n\n                logger.info(f\"Agent '{delegate_agent.name}' is executing step {context.step_index + 1}/{len(plan.tasks)}: '{step.description}' (Role: {step.role})\")\n\n                result = delegate_agent.execute_task(context)\n                if result:\n                    context.record_artifact(f\"{delegate_agent.name}_result.txt\", result, True)\n                    try:\n                        agent_output = AgentOutput.model_validate_json(result)\n                        if agent_output.team and agent_output.team.prompts:\n                            logger.info(f\"Agent {delegate_agent.name} provided new prompts.\")\n                            for prompt_data in agent_output.team.prompts:\n                                self._update_agent_prompt(self.session, prompt_data.agent, prompt_data.system_prompt)\n                    except (json.JSONDecodeError, Exception) as e:\n                        logger.warning(f\"Could not parse agent output as AgentOutput: {e}. Treating as plain text.\")\n                context.next()\n            logger.info(f\"System has completed workflow round {context.round_num}.\")\n        logger.info(\"--- Workflow Complete ---\")\n\n    def _update_agent_prompt(self, session: Session, agent_name: str, new_prompt: str) -> None:\n        \"\"\"\n        Updates an agent's system prompt.\n\n        Args:\n            session (Session): The current session object.\n            agent_name (str): The name or role of the agent to update.\n            new_prompt (str): The new system prompt.\n        \"\"\"\n        if not (agent_name and new_prompt):\n            return\n\n        if not (self.orchestrator and self.orchestrator.team):\n            logger.warning(\"Orchestrator or its team is not initialized. Cannot update agent prompt.\")\n            return\n\n        # Combine team agents and session agents for a comprehensive search, ensuring uniqueness\n        all_agents = {agent.name: agent for agent in list(self.orchestrator.team.values()) + session.agents}.values()\n\n        # First, try to find by name\n        target_agent = next((agent for agent in all_agents if agent.name.lower() == agent_name.lower()), None)\n\n        # If not found by name, try to find by role\n        if not target_agent:\n            target_agent = next((agent for agent in all_agents if agent.role.lower() == agent_name.lower()), None)\n\n        if target_agent:\n            target_agent.update_system_prompt(new_prompt)\n            logger.info(f\"Agent '{target_agent.name}' (matched by '{agent_name}') system prompt updated.\")\n        else:\n            logger.warning(f\"Target agent '{agent_name}' not found for prompt update.\")\n\n    def _instantiate_agent(self, agent_spec: Dict[str, Any], prompts: Dict[str, str], all_agent_specs: List[Dict[str, Any]]) -> Optional[Agent]:\n        \"\"\"\n        Instantiates an Agent (or a subclass) based on the provided specification.\n\n        Args:\n            agent_spec (Dict[str, Any]): A dictionary containing the agent's specifications.\n            prompts (Dict[str, str]): A dictionary of available system prompts.\n            all_agent_specs (List[Dict[str, Any]]): A list of all agent specifications for team resolution.\n\n        Returns:\n            Optional[Agent]: An instantiated Agent object, or None if the prompt is not found.\n        \"\"\"\n        prompt_key = f\"{agent_spec['name'].lower()}_instructions.txt\"\n        if prompt_key not in prompts:\n            logger.warning(\n                f\"Prompt for agent '{agent_spec['name']}' not found with key '{prompt_key}'. Empty system prompt will be used.\"\n            )\n\n        agent = Orchestrator(\n            name=agent_spec.get(\"name\", \"Unnamed Agent\"),\n            role=agent_spec.get(\"role\", \"Agent\"),\n            goal=agent_spec.get(\"goal\", \"\"),\n            model=agent_spec.get(\"model\", \"default-model\"),\n            system_prompt=\"\" if prompt_key not in prompts else prompts[prompt_key],\n        )\n\n        if agent_spec.get(\"delegation\") and \"team\" in agent_spec:\n            agent.team = {}\n            for team_member_name in agent_spec[\"team\"]:\n                member_spec = next(\n                    (\n                        s\n                        for s in all_agent_specs\n                        if s[\"name\"].lower() == team_member_name.lower()\n                    ),\n                    None,\n                )\n                if member_spec:\n                    member_agent = self._instantiate_agent(\n                        member_spec, prompts, all_agent_specs\n                    )  # Recursive call\n                else:\n                    logger.warning(\n                        f\"Team member '{team_member_name}' not found in agent specs.\"\n                    )\n                    member_agent = self._instantiate_agent(\n                        agent_spec={\n                            \"name\": team_member_name,\n                            \"role\": \"Unknown\",\n                            \"goal\": \"\",\n                            \"model\": agent_spec.get(\"model\", \"default-model\"),\n                            \"system_prompt\": \"\",\n                        },\n                        prompts=prompts,\n                        all_agent_specs=all_agent_specs,\n                    )  # Recursive call\n                if member_agent:\n                    agent.team[team_member_name] = member_agent\n        return agent\n\n    def _load_config(self, config_path: str) -> Any:\n        \"\"\"\n        Loads the runtime configuration from a YAML file.\n\n        Args:\n            config_path (str): The absolute path to the configuration file.\n\n        Returns:\n            dict: The loaded configuration as a dictionary.\n        \"\"\"\n        logger.info(f\"Loading configuration from: {config_path}\")\n        with open(config_path, 'r', encoding='utf-8') as f:\n            return yaml.safe_load(f)\n\n    def _load_agent_templates(self, agents_dir: str, config: dict, default_model: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Loads all agent specifications from YAML files within a specified directory.\n\n        Args:\n            agents_dir (str): The absolute path to the directory containing agent YAML files.\n\n        Returns:\n            list: A list of dictionaries, where each dictionary represents an agent's specification.\n        \"\"\"\n        logger.info(f\"Loading agent templates from: {agents_dir}\")\n        agent_files = glob(os.path.join(agents_dir, '*.yaml'))\n        templates = []\n        for agent_file in agent_files:\n            with open(agent_file, 'r', encoding='utf-8') as f:\n                template = yaml.safe_load(f)\n                # Resolve prompt path relative to the agent file's directory\n                if 'system_prompt' in template and template['system_prompt']:\n                    base_dir = os.path.dirname(agent_file)\n                    prompt_path = os.path.abspath(os.path.join(base_dir, template['system_prompt']))\n                    template['system_prompt_path'] = prompt_path\n                template[\"model\"] = default_model\n                templates.append(template)\n        logger.info(f\"{len(templates)} agent templates loaded.\")\n        return templates\n\n    def _load_prompts(self, prompts_dir: str) -> Dict[str, str]:\n        \"\"\"\n        Loads all system prompts from text files within a specified directory.\n\n        Args:\n            prompts_dir (str): The absolute path to the directory containing prompt text files.\n\n        Returns:\n            dict: A dictionary where keys are prompt filenames (e.g., 'agent_instructions.txt')\n                and values are the content of the prompt files.\n        \"\"\"\n        logger.info(f\"Loading prompts from: {prompts_dir}\")\n        prompt_files = glob(os.path.join(prompts_dir, '*.txt'))\n        prompts = {}\n        for prompt_file in prompt_files:\n            with open(prompt_file, 'r', encoding='utf-8') as f:\n                # Use the absolute path as the key for reliable lookup\n                prompts[os.path.basename(prompt_file)] = f.read()\n        logger.info(f\"{len(prompts)} prompts loaded.\")\n        return prompts\n"},{"path":"runtime/util.py","content":"\"\"\"This module provides miscellaneous utility functions for the runtime.\n\nIt includes helper functions that are used across different modules\nbut do not belong to a specific component.\n\"\"\"\n\nimport os\n\ndef read_file(file_path: str) -> str:\n    \"\"\"Reads content from a given file path.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except FileNotFoundError:\n        return f\"Error: File not found at {file_path}\"\n    except Exception as e:\n        return f\"Error reading file {file_path}: {e}\"\n"}]}

--- Artifact '__1/__step_5_Lyra_result.txt' from (Lyra) in [T6]:
{"output":"Implemented the core components of the TypeScript Node.js runtime: Agent, Session, and ExecutionContext. Defined interfaces and classes for these components, adhering to SOTA TypeScript and Node.js practices. Established basic project structure and file organization.","artifact":{"task":"T6","files":[{"path":"src/agent/Agent.ts","content":"import { LLM, LLMGenerateContentArgs } from '../llm/LLM';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, Artifact, AgentOutput, File, Team, Role } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\n// Interface for Agent configuration loaded from YAML\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string; // Path to the system prompt file relative to agent definition\n  team?: PromptSpec[]; // For delegation, specifies members of its team\n}\n\n// Interface for team member specification (used in AgentSpec.team)\nexport interface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  protected systemPrompt: string;\n  protected llm: LLM;\n  protected model: string;\n  // Team is primarily for Orchestrator, but can be used by other agents for delegation\n  public team: Record<string, Agent> = {}; \n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    this.llm = LLM.factory(model); // Use the factory to get an LLM instance\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Model: ${this.model})`);\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task assigned to this agent within the given context.\n   * @param context The execution context containing plan, session, and previous artifacts.\n   * @returns A Promise resolving to an AgentOutput object or null if execution fails.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    const currentTask = context.currentStep();\n    const previousArtifactsContent = await this._getPreviousArtifacts(context);\n\n    // Construct the prompt for the LLM\n    const taskPrompt = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${currentTask.description}'`,\n      // Include team roles for context\n      `The team's roles are:\n${JSON.stringify(context.plan.roles.map((role: Role) => ({ title: role.title, purpose: role.purpose })), null, 2)}`,\n    ].join('\\n\\n');\n\n    let fullPrompt = taskPrompt;\n    if (previousArtifactsContent) {\n      fullPrompt += `\\n\\nPlease use the following outputs from other agents as your input:\\n\\n${previousArtifactsContent}\\n\\n`;\n    }\n    // Add instruction for structured output\n    fullPrompt += `Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project. Respond in a JSON format that includes 'output', and optionally 'artifact' and 'team' fields.`\n\n    logger.debug(`Constructed prompt for ${this.name} for task ${currentTask.id}:\n${fullPrompt}`);\n\n    // Record the prompt used for this task for debugging and auditing\n    context.recordArtifact(`${this.name}_prompt_${currentTask.id}.txt`, fullPrompt);\n\n    try {\n      // Define arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: fullPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: AgentOutput, // Expecting AgentOutput schema\n        responseMimeType: 'application/json',\n        temperature: 0.7, // Default temperature, can be overridden\n      };\n\n      // Call the LLM to generate content\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error(`LLM returned no response for task ${currentTask.id} by ${this.name}.`);\n        return null;\n      }\n\n      // Ensure the response is parsed as AgentOutput\n      if (typeof response === 'string') {\n          try {\n              // Attempt to parse the string response as JSON and validate against AgentOutput schema\n              const parsedOutput = AgentOutput.parse(JSON.parse(response));\n              logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n              this._processAndRecordArtifacts(context, parsedOutput);\n              return parsedOutput;\n          } catch (parseError) {\n              logger.error(`Failed to parse LLM response as AgentOutput for task ${currentTask.id} by ${this.name}:`, parseError);\n              logger.error(`Raw LLM response: ${response}`);\n              // Record the raw, unparseable response as an artifact\n              context.recordArtifact(`${this.name}_raw_output_${currentTask.id}.txt`, response);\n              // Return an AgentOutput indicating the error\n              return new AgentOutput({ output: `Error processing response: LLM returned invalid JSON or schema.`, reasoning: 'LLM response was not valid JSON or AgentOutput schema.' });\n          }\n      } else if (response instanceof AgentOutput) {\n          // If the response is already an AgentOutput instance, use it directly\n          logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n          this._processAndRecordArtifacts(context, response);\n          return response;\n      }\n\n      // Handle unexpected response types from the LLM\n      logger.warn(`Unexpected response type from LLM for task ${currentTask.id} by ${this.name}: ${typeof response}`);\n      return new AgentOutput({ output: `Unexpected response type received from LLM.`, reasoning: 'LLM returned an unexpected data type.' });\n\n    } catch (error) {\n      logger.error(`Error during LLM interaction for task ${currentTask.id} by ${this.name}:`, error);\n      // Record the error as an artifact for debugging\n      context.recordArtifact(`${this.name}_error_${currentTask.id}.txt`, `Error executing task: ${error.message}`);\n      // Return an AgentOutput indicating the failure\n      return new AgentOutput({ output: `Failed to execute task due to an error.`, reasoning: 'LLM interaction failed.' });\n    }\n  }\n\n  /**\n   * Retrieves and formats artifacts required by the current task from the execution context.\n   * @param context The execution context.\n   * @returns A formatted string of required artifacts, or an empty string if none are found.\n   */\n  private async _getPreviousArtifacts(context: ExecutionContext): Promise<string> {\n    // Get the IDs of tasks whose outputs are required for the current step\n    const requiredTaskIds = ['initial', ...context.currentStep().requires];\n    const relevantArtifacts: string[] = [];\n\n    // Iterate through all items remembered in the context\n    for (const [key, contextItem] of Object.entries(context.items)) {\n      // Check if the artifact's source task ID is in the required list\n      if (requiredTaskIds.includes(contextItem.step.id)) {\n        let artifactContent = '';\n        // Format the artifact content based on its type\n        if (contextItem.content instanceof Artifact) {\n          // If it's an Artifact object, format its files\n          artifactContent = contextItem.content.files.map(file => \n            `--- File: ${file.path}\\n${file.content}`\n          ).join('\\n');\n        } else if (typeof contextItem.content === 'string') {\n          artifactContent = contextItem.content;\n        } else {\n          try {\n            // Attempt to stringify if it's a JSON-serializable object\n            artifactContent = JSON.stringify(contextItem.content, null, 2);\n          } catch (e) {\n            // Fallback to simple string conversion\n            artifactContent = String(contextItem.content);\n          }\n        }\n        // Add the formatted artifact to the list\n        relevantArtifacts.push(`--- Artifact '${key}' from (${contextItem.step.role}) in [${contextItem.step.id}]:\\n${artifactContent}`);\n      }\n    }\n\n    return relevantArtifacts.join('\\n\\n');\n  }\n\n  /**\n   * Processes the AgentOutput, records artifacts and reasoning, and handles potential team updates.\n   * @param context The execution context.\n   * @param agentOutput The output received from the agent.\n   */\n  private _processAndRecordArtifacts(context: ExecutionContext, agentOutput: AgentOutput): void {\n    // Record the main output message\n    context.recordArtifact(`${this.name}_output.txt`, agentOutput.output, true);\n\n    // Process and save any artifacts generated by the agent\n    if (agentOutput.artifact && agentOutput.artifact.files) {\n      agentOutput.artifact.files.forEach(file => {\n        // Save the file to the session directory\n        this.session?.addArtifact(file.path, file.content);\n        console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n        // Record the file artifact in the context's memory for potential future use\n        // We record the entire Artifact object here for simplicity, assuming it contains relevant context\n        context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); \n      });\n    }\n\n    // Record the agent's reasoning if provided\n    if (agentOutput.reasoning) {\n      context.recordArtifact(`${this.name}_reasoning.txt`, agentOutput.reasoning, true);\n    }\n\n    // Handle team updates (e.g., new prompts) - the System class will manage applying these\n    if (agentOutput.team && agentOutput.team.prompts) {\n      logger.info(`Agent ${this.name} provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n      // The System class is responsible for applying these prompt updates to other agents.\n    }\n  }\n}\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Role, AgentOutput } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  // Orchestrator might have a more complex team structure or delegation logic\n  // For now, we'll keep it simple and assume 'team' is managed by the System class\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name}`);\n  }\n\n  /**\n   * Generates a plan for the given high-level goal using the LLM.\n   * @param session The current session object.\n   * @param highLevelGoal The overall objective for the plan.\n   * @param initialFiles Any initial files provided to the system.\n   * @returns A Promise resolving to the generated Plan object, or null if generation fails.\n   */\n  async generatePlan(session: Session, highLevelGoal: string, initialFiles: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator '${this.name}' is generating a plan for: \"${highLevelGoal}\"`);\n\n    if (!this.llm) {\n      logger.error('LLM client not initialized for Orchestrator.');\n      return null;\n    }\n\n    // Get available agents and their details to include in the prompt for context\n    const availableAgents = session.agents.map(agent => ({\n      name: agent.name,\n      role: agent.role,\n      goal: agent.goal,\n    }));\n\n    // Format agent information for the prompt\n    const agentDescription = availableAgents.map(agent => \n      `- Name: '${agent.name}'\\n  Role: ${agent.role}\\n  Goal: \"${agent.goal}\"`\n    ).join('\\n');\n\n    // Load the planning prompt template from a file\n    // The path is relative to the project root, adjust as necessary\n    const promptTemplatePath = join(this.rootDir, 'prompts', 'general_planning.txt');\n    let promptTemplate: string;\n    try {\n      promptTemplate = readFileSync(promptTemplatePath, 'utf-8');\n    } catch (error) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}:`, error);\n      return null;\n    }\n\n    // Format the prompt with dynamic information\n    const planningPrompt = promptTemplate.replace('{{high_level_goal}}', highLevelGoal)\n                                      .replace('{{team_description}}', agentDescription)\n                                      .replace('{{initial_files}}', initialFiles.map(f => `--- File: ${f.path}\\n```\\n${f.content}\\n```).join('\\n\\n'));\n\n    logger.debug(`Constructed planning prompt for Orchestrator:\n${planningPrompt}`);\n    // Save the prompt used for planning as an artifact\n    session.addArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      // Prepare arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: planningPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: Plan, // Expecting a Plan object\n        responseMimeType: 'application/json',\n        temperature: 0.1, // Lower temperature for more deterministic planning\n      };\n      \n      // Call the LLM to generate the plan\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error('LLM returned no response for plan generation.');\n        return null;\n      }\n\n      // Ensure the response is parsed as a Plan object\n      if (typeof response === 'string') {\n        try {\n          const plan = Plan.parse(JSON.parse(response));\n          return plan;\n        } catch (parseError) {\n          logger.error('Failed to parse LLM response as Plan:', parseError);\n          logger.error(`Raw LLM response: ${response}`);\n          session.addArtifact('plan_generation_error.txt', `LLM response was not valid JSON or Plan schema: ${response}`);\n          return null;\n        }\n      } else if (response instanceof Plan) {\n        return response; // Response is already a Plan instance\n      }\n\n      logger.error(`Unexpected response type from LLM during plan generation: ${typeof response}`);\n      return null;\n\n    } catch (error) {\n      logger.error('Error during plan generation:', error);\n      session.addArtifact('plan_generation_error.txt', `Error: ${error.message}`);\n      return null;\n    }\n  }\n\n  /**\n   * Overrides the base Agent's executeTask. Orchestrator's primary role is planning,\n   * but it might be assigned specific tasks by the plan itself (e.g., plan refinement).\n   * @param context The execution context.\n   * @returns AgentOutput or null.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    logger.warn(`Orchestrator '${this.name}' executeTask called directly. Task: '${context.currentStep().description}'. Falling back to default agent behavior.`);\n    // Delegate to the base Agent's executeTask to process the prompt as a standard task.\n    // This allows the orchestrator to perform tasks defined within the plan it generated.\n    return super.executeTask(context);\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Artifact, File } from '../custom_types';\nimport { join, dirname as getDirname } from 'path';\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'fs';\n\nconst logger = Logger.getInstance();\n\nexport class Session {\n  public readonly sessionId: string;\n  public readonly agents: Agent[];\n  public state: string;\n  public readonly sessionDir: string;\n  public readonly projectRoot: string;\n\n  constructor(agents: Agent[], projectRoot: string, sessionId?: string) {\n    this.sessionId = sessionId || `session_${Date.now().toString()}`;\n    this.agents = agents;\n    this.state = 'initialized';\n    this.projectRoot = projectRoot;\n    this.sessionDir = join(this.projectRoot, 'sessions', this.sessionId);\n\n    // Ensure the session directory exists\n    if (!existsSync(this.sessionDir)) {\n      mkdirSync(this.sessionDir, { recursive: true });\n    }\n    logger.info(`Session created: ${this.sessionId} (Directory: ${this.sessionDir})`);\n  }\n\n  /**\n   * Saves an artifact to the session directory.\n   * @param name The name of the artifact file.\n   * @param content The content to write. Can be an object, string, etc.\n   */\n  addArtifact(name: string, content: any): void {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      const dir = getDirname(artifactPath);\n      // Ensure the directory for the artifact exists\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n      \n      let contentToWrite: string;\n      // Determine if content should be saved as JSON or plain text\n      if (typeof content === 'object' && content !== null && !(content instanceof File) && !(content instanceof Artifact)) {\n        // If it's a plain object (like from AgentOutput.toJSON()), save as JSON\n        contentToWrite = JSON.stringify(content, null, 2);\n      } else if (typeof content === 'string') {\n        contentToWrite = content;\n      } else {\n        // Fallback for other types, convert to string\n        contentToWrite = String(content);\n      }\n\n      writeFileSync(artifactPath, contentToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (e) {\n      logger.error(`Error saving artifact '${name}' in session ${this.sessionId}:`, e);\n    }\n  }\n\n  /**\n   * Retrieves an artifact from the session directory.\n   * @param name The name of the artifact file to load.\n   * @returns The content of the artifact, or null if not found or an error occurs.\n   */\n  getArtifact(name: string): any {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      if (existsSync(artifactPath)) {\n        const content = readFileSync(artifactPath, 'utf-8');\n        // Attempt to parse as JSON if the filename suggests it\n        if (name.endsWith('.json')) {\n          try {\n            return JSON.parse(content);\n          } catch (e) {\n            logger.error(`Error parsing JSON artifact '${name}':`, e);\n            return null; // Return null if JSON parsing fails\n          }\n        }\n        return content; // Return as plain text\n      } else {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n        return null;\n      }\n    } catch (e) {\n      logger.error(`Error retrieving artifact '${name}' from session ${this.sessionId}:`, e);\n      return null;\n    }\n  }\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface ContextItem {\n  name: string;\n  content: any;\n  step: Task;\n}\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores artifacts remembered from previous steps/rounds\n\n  constructor(session: Session, plan: Plan) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = 0;\n    this.roundNum = 1;\n    this.items = {};\n    logger.info('ExecutionContext initialized.');\n  }\n\n  /** Advances to the next step in the plan. */\n  nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  /** Resets the step index to the beginning of the plan. */\n  resetStepIndex(): void {\n    this.stepIndex = 0;\n    logger.debug('Step index reset to 0.');\n  }\n\n  /**\n   * Returns the current task based on the step index.\n   * @returns The current Task object.\n   * @throws Error if the step index is out of bounds.\n   */\n  currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error('Step index out of bounds. No more tasks in the plan.');\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  /**\n   * Records an artifact generated by a task and optionally remembers it in the context's memory.\n   * @param key The key identifier for the artifact.\n   * @param value The artifact content.\n   * @param remember If true, the artifact is stored in context.items for future steps.\n   */\n  recordArtifact(key: string, value: any, remember: boolean = false): void {\n    // Construct a unique artifact key including round and step number\n    const artifactKey = `__${this.roundNum}/__step_${this.stepIndex}_${key}`;\n    this.session.addArtifact(artifactKey, value);\n    // If 'remember' is true, store the artifact in the context's memory\n    if (remember) {\n      this._rememberItem(artifactKey, value, this.currentStep());\n    }\n  }\n\n  /**\n   * Records initial artifacts provided at the beginning of the workflow.\n   * @param key The key identifier for the initial artifact.\n   * @param value The artifact content.\n   */\n  recordInitial(key: string, value: any): void {\n    // Use a special task ID 'initial' for items provided at the start\n    const initialTask: Task = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: []\n    });\n    this._rememberItem(key, value, initialTask);\n    // If the initial item is an Artifact containing files, add them to the session as well\n    if (value instanceof Artifact && value.files) {\n        value.files.forEach(file => this.session.addArtifact(file.path, file.content));\n    }\n    logger.debug(`Recorded initial item '${key}'.`);\n  }\n\n  /**\n   * Helper method to store an item in the context's memory (items dictionary).\n   * @param name The name/key for the item.\n   * @param content The content of the item.\n   * @param step The Task associated with this item.\n   */\n  private _rememberItem(name: string, content: any, step: Task): void {\n    this.items[name] = { name, content, step };\n    logger.debug(`Item '${name}' remembered in context.`);\n  }\n}\n"},{"path":"src/llm/LLM.ts","content":"import { BaseModel, model_validator } from 'pydantic';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface for the LLM generation arguments, providing a structured way to pass parameters\nexport interface LLMGenerateContentArgs {\n  modelName: string;\n  contents: string;\n  systemInstruction?: string;\n  temperature?: number;\n  responseMimeType?: 'text/plain' | 'application/json' | 'application/json-schema';\n  responseSchema?: typeof BaseModel; // Expecting a Pydantic BaseModel class\n}\n\nexport abstract class LLM {\n  /**\n   * Abstract method to be implemented by concrete LLM providers.\n   * This method performs the actual call to the LLM API.\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the LLM's response (can be string, object, or parsed schema instance).\n   */\n  protected abstract generateContentImpl(args: LLMGenerateContentArgs): Promise<any>;\n\n  /**\n   * Public method to generate content using an LLM. Handles common logic like:\n   * - Retries with exponential backoff\n   * - Response parsing based on mime type and schema\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the processed LLM response, or null if all retries fail.\n   */\n  public async generateContent(args: LLMGenerateContentArgs): Promise<any> {\n    const { modelName, responseSchema, responseMimeType } = args;\n    const maxRetries = 3; // Number of times to retry the LLM call\n    let lastError: Error | null = null;\n\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        logger.info(`Calling LLM model '${modelName}' (Attempt ${attempt}/${maxRetries})...`);\n        const response = await this.generateContentImpl(args);\n\n        if (!response) {\n          throw new Error('LLM returned an empty response.');\n        }\n\n        // Process the response based on the requested schema and mime type\n        if (responseSchema && responseMimeType === 'application/json') {\n          if (typeof response === 'string') {\n            try {\n              const parsedJson = JSON.parse(response);\n              // Validate the parsed JSON against the provided Pydantic schema\n              const validatedData = responseSchema.parse(parsedJson);\n              logger.debug(`LLM response successfully parsed and validated against schema ${responseSchema.name}.`);\n              return validatedData;\n            } catch (e) {\n              // Catch JSON parsing or Pydantic validation errors\n              lastError = new Error(`Failed to parse or validate LLM JSON response with schema ${responseSchema.name}: ${e.message}`);\n              logger.error(lastError.message);\n              // Do not retry on parsing/validation errors, as the response itself is likely malformed\n              throw lastError;\n            }\n          } else if (response instanceof responseSchema) {\n            // If the response is already an instance of the expected schema, return it directly\n            logger.debug(`LLM response is already an instance of ${responseSchema.name}.`);\n            return response;\n          } else {\n             // Handle cases where the response is neither a string nor the expected schema type\n             lastError = new Error(`LLM response was not a string or expected schema type for JSON request.`);\n             logger.error(lastError.message);\n             throw lastError;\n          }\n        } else if (responseSchema && responseMimeType === 'application/json-schema') {\n            // Specific handling for JSON schema responses if needed\n             if (typeof response === 'string') {\n                try {\n                    const parsedSchema = JSON.parse(response);\n                    // Further validation or processing might be needed here\n                    logger.debug('LLM response parsed as JSON Schema.');\n                    return parsedSchema;\n                } catch (e) {\n                     lastError = new Error(`Failed to parse LLM JSON Schema response: ${e.message}`);\n                     logger.error(lastError.message);\n                     throw lastError;\n                }\n            }\n        }\n\n        // Default handling for text responses or un-schematized JSON\n        logger.debug('LLM response processed.');\n        return response;\n\n      } catch (error) {\n        // Capture the error and prepare for retry or final failure\n        lastError = error;\n        logger.error(`LLM generation failed (Attempt ${attempt}): ${error.message}`);\n        // Implement exponential backoff for retries\n        if (attempt < maxRetries) {\n          const delay = Math.pow(2, attempt) * 1000; // Delay in ms (1s, 2s, 4s)\n          logger.info(`Waiting for ${delay / 1000} seconds before retrying...`);\n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n\n    // If all retries fail, log the last error and return null\n    logger.error(`LLM generation failed after ${maxRetries} attempts. Last error: ${lastError?.message}`);\n    return null;\n  }\n\n  /**\n   * Factory method to create an LLM instance based on a species string.\n   * This allows for dynamic instantiation of different LLM providers.\n   * @param species - A string identifying the LLM species (e.g., 'gemini:gemini-pro', 'ollama:llama3').\n   * @returns An instance of a concrete LLM class.\n   */\n  static factory(species: string): LLM {\n    logger.info(`LLM Factory: Creating LLM instance for species '${species}'`);\n    \n    // TODO: Replace dummy implementations with actual LLM provider classes\n    // Example: Implement OllamaLLM, GeminiLLM, OpenAILLM, MistralLLM classes\n    // and instantiate them here based on the 'species' string.\n\n    if (species.startsWith('ollama:')) {\n        const modelName = species.split(':')[1];\n        // return new OllamaLLM(modelName);\n        logger.warn('OllamaLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('gemini:') || species === 'gemini-2.5-flash-lite') {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new GeminiLLM(modelName);\n        logger.warn('GeminiLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n     if (species.startsWith('openai:') || species.startsWith('gpt:') || species.startsWith('opi:')) {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new OpenAILLM(modelName);\n         logger.warn('OpenAILLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('mistral:')) {\n        const modelName = species.split(':')[1];\n        // return new MistralLLM(modelName);\n         logger.warn('MistralLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n\n    // Fallback to DummyLLM if the species is unknown\n    logger.warn(`Unknown LLM species '${species}'. Using DummyLLM as fallback.`);\n    return new DummyLLM(species);\n  }\n}\n\n/**\n * A dummy LLM implementation used as a placeholder.\n * It simulates LLM responses without making actual API calls.\n */\nclass DummyLLM extends LLM {\n  constructor(private modelName: string) {\n    super();\n    logger.warn(`DummyLLM initialized with model: ${this.modelName}. This is a placeholder.`);\n  }\n\n  protected async generateContentImpl(args: LLMGenerateContentArgs): Promise<any> {\n    logger.warn(`DummyLLM: generateContentImpl called for model ${this.modelName}. Returning mock response.`);\n    \n    // Simulate response based on schema if provided\n    if (args.responseSchema) {\n        try {\n            // Create a mock response adhering to the schema\n            if (args.responseSchema === AgentOutput) {\n                return new AgentOutput({\n                    output: `Mock output for ${this.modelName} task: ${args.contents.substring(0, 50)}...`,\n                    reasoning: 'This is a mock response from DummyLLM.',\n                    artifact: new Artifact({ task: 'mock_task', files: [new File({ path: 'mock_output.txt', content: 'Mock file content.'})]}),\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            } else if (args.responseSchema === Plan) {\n                 return new Plan({\n                    highLevelGoal: 'Mock Goal',\n                    reasoning: 'Mock reasoning for plan generation',\n                    roles: [{title: 'Mock Role', purpose: 'Mock Purpose'}],\n                    tasks: [{id: 'T1', description: 'Mock Task', role: 'Mock Role', agent: 'Mock Agent', requires: []}],\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            }\n            // Add handlers for other schemas as needed\n        } catch (e) {\n            logger.error(`Error creating mock response for schema ${args.responseSchema.name}:`, e);\n            return `Mock response failed for schema ${args.responseSchema.name}.`;\n        }\n    }\n    \n    // Default mock response if no schema is provided or handled\n    return `This is a mock response from DummyLLM for model ${this.modelName}.`;\n  }\n}\n"},{"path":"src/logger/Logger.ts","content":"import winston from 'winston';\nimport { join } from 'path';\nimport fs from 'fs';\n\n// Define log levels with priorities\nconst logLevels = {\n  error: 0,\n  warn: 1,\n  info: 2,\n  debug: 3,\n};\n\n// Define the format for log messages\nconst logFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }), // Include stack trace for errors\n  winston.format.splat(), // Support for string interpolation like printf\n  winston.format.printf(({ level, message, timestamp, ...metadata }) => {\n    // Construct the log message string\n    let msg = `${timestamp} [${level.toUpperCase()}]: ${message}`;\n    // Append any additional metadata (e.g., agent name, task ID)\n    if (Object.keys(metadata).length > 0) {\n      msg += ` ${JSON.stringify(metadata)}`;\n    }\n    return msg;\n  })\n);\n\n// Define a format for JSON line logs (useful for log aggregation)\nconst jsonLogFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.json() // Output logs as JSON objects on separate lines\n);\n\n// Ensure the log directory exists\nconst logDir = process.env.LOG_DIR || 'logs'; // Use LOG_DIR environment variable or default to 'logs'\nif (!fs.existsSync(logDir)) {\n  fs.mkdirSync(logDir, { recursive: true });\n}\n\nconst logFilePath = join(logDir, 'app.log');\nconst jsonLogFilePath = join(logDir, 'app.jsonl');\n\nexport class Logger {\n  private static instance: winston.Logger;\n\n  /**\n   * Gets the singleton Winston logger instance.\n   * Configures transports for console, file (plain text), and file (JSONL).\n   * @returns The configured Winston logger instance.\n   */\n  public static getInstance(): winston.Logger {\n    if (!Logger.instance) {\n      Logger.instance = winston.createLogger({\n        level: process.env.LOG_LEVEL || 'info', // Set log level from environment or default to 'info'\n        levels: logLevels,\n        format: logFormat, // Default format for transports unless overridden\n        transports: [\n          // Console transport: Outputs logs to the console with colorization\n          new winston.transports.Console({\n            format: winston.format.combine(\n              winston.format.colorize({ all: true }), // Apply colors to all levels\n              logFormat\n            ),\n          }),\n          // File transport: Logs to 'app.log' in plain text format\n          new winston.transports.File({\n            filename: logFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: logFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true, // Allows log rotation\n          }),\n          // JSONL File transport: Logs to 'app.jsonl' in JSON format\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: jsonLogFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true,\n          }),\n        ],\n        // Handle uncaught exceptions\n        exceptionHandlers: [\n          new winston.transports.Console({\n             format: winston.format.combine(winston.format.colorize({ all: true }), logFormat)\n          }),\n          new winston.transports.File({\n            filename: logFilePath,\n            format: logFormat,\n          }),\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            format: jsonLogFormat,\n          }),\n        ]\n      });\n\n      // Optional: Add a listener for logger errors\n      Logger.instance.on('error', (err) => {\n        console.error('Winston logger encountered an error:', err);\n      });\n    }\n    return Logger.instance;\n  }\n}\n"},{"path":"src/config/Config.ts","content":"import { readFileSync } from 'fs';\nimport { join } from 'path';\nimport yaml from 'js-yaml';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface defining the structure of the configuration object\nexport interface Config {\n  projectRoot?: string;       // Root directory of the project\n  agentDir?: string;          // Directory containing agent configurations\n  promptDir?: string;         // Directory containing prompt templates\n  logDir?: string;            // Directory for log files\n  defaultModel?: string;      // Default LLM model to use if not specified\n  workflowRounds?: number;    // Default number of workflow rounds\n  // Add other configuration properties as needed\n}\n\nexport class Config {\n  /**\n   * Loads configuration from a YAML file.\n   * @param fileName The name of the configuration file (e.g., 'runtime.yaml').\n   * @param basePath The base path to search for the config file (defaults to current working directory).\n   * @returns A Promise resolving to the loaded Config object with defaults applied.\n   */\n  public static async load(fileName: string, basePath: string = process.cwd()): Promise<Config> {\n    const configPath = join(basePath, fileName);\n    logger.info(`Attempting to load configuration from: ${configPath}`);\n    try {\n      // Read the file content\n      const fileContents = readFileSync(configPath, 'utf-8');\n      // Parse the YAML content\n      const config = yaml.load(fileContents) as Config;\n\n      // Apply default values if properties are missing\n      config.projectRoot = config.projectRoot || basePath;\n      config.agentDir = config.agentDir || 'agents';\n      config.promptDir = config.promptDir || 'prompts';\n      config.logDir = config.logDir || 'logs';\n      config.defaultModel = config.defaultModel || 'gemini-2.5-flash-lite';\n      config.workflowRounds = config.workflowRounds || 3;\n\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Failed to load configuration from ${configPath}:`, error);\n      // In case of failure, return a default configuration to allow the system to start\n      // Alternatively, you might want to throw an error to halt execution\n      return {\n        projectRoot: basePath,\n        agentDir: 'agents',\n        promptDir: 'prompts',\n        logDir: 'logs',\n        defaultModel: 'gemini-2.5-flash-lite',\n        workflowRounds: 3,\n      };\n    }\n  }\n}\n"},{"path":"src/custom_types.ts","content":"import { BaseModel, Field, model_validator } from 'pydantic';\nimport { z } from 'zod'; // Import Zod for potential schema definition or validation demonstration\n\n// --- Core Types representing Task Agnostic Steps (TAS) ---\n\n/** Represents a single actionable step within a plan. */\nexport class Task extends BaseModel {\n  id: string = Field(..., { description: \"Unique ID for the task (e.g., 'T1', 'T2').\" });\n  description: string = Field(..., { description: \"Detailed description of what to do for this task.\" });\n  role: string = Field(..., { description: \"The role responsible for performing this task.\" });\n  agent: string = Field(..., { description: \"The specific agent assigned to execute this task.\" });\n  requires: string[] = Field([], { description: \"List of task IDs whose outputs are required as prerequisites (e.g., ['T1', 'T2']).\" });\n}\n\n/** Represents a file generated or used within the system. */\nexport class File extends BaseModel {\n  path: string = Field(..., { description: \"File path or name (e.g., 'src/main.py').\" });\n  content: string = Field(..., { description: \"Full content of the file.\" });\n}\n\n/** Represents the output artifact of a completed task. */\nexport class Artifact extends BaseModel {\n  task: string = Field(..., { description: \"ID of the task that produced this artifact (e.g., 'T1').\" });\n  files: File[] = Field([], { description: \"List of files created or modified by the task.\" });\n}\n\n/** Defines a role within the system or team. */\nexport class Role extends BaseModel {\n  title: string = Field(..., { description: \"Title of the role (e.g., 'Engineer', 'Reviewer').\" });\n  purpose: string = Field(..., { description: \"The main purpose and high-level responsibilities of this role.\" });\n}\n\n/** Represents a system prompt configuration for an agent. */\nexport class Prompt extends BaseModel {\n  agent: string = Field(..., { description: \"Name of the agent this prompt is for. Must match exactly an existing agent.\" });\n  role: string = Field(..., { description: \"Role context of the agent.\" });\n  systemPrompt: string = Field(..., { description: \"Full text of the system prompt for this agent.\" });\n}\n\n/** Contains metadata and coordination information for the team, including prompt updates. */\nexport class Team extends BaseModel {\n  notes: string = Field('', { description: \"General notes or feedback about the team's performance or the plan.\" });\n  prompts: Prompt[] = Field([], { description: \"A list of new or updated system prompts for agents in the team.\" });\n}\n\n/** Defines the overall plan for a workflow, including goal, roles, and tasks. */\nexport class Plan extends BaseModel {\n  highLevelGoal: string = Field(..., { description: \"The main goal this plan is designed to achieve.\" });\n  reasoning: string = Field(..., { description: \"A brief explanation of the plan's structure and strategy.\" });\n  roles: Role[] = Field([], { description: \"List of all roles required to execute the plan.\" });\n  tasks: Task[] = Field([], { description: \"A step-by-step sequence of tasks to be executed in order.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n}\n\n/**\n * Represents the structured output returned by an agent after executing a task.\n * Adheres to the JSON output format expected by the runtime.\n */\nexport class AgentOutput extends BaseModel {\n  output: string = Field(..., { description: \"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\" });\n  artifact: Artifact | null = Field(null, { description: \"Optional artifact containing files created or modified by the agent.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n  reasoning: string | null = Field(null, { description: \"Explanation of how the agent arrived at this output.\" });\n\n  /**\n   * Custom validator to ensure that an AgentOutput contains either an 'output' message or an 'artifact'.\n   */\n  @model_validator(mode='after')\n  validateOutputAndArtifact(): this {\n    if (!this.output && !this.artifact) {\n      throw new Error('Agent output must contain either an \"output\" message or an \"artifact\".');\n    }\n    return this;\n  }\n}\n\n// --- Example Usage & Schema Generation ---\n// This block executes only when the script is run directly\nif (require.main === module) {\n  console.log('--- AgentOutput Schema ---\\n', JSON.stringify(AgentOutput.model_json_schema(), null, 2));\n  console.log('\\n--- Plan Schema ---\\n', JSON.stringify(Plan.model_json_schema(), null, 2));\n}\n"}]},"team":{"notes":"Core components (Agent, Session, ExecutionContext) have been implemented in TypeScript. The structure follows the defined architecture and leverages SOTA practices like classes, interfaces, and async/await. The LLM integration is abstracted but currently uses a dummy implementation. Further steps will involve implementing concrete LLM providers and the Orchestrator logic.","prompts":[]},"reasoning":"The task was to begin implementing the core components of the TypeScript runtime: Agent, Session, and ExecutionContext. I have created the necessary TypeScript files (`Agent.ts`, `Session.ts`, `ExecutionContext.ts`) within the `src/agent`, `src/core` directories respectively. These files define classes and interfaces that mirror the functionality described in the Python runtime and the architecture defined in T3. Key aspects like agent initialization, task execution, session management, artifact handling, and context management have been addressed using modern TypeScript features and Node.js APIs (like `fs` and `path`). The LLM interaction is abstracted behind an `LLM` class, with a `DummyLLM` serving as a placeholder until concrete implementations are ready. Error handling and logging have been incorporated using `winston` and `try-catch` blocks. The `custom_types.ts` file is updated to reflect these components. The `System.ts` and `Orchestrator.ts` files have been updated to reflect the new TypeScript structure and imports. The `package.json` and `tsconfig.json` from T4 provide the foundation for building and running these components."}



Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.