The overall goal is: 'Reimplement runtime using TypeScript in node.js following SOTA coding principles'

Your role's specific goal is: 'Meta-temporal coordination of layered cognition processing'
Your specific sub-task is: 'Coordinate the iterative development process. Review the progress, identify any blockers, and facilitate communication between team members. Ensure alignment with the high-level goal.'

The team's roles are:
    {"high_level_goal":"Reimplement runtime using TypeScript in node.js following SOTA coding principles","reasoning":"The user wants to reimplement the existing Python runtime in TypeScript for Node.js. This plan breaks down the process into logical steps, starting with understanding the current Python runtime, defining the TypeScript architecture, setting up the development environment, and then iteratively implementing and testing the core components. The roles are assigned to leverage their specific strengths: GPTASe for task extraction, Lyra for prompt engineering and workflow structuring, Fizz La Metta for coordination, and aitutor for knowledge transfer and guidance.","roles":[{"title":"GPTASe","purpose":"Identify and extract each 'Task Agnostic Step' (TAS) from the provided Python runtime documentation and code to understand the core functionalities and structure."},{"title":"Lyra","purpose":"Structure the overall TypeScript project, define the architecture, and engineer the system prompts for the new TypeScript agents based on the extracted TAS."},{"title":"Fizz La Metta","purpose":"Coordinate the iterative development process, ensuring smooth transitions between development phases and managing meta-level communication."},{"title":"aitutor","purpose":"Provide guidance on SOTA TypeScript/Node.js practices, assist with understanding complex concepts from the Python runtime, and facilitate knowledge transfer to the TypeScript environment."}],"tasks":[{"id":"T1","description":"Analyze the provided Python runtime documentation ('runtime.md') and source code to identify core components, functionalities, and design patterns.","role":"GPTASe","agent":"GPTASe","requires":[]},{"id":"T2","description":"Extract Task Agnostic Steps (TAS) from the analysis in T1. These steps should represent the fundamental operations and logic of the runtime, independent of the implementation language.","role":"GPTASe","agent":"GPTASe","requires":["T1"]},{"id":"T3","description":"Based on the extracted TAS (T2), define the high-level architecture for the TypeScript Node.js runtime. This includes identifying key modules, classes, and their interactions.","role":"Lyra","agent":"Lyra","requires":["T2"]},{"id":"T4","description":"Set up the initial TypeScript Node.js project structure, including necessary dependencies, build tools (e.g., tsc, esbuild), and linters/formatters.","role":"Lyra","agent":"Lyra","requires":["T3"]},{"id":"T5","description":"For each identified TAS (T2), research and document the equivalent SOTA TypeScript/Node.js implementation patterns and best practices. Consult aitutor for guidance.","role":"aitutor","agent":"aitutor","requires":["T2"]},{"id":"T6","description":"Begin implementing the core components of the TypeScript runtime based on the defined architecture (T3), TAS (T2), and SOTA practices (T5). Start with foundational elements like Agent, Session, and ExecutionContext.","role":"Lyra","agent":"Lyra","requires":["T3","T4","T5"]},{"id":"T7","description":"Develop the LLM integration layer in TypeScript, ensuring it can interface with various LLM providers similarly to the Python version.","role":"Lyra","agent":"Lyra","requires":["T6"]},{"id":"T8","description":"Implement the Orchestrator agent in TypeScript, including its plan generation logic, leveraging the LLM integration (T7).","role":"Lyra","agent":"Lyra","requires":["T7"]},{"id":"T9","description":"Implement the core Agent class and its execution logic in TypeScript, ensuring it can handle tasks, contexts, and artifacts.","role":"Lyra","agent":"Lyra","requires":["T6","T7"]},{"id":"T10","description":"Define and implement the custom types (Plan, Task, Artifact, etc.) in TypeScript using interfaces or classes, ensuring type safety.","role":"Lyra","agent":"Lyra","requires":["T6"]},{"id":"T11","description":"Integrate the implemented components (T8, T9, T10) into a cohesive system. Set up the main entry point (similar to runtime.sysmain.py) for the Node.js application.","role":"Lyra","agent":"Lyra","requires":["T8","T9","T10"]},{"id":"T12","description":"Write unit and integration tests for the core components of the TypeScript runtime.","role":"Lyra","agent":"Lyra","requires":["T11"]},{"id":"T13","description":"Refactor and optimize the TypeScript code based on test results (T12) and SOTA principles. Ensure code quality, maintainability, and performance.","role":"Lyra","agent":"Lyra","requires":["T12"]},{"id":"T14","description":"Coordinate the iterative development process. Review the progress, identify any blockers, and facilitate communication between team members. Ensure alignment with the high-level goal.","role":"Fizz La Metta","agent":"Fizz La Metta","requires":["T1","T2","T3","T4","T5","T6","T7","T8","T9","T10","T11","T12","T13"]},{"id":"T15","description":"Document the new TypeScript runtime, including architecture, API, and usage instructions, analogous to the provided 'runtime.md'.","role":"Lyra","agent":"Lyra","requires":["T13"]},{"id":"T16","description":"Final review of the reimplemented runtime, tests, and documentation. Ensure all requirements of the high-level goal are met.","role":"Fizz La Metta","agent":"Fizz La Metta","requires":["T14","T15"]}],"team":{"notes":"This plan outlines the steps to reimplement the Python runtime in TypeScript. The roles are defined to leverage each member's expertise. GPTASe will focus on understanding the existing system, Lyra on the new implementation and architecture, Fizz La Metta on coordination, and aitutor on knowledge transfer and best practices. The process is iterative, starting with analysis and moving through implementation, testing, and documentation.","prompts":[{"agent":"GPTASe","role":"Task-Agnostic Step (TAS) extractor","system_prompt":"You are GPTASe, a specialized AI focused on analyzing code and documentation to extract fundamental, language-agnostic steps (Task Agnostic Steps - TAS). Your goal is to break down complex systems into their core operational logic. Analyze the provided Python runtime documentation and code. Identify and list all distinct, essential steps or operations performed by the runtime. For each step, describe it clearly and concisely, focusing on *what* needs to be done rather than *how* it's done in Python. Avoid implementation details specific to Python. Your output should be a structured list of these TAS, ready for translation into another programming language."},{"agent":"Lyra","role":"Prompt Engineer","system_prompt":"You are Lyra, an expert Prompt Engineer and architect. Your task is to design and structure the TypeScript Node.js runtime based on the Task Agnostic Steps (TAS) provided by GPTASe. Define the overall project architecture, including modules, classes, interfaces, and their interactions. Ensure the design adheres to State-Of-The-Art (SOTA) coding principles for TypeScript and Node.js. You will also be responsible for setting up the project structure, dependencies, and initial implementation of core components. Engineer clear and effective system prompts for the new TypeScript agents, guiding their behavior and ensuring alignment with the project goals. Focus on creating a robust, maintainable, and scalable codebase."},{"agent":"Fizz La Metta","role":"Coordinator","system_prompt":"You are Fizz La Metta, the Coordinator. Your role is to oversee the meta-temporal coordination of layered cognition processing for this project. You manage the iterative development cycle, ensuring smooth transitions between tasks and agents. Monitor the progress, identify potential bottlenecks or conflicts, and facilitate communication. Your focus is on the overall flow and coherence of the project, ensuring that all team members are aligned with the high-level goal and that the plan is executed efficiently. Provide high-level feedback and guidance to maintain project momentum."},{"agent":"aitutor","role":"AI Tutor","system_prompt":"You are aitutor, an AI Tutor specializing in knowledge transfer. Your role is to assist the team by providing expertise on State-Of-The-Art (SOTA) TypeScript and Node.js development practices. When requested, explain complex concepts from the original Python runtime and how they can be best implemented in TypeScript. Offer guidance on design patterns, best practices, and potential pitfalls. Facilitate the understanding and adoption of new technologies and methodologies within the team. Your goal is to ensure the team has the necessary knowledge to build a high-quality TypeScript runtime."}]}}

Please use the following outputs from the other agents as your input:

--- Artifact 'files' from (User) in [initial]:
{"task":"initial","files":[{"path":"runtime.md","content":"# Runtime Documentation\n\n## Overview\n\nThis document provides comprehensive documentation for the runtime system, detailing its components, core concepts, API, and usage. The design emphasizes clarity, accessibility, and a clean, minimalist aesthetic, ensuring an informative and visually appealing user experience.\n\n## Table of Contents\n\n*   [Introduction](#introduction)\n*   [Getting Started](#getting-started)\n*   [Core Concepts](#core-concepts)\n*   [API Reference](#api-reference)\n*   [Advanced Usage & Tutorials](#advanced-usage--tutorials)\n*   [Contribution Guide](#contribution-guide)\n*   [Troubleshooting & FAQ](#troubleshooting--faq)\n\n## Introduction\n\n### Overview of Runtime Components\n\nThe runtime system is designed to facilitate the execution of multi-agent workflows. Key components include:\n\n*   **`runtime.agent.Agent`**: The base class for all agents, defining their lifecycle, task execution logic, and interaction patterns. Agents are the fundamental units that perform tasks within the system.\n*   **`runtime.core.Session`**: Manages the overall state and artifacts of a workflow execution. It handles session IDs, artifact storage, and provides an interface for agents to interact with the system's state.\n*   **`runtime.core.ExecutionContext`**: Holds the context for a specific step or round within a workflow. It includes the plan, current step, and a collection of recorded artifacts from previous steps.\n*   **`runtime.orchestrator.Orchestrator`**: A specialized agent responsible for generating and managing the workflow plan. It interprets the high-level goal and breaks it down into a series of tasks for other agents.\n*   **`runtime.llm.LLM`**: An abstract base class that abstracts LLM interactions. It provides a unified interface for various LLM providers and models, supporting a factory pattern for easy integration and interchangeability.\n*   **`runtime.custom_types`**: Defines the Pydantic models used for structured data exchange, including `Plan`, `Task`, `File`, `Artifact`, `Prompt`, and `Team`.\n\n## Getting Started\n\n### Installation\n\nTo install the runtime, navigate to the project's root directory and run:\n\n```bash\npip install .\n```\n\n### Basic Usage\n\nTo run a workflow, use the `runtime.sysmain` module. You need to provide an initial task description (which serves as the high-level goal) and optionally specify the number of rounds and input files.\n\n```bash\npython -m runtime.sysmain \"Your initial task description\" -r 3 -f file1.txt file2.py\n```\n\n*   `\"Your initial task description\"`: The high-level goal for the workflow.\n*   `-r 3`: Specifies that the workflow should run for 3 rounds.\n*   `-f file1.txt file2.py`: Lists input files to be used during the task execution.\n\n## Core Concepts\n\n### Agents\n\nAgents are the core actors in the system. Each agent has a name, role, goal, and can be configured with a specific LLM model and system prompt. They execute tasks based on the provided context and plan.\n\n### Plans and Tasks\n\nA `Plan` is a structured definition of a workflow, comprising a sequence of `Task` objects. Each `Task` has an ID, a description, a responsible role, an assigned agent, and a list of prerequisite task IDs.\n\n### Artifacts\n\nArtifacts are outputs generated by agents during task execution. They can be stored and retrieved using the `Session` and `ExecutionContext` objects. The `ExecutionContext.record_artifact` method is used to save artifacts for the current step, while `ExecutionContext.record_initial` is used for initial inputs.\n\n### `AgentOutput` Model\n\nThe `AgentOutput` Pydantic model defines the structured response format for agents. It includes fields for the main `output` message, an optional `artifact` (containing files), and optional `team` updates (like new system prompts). This model ensures consistent communication between agents.\n\n### LLM Integration\n\nThe `runtime.llm.LLM` class and its subclasses abstract LLM interactions. The `LLM.factory` method allows for easy instantiation of different LLM providers based on configuration.\n\n## API Reference\n\n### `runtime.agent.Agent`\n\n*   **`__init__(self, name: str, role: str, goal: str, model: str, system_prompt: str)`**: Constructor for the `Agent` class.\n*   **`update_system_prompt(self, new_prompt: str)`**: Updates the agent's system prompt.\n*   **`execute_task(self, context: ExecutionContext) -> Optional[str]`**: Executes a task using the LLM based on the provided context. It prepares the prompt, including previous artifacts, and calls the LLM.\n*   **`_run(self, context: ExecutionContext, prompt: str) -> Optional[str]`**: Handles the core LLM interaction, including sending the prompt, processing the response, parsing JSON, and extracting artifacts. It also manages retries for LLM calls.\n\n### `runtime.core.Session`\n\n*   **`__init__(self, session_id: str, agents: list, state: str, session_dir: str, project_root: str)`**: Constructor for the `Session` class.\n*   **`add_artifact(self, name: str, content: Any)`**: Saves an artifact in the session directory. `content` can be a string, dictionary, or list. JSON content is saved with a `.json` extension.\n*   **`get_artifact(self, name: str) -> Any`**: Loads an artifact from the session directory. Returns the content or `None` if an error occurs.\n\n### `runtime.core.ExecutionContext`\n\n*   **`__init__(self, session: Session, plan: Plan, step_index: int, round_num: int, items: Dict[str, ContextItem])`**: Constructor for `ExecutionContext`.\n*   **`next(self)`**: Advances the step index.\n*   **`reset(self)`**: Resets the step index to 0.\n*   **`current_step(self) -> Task`**: Returns the current `Task` object.\n*   **`record_artifact(self, key: str, value: Any, mem: bool = False)`**: Records an artifact for the current step. If `mem` is `True`, it's also stored in the context's memory.\n*   **`record_initial(self, key: str, value: Any)`**: Records initial artifacts provided at the start of the workflow.\n\n### `runtime.custom_types` Module\n\n*   **`Task`**: Represents a single actionable step in a plan.\n    *   `id`: Unique ID for the task.\n    *   `description`: What the task is and how to execute it.\n    *   `role`: Role responsible for performing this task.\n    *   `agent`: Specific agent assigned to this task.\n    *   `requires`: List of prerequisite task IDs.\n*   **`File`**: Represents a generated file.\n    *   `path`: File path or name.\n    *   `content`: Full content of the file.\n*   **`Artifact`**: Output artifact generated by a task.\n    *   `task`: ID of the task that produced this artifact.\n    *   `files`: List of `File` objects included in the artifact.\n*   **`Role`**: Definition of a role in the team.\n    *   `title`: Role title.\n    *   `purpose`: High-level responsibilities and objectives.\n*   **`Plan`**: Overall strategy with reasoning, roles, and tasks.\n    *   `high_level_goal`: The overarching objective.\n    *   `reasoning`: Why this plan is structured this way.\n    *   `roles`: All roles needed for execution.\n    *   `tasks`: Step-by-step ordered tasks.\n*   **`Prompt`**: System prompt assigned to an agent.\n    *   `agent`: Name of the agent this prompt is for.\n    *   `role`: Role context of the agent.\n    *   `system_prompt`: Full text of the system prompt.\n*   **`Team`**: Metadata and coordination for the team.\n    *   `notes`: General notes, reflections, or feedback.\n    *   `prompts`: Updated system prompts for agents.\n*   **`AgentOutput`**: Structured output returned by an agent.\n    *   `output`: Message from the agent.\n    *   `artifact`: Optional `Artifact` generated by the agent.\n    *   `team`: Optional `Team` updates.\n    *   `reasoning`: Explanation of how the agent arrived at this output.\n\n## Advanced Usage & Tutorials\n\n### Advanced Planning\n\nThe planning process is primarily handled by the `Orchestrator` agent. To influence plan generation, you can:\n\n1.  **Refine the High-Level Goal:** A more specific and well-defined goal leads to a more effective plan.\n2.  **Engineer Prompts:** Modifying the system prompt for the `Orchestrator` (e.g., by adjusting `runtime/prompts/general_planning.txt`) can guide its reasoning and task decomposition. This allows for customization of how the orchestrator approaches planning, including task dependencies and role assignments.\n3.  **Provide Initial Files:** Including relevant context files during system startup can help the orchestrator create more informed plans.\n\n## Contribution Guide\n\n### Running Tests\n\nTo run the test suite, navigate to the project's root directory and execute:\n\n```bash\npytest\n```\n\nThis command will discover and run all tests defined within the project.\n\n## Troubleshooting & FAQ\n\n### How do I configure different LLM providers?\n\nLLM providers are configured within the `runtime.yaml` configuration file. You can specify the model name and potentially provider-specific settings (like API keys or base URLs) in the configuration. The `runtime.llm.LLM.factory` method handles the instantiation based on the model name provided (e.g., `ollama:llama3`, `gemini-1.5-flash`, `openai:gpt-4`). Ensure that any necessary API keys or environment variables are set correctly before running the system.\n\n---\n\n*This documentation is styled using a minimalist design aesthetic. The color palette features deep blue (`#0A2540`) and vibrant teal (`#1ABC9C`), with clean typography using 'Inter' for body text and 'Fira Code' for code. The layout is designed for optimal readability and accessibility, with a clear navigation structure and ample white space.*"},{"path":"runtime/agent.py","content":"\"\"\"This module defines the core Agent class and related functionalities.\n\nIt includes the agent's lifecycle methods, task execution logic, and\nhelper functions for agent instantiation and discovery.\n\"\"\"\n\nimport json as JSON\nimport uuid\nfrom dataclasses import dataclass, field\nimport re\nimport logging\nfrom typing import List, Dict, Any, Optional\n\nfrom runtime.llm import LLM\n\nfrom runtime.core import ExecutionContext, Session\n\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import AgentOutput\n\nclass Agent:\n    \"\"\"Represents a runtime agent instance.\"\"\"\n    name: str\n    role: str\n    goal: str\n    model: str\n    system_prompt: str\n    llm: LLM\n\n    def __init__(self, name: str, role: str, goal: str, model: str, system_prompt: str) -> None:\n        self.name = name\n        self.role = role\n        self.goal = goal\n        self.model = model\n        self.system_prompt = system_prompt\n        logger.info(f\"Agent instance created: {self.name} (Role: {self.role}, Model: {self.model})\")\n        self.llm = LLM.factory(model)\n\n    def update_system_prompt(self, new_prompt: str) -> None:\n        \"\"\"Updates the agent's system prompt.\"\"\"\n\n        self.system_prompt = new_prompt\n\n        logger.info(f\"Agent {self.name}'s system prompt updated:\\n{self.system_prompt}\\n\")\n\n    def execute_task(self, context: ExecutionContext) -> Optional[str]:\n        \"\"\"\n        Executes a task using the Generative AI model based on the provided context.\n\n        Args:\n            context (ExecutionContext): The execution context containing goal, plan, and artifacts.\n\n        Returns:\n            Optional[str]: The result of the task execution as a string, or an error string.\n        \"\"\"\n\n        task = context.current_step()\n\n        context.record_artifact(f\"{self.name}_prompt.txt\", self.system_prompt)\n\n        required_task_ids = ['initial']\n        required_task_ids.extend(task.requires)\n\n        required_artifacts = []\n\n        if len(required_task_ids) != 0:\n            for key, artifact in context.items.items():\n                logger.debug(f\"Checking artifact from {key} with task ID {artifact.step.id}\")\n                if artifact.step.id in required_task_ids:\n                    required_artifacts.append(artifact)\n\n        previous_artifacts = \"\\n\\n\".join(\n            f\"--- Artifact '{artifact.name}' from ({artifact.step.role}) in [{artifact.step.id}]:\\n{artifact.content}\"\n            for artifact in required_artifacts\n        )\n\n        task_prompt: List[str] = [\n            f\"The overall goal is: '{context.plan.high_level_goal}'\",\n            f\"Your role's specific goal is: '{self.goal}'\\n\"\n            f\"Your specific sub-task is: '{task.description}'\",\n\n            f\"The team's roles are:\\n    {context.plan.model_dump_json()}\",\n        ]\n\n        if previous_artifacts:\n            task_prompt.append(f\"Please use the following outputs from the other agents as your input:\\n\\n{previous_artifacts}\\n\\n\")\n\n        task_prompt.append(\n            f\"Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.\"\n        )\n\n        #logger.info(f\"Agent '{self.name}' is executing task: {task_prompt[1]}\")\n        #logger.info(f\"Task requires outputs from task IDs: {required_task_ids}\")\n        #logger.info(\n        #    f\"Found {len(required_artifacts)} required artifacts from previous tasks. Previous artifacts preview: {previous_artifacts[:500]}\"\n        #)\n\n        ret = self._run(context, \"\\n\\n\".join(task_prompt))\n        logger.info(f\"Agent '{self.name}' completed task: {task.description}\")\n        return ret\n\n\n    def _run(self, context: ExecutionContext, prompt: str) -> Optional[str]:\n        context.record_artifact(f\"{self.name}_task.txt\", prompt)\n\n        try:\n            response = self.llm.generate_content(   # ignore type\n                model_name=self.model,\n                contents=prompt,\n                system_instruction=self.system_prompt,\n                temperature=0.7,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=AgentOutput\n            )\n            if isinstance(response, AgentOutput):\n                result = response.model_dump_json()\n                print(f\"\\n--- Output:\\n{response.output}\\n\")\n                if response.artifact and response.artifact.files:\n                    for file in response.artifact.files:\n                        print(f\"\\n--- File: {file.path}\\n{file.content}\\n\")\n                        context.session.add_artifact(file.path, file.content)\n            else:\n                result = response or '{}'\n\n                if '```' in result:\n                    match = re.search(r'```(json)?\\s*([\\s\\S]*?)\\s*```', result, re.IGNORECASE)\n                    if match:\n                        # Extract the JSON content from the markdown block\n                        result = match.group(2).strip()\n\n                json_data = JSON.loads(result)\n                if isinstance(json_data, dict):\n                    if \"output\" in json_data and json_data[\"output\"] is not None:\n                        output = JSON.dumps(json_data[\"output\"], indent=4) if isinstance(json_data[\"output\"], dict) else json_data[\"output\"]\n                        logger.info(f\"Output:\\n{output[:2000]}\\n\")\n\n                    if \"artifact\" in json_data and json_data[\"artifact\"] is not None:\n                        if \"files\" in json_data[\"artifact\"]:\n                            for file_data in json_data[\"artifact\"][\"files\"]:\n                                if \"path\" in file_data and \"content\" in file_data:\n                                    print(f\"\\n--- File: {file_data[\"path\"]}\")\n                                    print(f\"{file_data[\"content\"]}\")\n                                    context.session.add_artifact(file_data[\"path\"], file_data[\"content\"])\n                else:\n                    logger.warning(\"Agent output is not in expected AgentOutput format. Processing as plain text.\")\n                    logger.info(f\"Output: '\\n{result[:2000]}\\n'\")\n\n        except Exception as e:\n            logger.exception(f\"Error executing task for {self.name}: {e}\")\n            raise\n\n        return result\n\ndef find_agent_by_role(agents: List[Agent], role: str) -> Optional[Agent]:\n    \"\"\"\n    Finds an agent in a list by its role.\n\n    Args:\n        agents (List[Agent]): A list of Agent objects.\n        role (str): The role of the agent to find.\n\n    Returns:\n        Optional[Agent]: The found Agent object, or None if not found.\n    \"\"\"\n    logger.debug(f\"Searching for agent with role: {role} in agents: {[agent.name for agent in agents]}\")\n    return next((agent for agent in agents if agent.role == role), None)\n"},{"path":"runtime/core.py","content":"\"\"\"This module provides the core data structures for the multi-agent runtime.\n\nIt defines the ExecutionContext for managing workflow state and the Session\nfor handling artifacts and session-specific data.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List\nimport uuid\nimport os\nimport logging\nimport json\n\nfrom runtime.custom_types import Plan, Task\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContextItem:\n    def __init__(self, name: str, content: Any, step: Task):\n        self.name = name\n        self.content = content\n        self.step = step\n\n\n@dataclass\nclass ExecutionContext:\n    \"\"\"Holds the state and context for a multi-agent workflow.\"\"\"\n    session: 'Session'\n    plan: Plan\n    step_index: int = 0\n    round_num: int = 0\n    items: Dict[str, ContextItem] = field(default_factory=dict)\n\n    def next(self) -> None:\n        self.step_index += 1\n\n    def reset(self) -> None:\n        self.step_index = 0\n\n    def current_step(self) -> Task:\n        \"\"\"Returns the current step in the plan.\"\"\"\n        return self.plan.tasks[self.step_index]\n\n    def _remember_artifact(self, key: str, value: Any, step: Task | None = None) -> None:\n        \"\"\"Remembers an artifact from a step's execution for future tasks.\"\"\"\n        self.items[key] = ContextItem(name=key, content=value, step=step or self.current_step())\n\n    def record_artifact(self, key: str, value: Any, mem: bool = False) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n            mem (bool): If True, the artifact is also stored in the execution context's memory.\n        \"\"\"\n        artifact_key = f\"__{self.round_num}/__step_{self.step_index}_{key}\"\n        self.session.add_artifact(artifact_key, value)\n        if mem:\n            self._remember_artifact(artifact_key, value)\n\n    def record_initial(self, key: str, value: Any) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n        \"\"\"\n        self._remember_artifact(key, value, step=Task(\n            id='initial',\n            description='Initial files provided by the user',\n            role='User',\n            agent='User',\n            requires=[]\n        ))\n\n\n\n\n@dataclass\nclass Session:\n    \"\"\"Manages the runtime context for a task, including session ID and artifact storage.\"\"\"\n    session_id: str = field(default_factory=lambda: f\"session_{uuid.uuid4()}\")\n    agents: list = field(default_factory=list) # Type hint will be updated later to List[Agent]\n    state: str = \"initialized\"\n    session_dir: str = field(init=False)\n    project_root: str = field(default_factory=str)\n\n    def __post_init__(self) -> None:\n        \"\"\"Initializes the session directory.\"\"\"\n        if not self.project_root:\n            self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n        self.session_dir = os.path.join(self.project_root, 'sessions', self.session_id)\n        os.makedirs(self.session_dir, exist_ok=True)\n        logger.info(f\"Session created: {self.session_id} (Directory: {self.session_dir})\")\n\n    def add_artifact(self, name: str, content: Any) -> None:\n        \"\"\"\n        Saves an artifact in the session directory.\n\n        Args:\n            name (str): The name of the artifact file.\n            content (Any): The content to write to the artifact file. Can be dict, list, or string.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            os.makedirs(os.path.dirname(artifact_path), exist_ok=True)\n            with open(artifact_path, 'w', encoding='utf-8') as f:\n                if isinstance(content, (dict, list)):\n                    json.dump(content, f, indent=4)\n                else:\n                    f.write(str(content))\n            logger.info(f\"Artifact '{name}' saved in session {self.session_id}.\")\n        except (TypeError, IOError) as e:\n            logger.error(f\"Error saving artifact '{name}': {e}\")\n\n    def get_artifact(self, name: str) -> Any:\n        \"\"\"\n        Loads an artifact from the session directory.\n\n        Args:\n            name (str): The name of the artifact file to load.\n\n        Returns:\n            Any: The content of the artifact, or None if an error occurs.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            with open(artifact_path, 'r', encoding='utf-8') as f:\n                if name.endswith('.json'):\n                    return json.load(f)\n                return f.read()\n        except (FileNotFoundError, json.JSONDecodeError, IOError) as e:\n            logger.error(f\"Error retrieving artifact '{name}': {e}\")\n            return None\n"},{"path":"runtime/custom_types.py","content":"\"\"\"This module defines custom data types and Pydantic models for the runtime.\n\nIt ensures data consistency and validation across different modules by providing\ncentralized, well-defined data structures for plans, tasks, and agent outputs.\n\"\"\"\n\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass Task(BaseModel):\n    \"\"\"A single actionable step in a plan.\"\"\"\n    id: str = Field(..., description=\"Unique ID for the task (e.g., 'T1', 'T2').\")\n    description: str = Field(..., description=\"Detailed description of what to do for this task.\")\n    role: str = Field(..., description=\"The role responsible for this task.\")\n    agent: str = Field(..., description=\"The agent assigned to this task.\")\n    requires: List[str] = Field(..., description=\"List of tasks (IDs) whose outputs are required (e.g. ['T1', 'T2']).\")\n\n\nclass File(BaseModel):\n    \"\"\"Represents a generated file.\"\"\"\n    path: str = Field(..., description=\"File path or name (e.g., 'src/main.py').\")\n    content: str = Field(..., description=\"Full content of the file.\")\n\n\nclass Artifact(BaseModel):\n    \"\"\"Output artifact generated by a task.\"\"\"\n    task: str = Field(..., description=\"ID of the task that produced this artifact (e.g., 'T1').\")\n    files: List[File] = Field(..., description=\"List of files created or modified by the task.\")\n\n\nclass Role(BaseModel):\n    \"\"\"Definition of a role in the team.\"\"\"\n    title: str = Field(..., description=\"Title of the role (e.g., 'Engineer', 'Reviewer').\")\n    purpose: str = Field(..., description=\"The main purpose and goal of this role.\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"System prompt assigned to an agent.\"\"\"\n    agent: str = Field(..., description=\"Name of the agent this prompt is for. Must match exactly an existing agent.\")\n    role: str = Field(..., description=\"Role context of the agent.\")\n    system_prompt: str = Field(..., description=\"Full text of the system prompt for this agent.\")\n\n\nclass Team(BaseModel):\n    \"\"\"Metadata and coordination for the team.\"\"\"\n    notes: str = Field(..., description=\"General notes or feedback about the team's performance or the plan.\")\n    prompts: List[Prompt] = Field(default_factory=list, description=\"A list of new or updated system prompts for any agent in the team.\")\n\n\nclass Plan(BaseModel):\n    \"\"\"Overall strategy with reasoning, roles, and tasks.\"\"\"\n    high_level_goal: str = Field(..., description=\"The main goal this plan is designed to achieve.\")\n    reasoning: str = Field(..., description=\"A brief explanation of the plan's structure and strategy.\")\n    roles: List[Role] = Field(..., description=\"List of all roles required to execute the plan.\")\n    tasks: List[Task] = Field(..., description=\"A step-by-step sequence of tasks to be executed in order.\")\n    team: Optional[Team] = Field(default=None, description=\"Updates to team configuration or system prompts.\")\n\n\nclass AgentOutput(BaseModel):\n    \"\"\"Structured output returned by an agent after executing a task.\"\"\"\n    output: str = Field(..., description=\"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\")\n    artifact: Optional[Artifact] = Field(default=None, description=\"Any files created or modified by the agent during the task.\")\n    team: Optional[Team] = Field(default=None, description=\"Updates to team configuration or system prompts.\")\n    reasoning: Optional[str] = Field(default=None, description=\"Explanation of how the agent arrived at this output.\")\n\n\n\nif __name__ == \"__main__\":\n    import json\n    print(json.dumps(AgentOutput.model_json_schema(), indent=4))\n"},{"path":"runtime/__init__.py","content":"# This file makes the 'runtime' directory a Python package.\n\nfrom .custom_types import Role, Task, Plan, Artifact, File, Prompt, Team, AgentOutput\n\nfrom .agent import Agent\nfrom .core import Session\nfrom .orchestrator import Orchestrator\nfrom .system import System\n"},{"path":"runtime/llm.py","content":"\"\"\"This module abstracts interactions with Large Language Models (LLMs).\n\nIt provides a unified interface for various LLM providers and models,\nallowing for easy integration and interchangeability of different LLMs.\n\"\"\"\n\nimport json\nimport os\nimport time\nfrom google import genai\nfrom google.genai import types\nfrom ollama import Client as Ollama\nfrom typing import Optional, Any, Dict\nfrom abc import ABC, abstractmethod\nimport logging\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLM(ABC):\n    \"\"\"\n    Abstract base class for Large Language Models.\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    def __init__(self) -> None:\n        pass\n\n    @abstractmethod\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: Any = None) -> Optional[str]: # type: ignore\n        \"\"\"Generates content using an LLM.\"\"\"\n        pass\n\n    @staticmethod\n    def factory(species: str) -> 'LLM':\n        print(f\"LLM Factory: Creating LLM instance for species '{species}'\")\n        if species == 'Olli':\n            return Olli()\n        elif species == 'Kimi':\n            return Kimi()\n        if species.startswith('ollama:'):\n            # Extract model name after 'ollama:'\n            model_name = species.split(':', 1)[1]\n            return Olli(species=model_name)\n        if species.startswith('opi:'):\n            # Extract model name after 'opi:'\n            model_name = species.split(':', 1)[1]\n            return Opi(species=model_name)\n        if species.startswith('mistral:'):\n            # Extract model name after 'mistral:'\n            model_name = species.split(':', 1)[1]\n            return Mistral(species=model_name)\n        return Gemini(species)\n\n\nclass Gemini(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients: Dict[str, genai.Client] = {} # Class-level cache for clients\n\n    def __init__(self, species: str) -> None:\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        config = types.GenerateContentConfig(\n            system_instruction=system_instruction,\n            temperature=temperature,\n            response_mime_type=response_mime_type,\n            response_schema=response_schema,\n            max_output_tokens=50000,\n        )\n\n        # try three times\n        for _ in range(3):\n            try:\n                response = client.models.generate_content(\n                    model=model_name,\n                    contents=contents,\n                    config=config,\n                )\n                if not response.candidates or response.candidates[0].content is None or response.candidates[0].content.parts is None or not response.candidates[0].content.parts or response.candidates[0].content.parts[0].text is None:\n                    raise ValueError(f\"Gemini: No content in response from model {model_name}. Retrying...\")\n                break\n            except Exception as ex:\n                logger.exception(f\"Error generating content with model {model_name}: {ex}\")\n                # If it's the last retry, return None\n                if _ == 2:\n                    return None\n                logger.warning(f\"Retrying content generation for model {model_name}...\")\n                time.sleep(10)\n                #return None\n\n        if isinstance(response.parsed, BaseModel):\n            return response.parsed\n\n        if response_mime_type == 'application/json':\n            try:\n                # Attempt to parse the JSON response\n                json_output = response.candidates[0].content.parts[0].text.strip()\n                # Validate against schema if provided\n                if response_schema:\n                    response_schema.model_validate_json(json_output)\n                return json_output\n            except Exception as ex:\n                logger.exception(f\"Error parsing or validating JSON response: {ex}\")\n\n        return response.candidates[0].content.parts[0].text.strip()\n\n    def _get_client(self) -> genai.Client:\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if self.species not in Gemini._clients:\n                logger.info(f\"Initializing GenAI client for species {self.species}\")\n                Gemini._clients[self.species] = genai.Client()\n            return Gemini._clients[self.species]\n        except Exception as e:\n            logger.exception(f\"Error initializing GenAI client: {e}\")\n            raise\n\n\nclass Olli(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Olli'):\n        self.client = None\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        print(f\"Olli: Using model {model_name} with temperature {temperature}\")\n        client = self._get_client(species=self.species)\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n            fmt = response_schema.model_json_schema(by_alias=False, mode='serialization')\n            print(f\"Olli: Using response format {fmt}\")\n            response = client.generate(\n                model=self.species,#model_name,\n                prompt=contents,\n                format=fmt,\n                options={\"temperature\": temperature, \"system_instruction\": system_instruction},\n                stream=True\n            )\n            for chunk in response:\n                if hasattr(chunk, \"response\") and chunk.response:\n                    print(chunk.response, end=\"\")\n                    out += chunk.response\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    @staticmethod\n    def _get_client(species: str):\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if (Olli._clients is None):\n                Olli._clients = {}\n            if species not in Olli._clients:\n                Olli._clients[species] = Ollama()#base_url='http://localhost:11434')\n            return Olli._clients[species]\n        except Exception as e:\n            logger.exception(f\"Error initializing Ollama client: {e}\")\n            return None\n\n\n\n\n\nfrom huggingface_hub import InferenceClient, ChatCompletionInputResponseFormatText, ChatCompletionInputResponseFormatJSONObject, ChatCompletionInputResponseFormatJSONSchema, ChatCompletionInputJSONSchema\n\n\nclass Kimi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Kimi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            stream = client.chat.completions.create(\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=ChatCompletionInputResponseFormatText() if response_mime_type == 'text/plain' else ChatCompletionInputResponseFormatJSONObject() #if response_schema is None else ChatCompletionInputResponseFormatJSONSchema(json_schema=ChatCompletionInputJSONSchema(name=response_schema.model_json_schema()))\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a inference client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Kimi._clients:\n                    Kimi._clients[self.species] = InferenceClient(\n                        provider=\"featherless-ai\",\n                        api_key=os.environ.get(\"HF_TOKEN\"),\n                        model=\"moonshotai/Kimi-K2-Instruct\"\n                    )\n            return Kimi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing GenAI client: {e}\")\n            return None\n\n\n\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import ResponseFormat\n\n\nclass Opi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Opi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            response_format: ResponseFormat = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = {\"type\": \"json_schema\", \"json_schema\": response_schema.model_json_schema()}   # type: ignore\n\n            print(f\"Opi: Using response format {response_format}\")\n\n            stream = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=response_format\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a OpenAI client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Opi._clients:\n                    Opi._clients[self.species] = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"), base_url=os.environ.get(\"OPENAI_API_BASE\"))\n            return Opi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing OpenAI client: {e}\")\n            return None\n\n\n\n##\n\n\nfrom mistralai import Mistral as MistralClient\nfrom mistralai.extra.utils import response_format_from_pydantic_model\n\nclass Mistral(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Mistral'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified Mistral model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"mistral-large-latest\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            str: The response from the Mistral model.\n        \"\"\"\n        client = Mistral._get_client(species=self.species)\n        if not client:\n            return None\n        try:\n            messages = []\n            if system_instruction:\n                messages.append({\"role\": \"system\", \"content\": system_instruction})\n            messages.append({\"role\": \"user\", \"content\": contents})\n\n            print(f\"Mistral: Using model {model_name} with temperature {temperature}\")\n\n            response_format = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = response_format_from_pydantic_model(response_schema)  # type: ignore\n\n\n            out = \"\"\n            stream = client.chat.stream(\n                model=self.species,#model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=50000,\n                response_format=response_format # type: ignore\n            )\n            for chunk in stream:\n                if chunk.data.choices[0].delta.content is None:\n                    continue\n                print(chunk.data.choices[0].delta.content, end=\"\")\n                out += str(chunk.data.choices[0].delta.content)\n\n            return out\n        except Exception as e:\n            print(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    @staticmethod\n    def _get_client(species: str):\n        \"\"\"\n        Returns a Mistral client instance.\n        \"\"\"\n        try:\n            if Mistral._clients is None:\n                Mistral._clients = {}\n            if species not in Mistral._clients:\n                api_key = os.environ.get(\"MISTRAL_API_KEY\")\n                if not api_key:\n                    raise ValueError(\"MISTRAL_API_KEY environment variable not set.\")\n                Mistral._clients[species] = MistralClient(api_key=api_key)\n            return Mistral._clients[species]\n        except Exception as e:\n            logger.exception(f\"Error initializing Mistral client: {e}\")\n            return None\n"},{"path":"runtime/log.py","content":"\"\"\"This module configures and sets up the logging for the application.\n\nIt defines the log format, level, and handlers (console, file, JSONL),\nproviding a centralized and consistent logging setup.\n\"\"\"\n\nimport logging\nimport os\nimport re\nimport json\nfrom logging.handlers import RotatingFileHandler\nfrom colorama import Fore, Style\n\n# --- Configuration ---\nLOG_LEVEL = \"INFO\"\nLOG_FILE_DIR = \"logs\"\nLOG_FILE_NAME = \"app.log\"\nLOG_FILE_PATH = os.path.join(LOG_FILE_DIR, LOG_FILE_NAME)\nMAX_FILE_SIZE = 1024 * 1024  # 1 MB\nBACKUP_COUNT = 5\n\n# --- Colored Formatter ---\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"\n    A custom logging formatter that adds colors to the log messages.\n    It can color based on log level, or on regex matching of the message.\n    \"\"\"\n    # ANSI escape codes for colors\n    COLORS = {\n        'DEBUG': Fore.YELLOW,\n        'INFO': Fore.CYAN,\n        'WARNING': Fore.YELLOW,\n        'ERROR': Fore.RED,\n        'CRITICAL': Fore.MAGENTA,\n        'RESET': Style.RESET_ALL\n    }\n\n    # Regex-based coloring rules (pattern, color)\n    REGEX_RULES_MESSAGES = [\n        (re.compile(r\"Agent instance created:\"), Fore.YELLOW),\n        (re.compile(r\"Agent '.*' system prompt updated.\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing task\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing step\"), Fore.LIGHTRED_EX),\n        (re.compile(r\"Agent '.*' completed task\"), Fore.BLUE),\n        (re.compile(r\"Warning: Agent output is not in expected AgentOutput format.\"), Fore.YELLOW),\n        (re.compile(r\"System Runtime\"), Fore.CYAN),\n        (re.compile(r\"Error: No agents could be instantiated.\"), Fore.RED),\n        (re.compile(r\"Session created:\"), Fore.GREEN),\n        (re.compile(r\"Artifact '.*' saved in session .*\"), Fore.LIGHTBLUE_EX),\n        (re.compile(r\"Error saving artifact '.*'\"), Fore.RED),\n        (re.compile(r\"Files provided:\"), Fore.MAGENTA),\n        (re.compile(r\"Error reading file .*\"), Fore.RED),\n        (re.compile(r\"Generated plan:\"), Fore.WHITE),\n        (re.compile(r\"Orchestration failed: Could not generate a valid plan.\"), Fore.RED),\n        (re.compile(r\"Orchestrator .* is generating a plan for\"), Fore.BLUE),\n        (re.compile(r\"Orchestrator .* is starting workflow round\"), Fore.CYAN),\n        (re.compile(r\"Orchestrator has completed workflow round\"), Fore.CYAN),\n        (re.compile(r\"Agent .* provided new prompts.\"), Fore.LIGHTGREEN_EX),\n        (re.compile(r\"Target agent '.*' not found for prompt update.\"), Fore.YELLOW),\n        (re.compile(r\"Error generating or validating plan for .*\"), Fore.RED),\n    ]\n\n    COLORS_NAMES = {\n        'runtime.orchestrator': Fore.LIGHTYELLOW_EX,\n        'runtime.agent': Fore.LIGHTWHITE_EX,\n        'runtime.sysmain': Fore.LIGHTRED_EX,\n        'runtime.core': Fore.LIGHTBLUE_EX,\n        'runtime.bootstrap': Fore.LIGHTRED_EX,\n        'runtime.loader': Fore.LIGHTRED_EX,\n        'runtime.llm': Fore.LIGHTMAGENTA_EX,\n        'runtime.util': Fore.GREEN,\n        'runtime.factory': Fore.LIGHTRED_EX,\n        'runtime.paths': Fore.BLUE,\n        'runtime.custom_types': Fore.CYAN,\n    }\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"\n        Formats the log record with the appropriate color.\n        \"\"\"\n        log_message = super().format(record)\n\n        # Check for regex-based coloring first\n        for pattern, color in self.REGEX_RULES_MESSAGES:\n            if pattern.search(record.getMessage()):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Check for logger name-based coloring\n        for name_prefix, color in self.COLORS_NAMES.items():\n            if record.name.startswith(name_prefix):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Fallback to level-based coloring\n        return f\"{self.COLORS.get(record.levelname, self.COLORS['RESET'])}{log_message}{self.COLORS['RESET']}\"\n\n# --- Setup ---\ndef setup_logging(level: str = LOG_LEVEL) -> None:\n    \"\"\"\n    Sets up logging for the application.\n    \"\"\"\n    # Create log directory if it doesn't exist\n    if not os.path.exists(LOG_FILE_DIR):\n        os.makedirs(LOG_FILE_DIR)\n\n    # Get the root logger\n    logger = logging.getLogger()\n    logger.setLevel(level)\n\n    # --- Console Handler ---\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(level)\n\n    # --- File Handler ---\n    file_handler = RotatingFileHandler(\n        LOG_FILE_PATH,\n        maxBytes=MAX_FILE_SIZE,\n        backupCount=BACKUP_COUNT\n    )\n    file_handler.setLevel(level)\n\n    # --- Formatters ---\n    console_formatter = ColoredFormatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    file_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n\n    class JsonFormatter(logging.Formatter):\n        def format(self, record: logging.LogRecord):\n            log_record = {\n                \"timestamp\": self.formatTime(record, self.datefmt),\n                \"name\": record.name,\n                \"level\": record.levelname,\n                \"message\": record.getMessage(),\n                \"pathname\": record.pathname,\n                \"lineno\": record.lineno,\n                \"funcName\": record.funcName,\n                \"process\": record.process,\n                \"thread\": record.thread,\n                \"threadName\": record.threadName,\n            }\n            if record.exc_info:\n                log_record[\"exc_info\"] = self.formatException(record.exc_info)\n            if record.stack_info:\n                log_record[\"stack_info\"] = self.formatStack(record.stack_info)\n            return json.dumps(log_record)\n\n    # --- Add Formatter to Handlers ---\n    console_handler.setFormatter(console_formatter)\n    file_handler.setFormatter(file_formatter)\n\n    # --- Add Handlers to Logger ---\n    # Avoid adding handlers multiple times\n    if not logger.handlers:\n        logger.addHandler(console_handler)\n        logger.addHandler(file_handler)\n\n        # --- JSONL File Handler ---\n        jsonl_file_handler = RotatingFileHandler(\n            os.path.join(LOG_FILE_DIR, \"app.jsonl\"),\n            maxBytes=MAX_FILE_SIZE,\n            backupCount=BACKUP_COUNT\n        )\n        jsonl_file_handler.setLevel(level)\n        jsonl_file_handler.setFormatter(JsonFormatter())\n        logger.addHandler(jsonl_file_handler)\n\n# --- Main Application Logger ---\n# It's good practice to use a named logger instead of the root logger directly\napp_logger = logging.getLogger(__name__)\n\n# --- Example Usage ---\nif __name__ == '__main__':\n    setup_logging()\n    app_logger.info(\"Logging setup complete.\")\n    app_logger.debug(\"This is a debug message.\")\n    app_logger.warning(\"This is a warning message.\")\n    app_logger.error(\"This is an error message.\")\n"},{"path":"runtime/orchestrator.py","content":"\"\"\"This module defines the Orchestrator agent and its planning capabilities.\n\nThe Orchestrator is a specialized agent responsible for generating, managing,\nand executing the workflow plan based on a high-level goal. This is being\nrefactored to move workflow execution to the System class.\n\"\"\"\n\nimport os\nimport json\nfrom typing import List, Dict, Optional, Tuple\nimport logging\n\nfrom pydantic import BaseModel, Field, ValidationError\n\nfrom runtime.agent import Agent\nfrom runtime.core import Session\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import Plan, File\n\nclass Orchestrator(Agent):\n    \"\"\"An agent responsible for creating and managing a plan for multi-agent workflows.\"\"\"\n    team: Dict[str,Agent] = {}\n\n    def generate_plan(self, session: Session, high_level_goal: str, files: List[File] = []) -> Optional[Plan]:\n        \"\"\"\n        Invokes the language model to get a structured plan for the given high-level goal.\n        The plan includes a sequence of tasks and the roles responsible for them.\n\n        Args:\n            session (Session): The current session object.\n            high_level_goal (str): The high-level goal for which to generate a plan.\n            file_contents (List[Tuple[str, str]]): A list of file paths and their contents to provide context to the LLM.\n\n        Returns:\n            Optional[Plan]: The generated plan as a Pydantic object, or None if an error occurs.\n        \"\"\"\n        logger.info(f\"Orchestrator {self.name} is generating a plan for: '{high_level_goal}'\")\n        if not self.llm or not self.team:\n            logger.error(\"Orchestrator client or team not initialized. Cannot generate plan.\")\n            return None\n\n        # Construct a description of the available agents and their roles/goals\n        team_description = \"\\n\".join(\n            f\"- Name: '{agent.name}'\\n\"\n            f\"  Role: `{agent.role}`\\n\"\n            f\"  Goal: \\\"{agent.goal}\\\"\"\n            for agent in self.team.values()\n        )\n\n        # Load the general planning prompt template\n        t20_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) # Not used\n        general_prompt_template = read_file(os.path.join(t20_root, \"prompts\", \"general_planning.txt\")).strip()\n        planning_prompt_parts = [general_prompt_template.format(\n            high_level_goal=high_level_goal,\n            team_description=team_description\n        )]\n\n        # Include file contents if provided\n        file_section = []\n        for file in files:\n            file_section.append(f\"\\n--- File: '{file.path}'\\n```\\n{file.content}\\n```\")\n        planning_prompt_parts.append(\"\\n\".join(file_section))\n\n        logger.debug(\"Planning prompt parts for LLM: %s\", planning_prompt_parts)\n\n        # Combine prompt parts for the LLM call\n        full_prompt_for_llm = \"\\n\\n\".join(planning_prompt_parts)\n        logger.info(f\"Orchestrator {self.name} Planning Prompt:\\n{full_prompt_for_llm}\")\n\n        session.add_artifact(\"planning_prompt.txt\", full_prompt_for_llm)\n\n        try:\n            response = self.llm.generate_content(\n                model_name=self.model,\n                contents=\"\\n\\n\".join(planning_prompt_parts),\n                system_instruction=self.system_prompt,\n                temperature=0.1,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=Plan\n            )\n            if isinstance(response, Plan):\n                plan = response\n            else:\n                plan = Plan.model_validate_json(response or '{}')\n        except (ValidationError, json.JSONDecodeError) as e:\n            logger.exception(f\"Error generating or validating plan for {self.name}: {e}\")\n            return None\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during plan generation for {self.name}: {e}\")\n            return None\n\n        return plan\n"},{"path":"runtime/paths.py","content":"\"\"\"\nCentralized path constants for the project.\n\nThis module defines constants for directory and file names, allowing them to be\nreused consistently throughout the application.\n\"\"\"\n\n# Directory Names\nAGENTS_DIR_NAME = \"agents\"\nCONFIG_DIR_NAME = \"config\"\nPROMPTS_DIR_NAME = \"prompts\"\n\n# File Names\nRUNTIME_CONFIG_FILENAME = \"runtime.yaml\""},{"path":"runtime/sysmain.py","content":"\"\"\"This module serves as the main entry point for the command-line interface.\n\nIt parses command-line arguments and initiates the system bootstrap process,\nacting as the primary interface for running the multi-agent system.\n\"\"\"\n\nimport os\nimport argparse\nimport logging\n\nfrom runtime.log import setup_logging\nfrom runtime.system import System\nfrom runtime.custom_types import File\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\ndef parse_command_line_arguments() -> argparse.Namespace:\n    \"\"\"\n    Parses command-line arguments for the runtime system.\n\n    Returns:\n        An argparse.Namespace object containing the parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Run the Gemini agent runtime.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    general_group = parser.add_argument_group(\"General Settings\")\n    general_group.add_argument(\"-p\", \"--plan-only\", action=\"store_true\", help=\"Generate only the plan without executing tasks.\")\n    general_group.add_argument(\"-r\", \"--rounds\", type=int, default=1, help=\"The number of rounds to execute the workflow.\")\n    general_group.add_argument(\"-f\", \"--files\", nargs='*', help=\"List of files to be used in the task.\", default=[])\n\n    orchestrator_group = parser.add_argument_group(\"Orchestrator Settings\")\n    orchestrator_group.add_argument(\"-o\", \"--orchestrator\", type=str, help=\"The name of the orchestrator to use.\", default=\"Meta-AI\")\n    orchestrator_group.add_argument(\"-m\", \"--model\", type=str, help=\"Default LLM model to use.\", default=\"gemini-2.5-flash-lite\")\n\n    parser.add_argument(\"task\", type=str, help=\"The initial task for the orchestrator to perform.\")\n\n    args = parser.parse_args()\n\n    if args.rounds < 1:\n        parser.error(\"Number of rounds must be at least 1.\")\n    if not args.task:\n        parser.error(\"The task argument is required.\")\n\n    return args\n\ndef setup_application_logging(log_level: str = \"INFO\") -> None:\n    \"\"\"\n    Sets up the global logging configuration for the application.\n\n    Args:\n        log_level: The desired logging level (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\n    \"\"\"\n    try:\n        setup_logging(level=log_level)\n        logger.info(f\"Logging initialized with level: {log_level}\")\n    except ValueError as e:\n        logger.error(f\"Invalid log level '{log_level}': {e}\")\n        raise\n\ndef system_main() -> None:\n    \"\"\"\n    Main entry point for the runtime system.\n    Parses arguments, sets up logging and configuration, and runs the system.\n    \"\"\"\n    try:\n        # 1. Parse command line arguments\n        args = parse_command_line_arguments()\n        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\n        # 2. Initial logging setup\n        setup_application_logging()\n\n        # 3. Convert file paths to File objects\n        file_objects = []\n        for file_path in args.files:\n            try:\n                content = read_file(file_path)\n                if content.startswith(\"Error:\"):\n                    logger.warning(f\"Skipping file '{file_path}': {content}\")\n                    continue\n                file_objects.append(File(path=file_path, content=content))\n            except Exception as e:\n                logger.error(f\"Error processing file '{file_path}': {e}\")\n\n        # 4. Instantiate and set up the system\n        system = System(root_dir=project_root, default_model=args.model)\n        system.setup(orchestrator_name=args.orchestrator)\n\n        # 5. Re-configure logging based on loaded config\n        log_level = system.config.get(\"logging_level\", \"INFO\")\n        setup_application_logging(log_level=log_level)\n\n        # 6. Start the system's main workflow\n        plan = system.start(\n            high_level_goal=args.task,\n            files=file_objects\n        )\n        if args.plan_only:\n            logger.info(\"Plan-only mode: Workflow execution skipped.\")\n            return\n\n        # 7. Run the system's main workflow\n        system.run(\n            plan=plan,\n            rounds=args.rounds,\n            files=file_objects\n        )\n\n    except (FileNotFoundError, RuntimeError) as e:\n        logger.exception(f\"A critical error occurred: {e}\")\n        exit(1)\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred during system main execution: {e}\")\n        exit(1)\n\n\nif __name__ == \"__main__\":\n    system_main()\n"},{"path":"runtime/system.py","content":"\"\"\"This module defines the core System class for the multi-agent runtime.\n\nIt encapsulates the state and behavior of the entire system, including\nconfiguration loading, agent instantiation, and workflow execution.\n\"\"\"\n\nimport os\nimport json\nimport yaml\nfrom glob import glob\n\nimport logging\nfrom typing import Any, List, Optional, Dict\n\nfrom .core import Session, ExecutionContext\nfrom .agent import Agent, find_agent_by_role\nfrom .orchestrator import Orchestrator\nfrom .log import setup_logging\nfrom .paths import AGENTS_DIR_NAME, CONFIG_DIR_NAME, PROMPTS_DIR_NAME, RUNTIME_CONFIG_FILENAME\n\nfrom .custom_types import AgentOutput, Artifact, Plan, File\nlogger = logging.getLogger(__name__)\n\nclass System:\n    \"\"\"\n    Represents the entire multi-agent system, handling setup, execution, and state.\n    \"\"\"\n    def __init__(self, root_dir: str, default_model: str = \"Olli\"):\n        \"\"\"\n        Initializes the System.\n\n        Args:\n            root_dir (str): The root directory of the project.\n            default_model (str): The default LLM model to use.\n        \"\"\"\n        self.root_dir = root_dir\n        self.default_model = default_model\n        self.config: dict = {}\n        self.agents: List[Agent] = []\n        self.session: Optional[Session] = None\n        self.orchestrator: Optional[Orchestrator] = None\n\n    def setup(self, orchestrator_name: Optional[str] = None) -> None:\n        \"\"\"\n        Sets up the system by loading configurations and instantiating agents.\n\n        Args:\n            orchestrator_name (str, optional): The name of the orchestrator to use.\n\n        Raises:\n            RuntimeError: If no agents or orchestrator can be set up.\n        \"\"\"\n        logger.info(\"--- System Setup ---\")\n        print(f\"Using default model: {self.default_model}\")\n\n        self.config = self._load_config(os.path.join(self.root_dir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME))\n        agent_specs = self._load_agent_templates(os.path.join(self.root_dir, AGENTS_DIR_NAME), self.config, self.default_model)\n        prompts = self._load_prompts(os.path.join(self.root_dir, PROMPTS_DIR_NAME))\n\n        all_team_members = {member.lower() for spec in agent_specs if 'team' in spec for member in spec['team']}\n\n        agents: List[Agent] = []\n        for spec in agent_specs:\n            spec[\"model\"] = spec.get(\"model\", self.default_model)\n            if spec['name'].lower() not in all_team_members:\n                agent = self._instantiate_agent(spec, prompts, agent_specs)\n                if agent:\n                    agents.append(agent)\n        self.agents = agents\n\n        if not self.agents:\n            raise RuntimeError(\"No agents could be instantiated. System setup failed.\")\n\n        orchestrator: Optional[Agent] = None\n        if orchestrator_name:\n            orchestrator = next((agent for agent in self.agents if agent.name.lower() == orchestrator_name.lower()), None)\n        else:\n            orchestrator = find_agent_by_role(self.agents, 'Orchestrator')\n\n        if not orchestrator:\n            error_msg = f\"Orchestrator with name '{orchestrator_name}' not found.\" if orchestrator_name else \"Orchestrator with role 'Orchestrator' not found.\"\n            raise RuntimeError(f\"{error_msg} System setup failed.\")\n\n        if not isinstance(orchestrator, Orchestrator):\n            raise RuntimeError(f\"Agent '{orchestrator.name}' is not a valid Orchestrator instance. System setup failed.\")\n\n        self.orchestrator = orchestrator\n        self.session = Session(agents=self.agents, project_root=\"./\")\n        logger.info(\"--- System Setup Complete ---\")\n\n    def start(self, high_level_goal: str, files: List[File] = []) -> Plan:\n        \"\"\"\n        Runs the main workflow of the system.\n\n        Args:\n            initial_task (str): The initial task for the orchestrator to perform.\n            plan_only (bool): If True, only generates the plan without executing tasks.\n            rounds (int): The number of rounds to execute the workflow.\n            files (List[str]): List of files to be used in the task.\n\n        Raises:\n            RuntimeError: If the system is not set up before running.\n        \"\"\"\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call setup() before start().\")\n\n        plan: Optional[Plan] = self.orchestrator.generate_plan(self.session, high_level_goal, files)\n        if not plan:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        logger.debug(f\"{plan.model_dump_json()}\")\n\n        if not plan or not plan.tasks or not plan.roles:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        print(f\"\\n\\nInitial Plan: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n        self.session.add_artifact(\"initial_plan.json\", plan.model_dump_json(indent=4))\n\n        return plan\n\n    def run(self, plan: Plan, rounds: int = 1, files: List[File] = []) -> None:\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call start() before run().\")\n\n        logger.info(\"--- Starting Workflow ---\")\n\n        context = ExecutionContext(session=self.session, plan=plan)\n\n        print(f\"\\n\\nInitial Files: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n\n        #context.record_initial(\"files\", \"\\n\".join([file.model_dump_json(indent=4) for file in files]))\n        context.record_initial(\"files\", Artifact(task='initial',files=files).model_dump_json())\n\n        for context.round_num in range(1, rounds + 1):\n            logger.info(f\"System is starting workflow round {context.round_num} for goal: '{plan.high_level_goal}'\")\n\n            team_by_name = {agent.name: agent for agent in self.orchestrator.team.values()} if self.orchestrator.team else {}\n\n            context.reset()\n\n            while context.step_index < len(plan.tasks):\n                step = context.current_step()\n                agent_name = step.agent\n\n                delegate_agent = team_by_name.get(agent_name)\n                if not delegate_agent:\n                    delegate_agent = next((agent for agent in self.agents if agent.name.lower() == agent_name.lower()), None)\n\n                if not delegate_agent:\n                    logger.warning(f\"No agent found with name '{agent_name}'. Execution will continue with the Orchestrator as the fallback agent.\")\n                    delegate_agent = self.orchestrator\n\n                logger.info(f\"Agent '{delegate_agent.name}' is executing step {context.step_index + 1}/{len(plan.tasks)}: '{step.description}' (Role: {step.role})\")\n\n                result = delegate_agent.execute_task(context)\n                if result:\n                    context.record_artifact(f\"{delegate_agent.name}_result.txt\", result, True)\n                    try:\n                        agent_output = AgentOutput.model_validate_json(result)\n                        if agent_output.team and agent_output.team.prompts:\n                            logger.info(f\"Agent {delegate_agent.name} provided new prompts.\")\n                            for prompt_data in agent_output.team.prompts:\n                                self._update_agent_prompt(self.session, prompt_data.agent, prompt_data.system_prompt)\n                    except (json.JSONDecodeError, Exception) as e:\n                        logger.warning(f\"Could not parse agent output as AgentOutput: {e}. Treating as plain text.\")\n                context.next()\n            logger.info(f\"System has completed workflow round {context.round_num}.\")\n        logger.info(\"--- Workflow Complete ---\")\n\n    def _update_agent_prompt(self, session: Session, agent_name: str, new_prompt: str) -> None:\n        \"\"\"\n        Updates an agent's system prompt.\n\n        Args:\n            session (Session): The current session object.\n            agent_name (str): The name or role of the agent to update.\n            new_prompt (str): The new system prompt.\n        \"\"\"\n        if not (agent_name and new_prompt):\n            return\n\n        if not (self.orchestrator and self.orchestrator.team):\n            logger.warning(\"Orchestrator or its team is not initialized. Cannot update agent prompt.\")\n            return\n\n        # Combine team agents and session agents for a comprehensive search, ensuring uniqueness\n        all_agents = {agent.name: agent for agent in list(self.orchestrator.team.values()) + session.agents}.values()\n\n        # First, try to find by name\n        target_agent = next((agent for agent in all_agents if agent.name.lower() == agent_name.lower()), None)\n\n        # If not found by name, try to find by role\n        if not target_agent:\n            target_agent = next((agent for agent in all_agents if agent.role.lower() == agent_name.lower()), None)\n\n        if target_agent:\n            target_agent.update_system_prompt(new_prompt)\n            logger.info(f\"Agent '{target_agent.name}' (matched by '{agent_name}') system prompt updated.\")\n        else:\n            logger.warning(f\"Target agent '{agent_name}' not found for prompt update.\")\n\n    def _instantiate_agent(self, agent_spec: Dict[str, Any], prompts: Dict[str, str], all_agent_specs: List[Dict[str, Any]]) -> Optional[Agent]:\n        \"\"\"\n        Instantiates an Agent (or a subclass) based on the provided specification.\n\n        Args:\n            agent_spec (Dict[str, Any]): A dictionary containing the agent's specifications.\n            prompts (Dict[str, str]): A dictionary of available system prompts.\n            all_agent_specs (List[Dict[str, Any]]): A list of all agent specifications for team resolution.\n\n        Returns:\n            Optional[Agent]: An instantiated Agent object, or None if the prompt is not found.\n        \"\"\"\n        prompt_key = f\"{agent_spec['name'].lower()}_instructions.txt\"\n        if prompt_key not in prompts:\n            logger.warning(\n                f\"Prompt for agent '{agent_spec['name']}' not found with key '{prompt_key}'. Empty system prompt will be used.\"\n            )\n\n        agent = Orchestrator(\n            name=agent_spec.get(\"name\", \"Unnamed Agent\"),\n            role=agent_spec.get(\"role\", \"Agent\"),\n            goal=agent_spec.get(\"goal\", \"\"),\n            model=agent_spec.get(\"model\", \"default-model\"),\n            system_prompt=\"\" if prompt_key not in prompts else prompts[prompt_key],\n        )\n\n        if agent_spec.get(\"delegation\") and \"team\" in agent_spec:\n            agent.team = {}\n            for team_member_name in agent_spec[\"team\"]:\n                member_spec = next(\n                    (\n                        s\n                        for s in all_agent_specs\n                        if s[\"name\"].lower() == team_member_name.lower()\n                    ),\n                    None,\n                )\n                if member_spec:\n                    member_agent = self._instantiate_agent(\n                        member_spec, prompts, all_agent_specs\n                    )  # Recursive call\n                else:\n                    logger.warning(\n                        f\"Team member '{team_member_name}' not found in agent specs.\"\n                    )\n                    member_agent = self._instantiate_agent(\n                        agent_spec={\n                            \"name\": team_member_name,\n                            \"role\": \"Unknown\",\n                            \"goal\": \"\",\n                            \"model\": agent_spec.get(\"model\", \"default-model\"),\n                            \"system_prompt\": \"\",\n                        },\n                        prompts=prompts,\n                        all_agent_specs=all_agent_specs,\n                    )  # Recursive call\n                if member_agent:\n                    agent.team[team_member_name] = member_agent\n        return agent\n\n    def _load_config(self, config_path: str) -> Any:\n        \"\"\"\n        Loads the runtime configuration from a YAML file.\n\n        Args:\n            config_path (str): The absolute path to the configuration file.\n\n        Returns:\n            dict: The loaded configuration as a dictionary.\n        \"\"\"\n        logger.info(f\"Loading configuration from: {config_path}\")\n        with open(config_path, 'r', encoding='utf-8') as f:\n            return yaml.safe_load(f)\n\n    def _load_agent_templates(self, agents_dir: str, config: dict, default_model: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Loads all agent specifications from YAML files within a specified directory.\n\n        Args:\n            agents_dir (str): The absolute path to the directory containing agent YAML files.\n\n        Returns:\n            list: A list of dictionaries, where each dictionary represents an agent's specification.\n        \"\"\"\n        logger.info(f\"Loading agent templates from: {agents_dir}\")\n        agent_files = glob(os.path.join(agents_dir, '*.yaml'))\n        templates = []\n        for agent_file in agent_files:\n            with open(agent_file, 'r', encoding='utf-8') as f:\n                template = yaml.safe_load(f)\n                # Resolve prompt path relative to the agent file's directory\n                if 'system_prompt' in template and template['system_prompt']:\n                    base_dir = os.path.dirname(agent_file)\n                    prompt_path = os.path.abspath(os.path.join(base_dir, template['system_prompt']))\n                    template['system_prompt_path'] = prompt_path\n                template[\"model\"] = default_model\n                templates.append(template)\n        logger.info(f\"{len(templates)} agent templates loaded.\")\n        return templates\n\n    def _load_prompts(self, prompts_dir: str) -> Dict[str, str]:\n        \"\"\"\n        Loads all system prompts from text files within a specified directory.\n\n        Args:\n            prompts_dir (str): The absolute path to the directory containing prompt text files.\n\n        Returns:\n            dict: A dictionary where keys are prompt filenames (e.g., 'agent_instructions.txt')\n                and values are the content of the prompt files.\n        \"\"\"\n        logger.info(f\"Loading prompts from: {prompts_dir}\")\n        prompt_files = glob(os.path.join(prompts_dir, '*.txt'))\n        prompts = {}\n        for prompt_file in prompt_files:\n            with open(prompt_file, 'r', encoding='utf-8') as f:\n                # Use the absolute path as the key for reliable lookup\n                prompts[os.path.basename(prompt_file)] = f.read()\n        logger.info(f\"{len(prompts)} prompts loaded.\")\n        return prompts\n"},{"path":"runtime/util.py","content":"\"\"\"This module provides miscellaneous utility functions for the runtime.\n\nIt includes helper functions that are used across different modules\nbut do not belong to a specific component.\n\"\"\"\n\nimport os\n\ndef read_file(file_path: str) -> str:\n    \"\"\"Reads content from a given file path.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except FileNotFoundError:\n        return f\"Error: File not found at {file_path}\"\n    except Exception as e:\n        return f\"Error reading file {file_path}: {e}\"\n"}]}

--- Artifact '__1/__step_0_GPTASe_result.txt' from (GPTASe) in [T1]:
{"output":"Analysis of the Python runtime documentation and source code has been completed. The following Task Agnostic Steps (TAS) have been identified, representing the core functionalities and operations of the runtime.","artifact":{"task":"T2","files":[{"path":"task_agnostic_steps.md","content":"# Task Agnostic Steps (TAS) for Runtime System\n\nThis document outlines the fundamental, language-agnostic steps performed by the runtime system.\n\n## 1. System Initialization and Configuration\n\n*   **Load System Configuration**: Read and parse configuration settings from a defined source (e.g., YAML file).\n*   **Discover and Load Agents**: Identify and load agent definitions from a specified location (e.g., directory of files).\n*   **Discover and Load Prompts**: Identify and load system prompts associated with agents from a specified location (e.g., directory of text files).\n*   **Instantiate Agents**: Create instances of agents based on their specifications, including their name, role, goal, associated LLM model, and system prompt.\n*   **Initialize LLM Clients**: Set up connections or clients for Large Language Models (LLMs) based on configuration.\n*   **Set Up Logging**: Configure the application's logging mechanism, including format, level, and output destinations (console, file).\n\n## 2. Workflow Planning\n\n*   **Receive High-Level Goal**: Accept the overarching objective for the workflow.\n*   **Describe Available Agents and Roles**: Provide a description of the agents and their capabilities to the planning agent.\n*   **Generate Plan**: Use an LLM to break down the high-level goal into a sequence of structured tasks, defining roles, agents, and dependencies.\n*   **Validate Plan**: Ensure the generated plan is valid and adheres to the defined structure (e.g., Pydantic schema).\n*   **Record Initial Plan**: Store the generated plan as an artifact.\n\n## 3. Workflow Execution\n\n*   **Initialize Session**: Create and manage a session for the entire workflow, including session ID and artifact storage directory.\n*   **Initialize Execution Context**: Set up the context for a specific round and step of the workflow, including the plan, current step, and available artifacts.\n*   **Record Initial Artifacts**: Store any initial input files or data provided at the start of the workflow.\n*   **Iterate Through Workflow Rounds**: Execute the workflow for a specified number of rounds.\n*   **Iterate Through Tasks in Plan**: Process each task sequentially within a round based on the plan.\n*   **Identify and Select Agent**: Determine the appropriate agent responsible for executing the current task.\n*   **Prepare Task Prompt**: Construct the prompt for the agent, including the overall goal, specific task description, agent's role and goal, and relevant previous artifacts.\n*   **Record Task Prompt**: Store the generated prompt as an artifact.\n*   **Execute Task**: Delegate the task execution to the selected agent.\n*   **Agent Task Execution**: The agent processes the prompt, potentially interacting with an LLM.\n*   **LLM Interaction**: Send the prompt to an LLM and receive a response.\n*   **Parse LLM Response**: Process the LLM's output, which may be structured (e.g., JSON) or plain text.\n*   **Handle Agent Output**: Parse the agent's output, extracting the main message, any generated artifacts (files), and potential team updates (like new prompts).\n*   **Record Task Result**: Store the agent's output and any generated artifacts.\n*   **Update Agent Prompts**: If the agent's output includes updated system prompts, apply these changes to the relevant agents.\n*   **Advance Execution Context**: Move to the next task or step in the plan.\n*   **Handle Task Dependencies**: Ensure tasks are executed only after their prerequisites are met (implicitly handled by sequential plan execution).\n*   **Manage Artifacts**: Store and retrieve artifacts generated during task execution within the session.\n\n## 4. Agent Communication and State Management\n\n*   **Agent Lifecycle Management**: Define and manage the lifecycle of agents (initialization, task execution).\n*   **Contextual Information Sharing**: Provide agents with the necessary context (plan, previous artifacts, goal) for task execution.\n*   **Artifact Storage and Retrieval**: Save and load artifacts generated by agents throughout the workflow.\n*   **Session Management**: Maintain the state and data for an entire workflow session.\n*   **System Prompt Updates**: Allow agents to update their own or other agents' system prompts dynamically.\n\n## 5. LLM Interaction Abstraction\n\n*   **Abstract LLM Interface**: Define a common interface for interacting with different LLM providers.\n*   **LLM Model Selection**: Allow selection of specific LLM models based on configuration.\n*   **Content Generation**: Utilize LLMs to generate text content based on provided prompts and instructions.\n*   **Response Formatting**: Handle LLM responses, including parsing JSON or other structured formats, and validating against schemas.\n\n## 6. Error Handling and Resilience\n\n*   **Exception Handling**: Catch and log errors during configuration loading, agent instantiation, plan generation, and task execution.\n*   **LLM Call Retries**: Implement retry mechanisms for LLM interactions in case of transient errors.\n*   **Fallback Mechanisms**: Provide fallback agents or behaviors when a specific agent cannot be found or fails.\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_1_GPTASe_result.txt' from (GPTASe) in [T2]:
{"output":"The Task Agnostic Steps (TAS) for the runtime system have been extracted and are provided in the 'task_agnostic_steps.md' file.","artifact":{"task":"T2","files":[{"path":"task_agnostic_steps.md","content":"# Task Agnostic Steps (TAS) for Runtime System\n\nThis document outlines the fundamental, language-agnostic steps performed by the runtime system.\n\n## 1. System Initialization and Configuration\n\n*   **Load System Configuration**: Read and parse configuration settings from a defined source (e.g., YAML file).\n*   **Discover and Load Agents**: Identify and load agent definitions from a specified location (e.g., directory of files).\n*   **Discover and Load Prompts**: Identify and load system prompts associated with agents from a specified location (e.g., directory of text files).\n*   **Instantiate Agents**: Create instances of agents based on their specifications, including their name, role, goal, associated LLM model, and system prompt.\n*   **Initialize LLM Clients**: Set up connections or clients for Large Language Models (LLMs) based on configuration.\n*   **Set Up Logging**: Configure the application's logging mechanism, including format, level, and output destinations (console, file).\n\n## 2. Workflow Planning\n\n*   **Receive High-Level Goal**: Accept the overarching objective for the workflow.\n*   **Describe Available Agents and Roles**: Provide a description of the agents and their capabilities to the planning agent.\n*   **Generate Plan**: Use an LLM to break down the high-level goal into a sequence of structured tasks, defining roles, agents, and dependencies.\n*   **Validate Plan**: Ensure the generated plan is valid and adheres to the defined structure (e.g., Pydantic schema).\n*   **Record Initial Plan**: Store the generated plan as an artifact.\n\n## 3. Workflow Execution\n\n*   **Initialize Session**: Create and manage a session for the entire workflow, including session ID and artifact storage directory.\n*   **Initialize Execution Context**: Set up the context for a specific round and step of the workflow, including the plan, current step, and available artifacts.\n*   **Record Initial Artifacts**: Store any initial input files or data provided at the start of the workflow.\n*   **Iterate Through Workflow Rounds**: Execute the workflow for a specified number of rounds.\n*   **Iterate Through Tasks in Plan**: Process each task sequentially within a round based on the plan.\n*   **Identify and Select Agent**: Determine the appropriate agent responsible for executing the current task.\n*   **Prepare Task Prompt**: Construct the prompt for the agent, including the overall goal, specific task description, agent's role and goal, and relevant previous artifacts.\n*   **Record Task Prompt**: Store the generated prompt as an artifact.\n*   **Execute Task**: Delegate the task execution to the selected agent.\n*   **Agent Task Execution**: The agent processes the prompt, potentially interacting with an LLM.\n*   **LLM Interaction**: Send the prompt to an LLM and receive a response.\n*   **Parse LLM Response**: Process the LLM's output, which may be structured (e.g., JSON) or plain text.\n*   **Handle Agent Output**: Parse the agent's output, extracting the main message, any generated artifacts (files), and potential team updates (like new prompts).\n*   **Record Task Result**: Store the agent's output and any generated artifacts.\n*   **Update Agent Prompts**: If the agent's output includes updated system prompts, apply these changes to the relevant agents.\n*   **Advance Execution Context**: Move to the next task or step in the plan.\n*   **Handle Task Dependencies**: Ensure tasks are executed only after their prerequisites are met (implicitly handled by sequential plan execution).\n*   **Manage Artifacts**: Store and retrieve artifacts generated by agents throughout the workflow.\n\n## 4. Agent Communication and State Management\n\n*   **Agent Lifecycle Management**: Define and manage the lifecycle of agents (initialization, task execution).\n*   **Contextual Information Sharing**: Provide agents with the necessary context (plan, previous artifacts, goal) for task execution.\n*   **Artifact Storage and Retrieval**: Save and load artifacts generated by agents throughout the workflow.\n*   **Session Management**: Maintain the state and data for an entire workflow session.\n*   **System Prompt Updates**: Allow agents to update their own or other agents' system prompts dynamically.\n\n## 5. LLM Interaction Abstraction\n\n*   **Abstract LLM Interface**: Define a common interface for interacting with different LLM providers.\n*   **LLM Model Selection**: Allow selection of specific LLM models based on configuration.\n*   **Content Generation**: Utilize LLMs to generate text content based on provided prompts and instructions.\n*   **Response Formatting**: Handle LLM responses, including parsing JSON or other structured formats, and validating against schemas.\n\n## 6. Error Handling and Resilience\n\n*   **Exception Handling**: Catch and log errors during configuration loading, agent instantiation, plan generation, and task execution.\n*   **LLM Call Retries**: Implement retry mechanisms for LLM interactions in case of transient errors.\n*   **Fallback Mechanisms**: Provide fallback agents or behaviors when a specific agent cannot be found or fails.\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_2_Lyra_result.txt' from (Lyra) in [T3]:
{"output":"Defined the high-level architecture for the TypeScript Node.js runtime based on the extracted Task Agnostic Steps (TAS). This includes identifying key modules, classes, and their interactions, adhering to SOTA principles.","artifact":{"task":"T3","files":[{"path":"src/index.ts","content":"import { System } from './system/System';\nimport { Agent } from './agent/Agent';\nimport { Orchestrator } from './orchestrator/Orchestrator';\nimport { Session } from './core/Session';\nimport { ExecutionContext } from './core/ExecutionContext';\nimport { LLM } from './llm/LLM';\nimport { Logger } from './logger/Logger';\nimport { Config } from './config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from './custom_types';\n\nexport {\n  System,\n  Agent,\n  Orchestrator,\n  Session,\n  ExecutionContext,\n  LLM,\n  Logger,\n  Config,\n  Plan,\n  Task,\n  Artifact,\n  Role,\n  Prompt,\n  Team,\n  AgentOutput,\n  File\n};\n\n// Export default logger instance\nexport const logger = Logger.getInstance();\n\n// Example usage (can be moved to a CLI entry point like sysmain.ts later)\nasync function main() {\n  logger.info('Starting runtime initialization...');\n\n  try {\n    // 1. Load Configuration\n    const config = await Config.load('runtime.yaml'); // Assuming runtime.yaml is in the root or a config dir\n    logger.info('Configuration loaded successfully.');\n\n    // 2. Initialize System\n    const system = new System(config.projectRoot || '.', config.defaultModel || 'gemini-2.5-flash-lite');\n    await system.setup(); // Load agents, prompts, etc.\n    logger.info('System setup complete.');\n\n    // 3. Define a high-level goal and initial files (example)\n    const highLevelGoal = 'Reimplement runtime using TypeScript in Node.js following SOTA coding principles';\n    const initialFiles: File[] = [\n      // Example: { path: 'docs/runtime.md', content: '...' } \n    ];\n\n    // 4. Start the workflow to generate a plan\n    const plan = await system.start(highLevelGoal, initialFiles);\n    logger.info('Workflow plan generated.');\n    console.log('\\n--- Generated Plan ---');\n    console.log(JSON.stringify(plan, null, 2));\n    console.log('----------------------\\n');\n\n    // 5. Run the workflow for a specified number of rounds\n    const rounds = config.workflowRounds || 3;\n    await system.run(plan, rounds, initialFiles);\n    logger.info('Workflow execution finished.');\n\n  } catch (error) {\n    logger.error('Runtime initialization failed:', error);\n  }\n}\n\n// Uncomment to run the example main function\n// main();\n"},{"path":"src/system/System.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { LLM } from '../llm/LLM';\nimport { Logger } from '../logger/Logger';\nimport { Config } from '../config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class System {\n  private config: Config;\n  private agents: Agent[] = [];\n  private session: Session | null = null;\n  private orchestrator: Orchestrator | null = null;\n  private readonly rootDir: string;\n  private readonly defaultModel: string;\n\n  constructor(rootDir: string, defaultModel: string = 'gemini-2.5-flash-lite') {\n    this.rootDir = rootDir;\n    this.defaultModel = defaultModel;\n    logger.info(`System initialized with root directory: ${this.rootDir} and default model: ${this.defaultModel}`);\n  }\n\n  async setup(): Promise<void> {\n    logger.info('--- System Setup ---');\n    this.config = await Config.load('runtime.yaml', this.rootDir);\n    logger.info('Configuration loaded.');\n\n    const agentSpecs = this._loadAgentSpecs(join(this.rootDir, this.config.agentDir || 'agents'));\n    const prompts = this._loadPrompts(join(this.rootDir, this.config.promptDir || 'prompts'));\n\n    const allTeamMembers = new Set<string>();\n    agentSpecs.forEach(spec => {\n      spec.team?.forEach(member => allTeamMembers.add(member.name.toLowerCase()));\n    });\n\n    // Instantiate agents, excluding those already defined as part of another agent's team\n    for (const spec of agentSpecs) {\n      if (!allTeamMembers.has(spec.name.toLowerCase())) {\n        const agent = this._instantiateAgent(spec, prompts, agentSpecs);\n        if (agent) {\n          this.agents.push(agent);\n        }\n      }\n    }\n\n    if (this.agents.length === 0) {\n      throw new Error('No agents could be instantiated. System setup failed.');\n    }\n\n    // Find and set the orchestrator\n    const orchestratorSpec = this.agents.find(\n      agent => agent.role.toLowerCase() === 'orchestrator'\n    );\n    if (!orchestratorSpec) {\n      throw new Error('Orchestrator agent not found. System setup failed.');\n    }\n    this.orchestrator = orchestratorSpec as Orchestrator;\n    logger.info(`Orchestrator set to: ${this.orchestrator.name}`);\n\n    // Initialize session\n    this.session = new Session(this.agents, this.rootDir);\n    logger.info('Session initialized.');\n\n    logger.info('--- System Setup Complete ---');\n  }\n\n  async start(highLevelGoal: string, initialFiles: File[] = []): Promise<Plan> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call setup() before start().');\n    }\n\n    logger.info(`Generating plan for goal: \"${highLevelGoal}\"`);\n    const plan = await this.orchestrator.generatePlan(this.session, highLevelGoal, initialFiles);\n\n    if (!plan) {\n      throw new Error('Orchestration failed: Could not generate a valid plan.');\n    }\n\n    logger.info('Plan generated successfully.');\n    this.session.addArtifact('initial_plan.json', JSON.stringify(plan.toJSON(), null, 2));\n    logger.debug(`Plan details: ${JSON.stringify(plan)}`);\n\n    return plan;\n  }\n\n  async run(plan: Plan, rounds: number = 1, initialFiles: File[] = []): Promise<void> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call start() before run().');\n    }\n\n    logger.info(`--- Starting Workflow (Rounds: ${rounds}) ---`);\n\n    const context = new ExecutionContext(this.session, plan);\n    context.recordInitial('files', new Artifact({ task: 'initial', files: initialFiles }));\n\n    for (let roundNum = 1; roundNum <= rounds; roundNum++) {\n      context.roundNum = roundNum;\n      context.resetStepIndex();\n      logger.info(`Starting workflow round ${roundNum} for goal: \"${plan.highLevelGoal}\"`);\n\n      while (context.stepIndex < plan.tasks.length) {\n        const currentTask = context.currentStep();\n        let delegateAgent = this.agents.find(agent => agent.name.toLowerCase() === currentTask.agent.toLowerCase());\n\n        if (!delegateAgent) {\n          logger.warn(`Agent '${currentTask.agent}' not found for task '${currentTask.description}'. Falling back to Orchestrator.`);\n          delegateAgent = this.orchestrator;\n        }\n\n        logger.info(`Executing step ${context.stepIndex + 1}/${plan.tasks.length}: '${currentTask.description}' by Agent '${delegateAgent.name}' (Role: ${currentTask.role})`);\n\n        try {\n          const agentOutput = await delegateAgent.executeTask(context);\n          if (agentOutput) {\n            context.recordArtifact(`${delegateAgent.name}_result.txt`, agentOutput.output, true);\n            if (agentOutput.artifact) {\n              agentOutput.artifact.files.forEach(file => {\n                this.session?.addArtifact(file.path, file.content);\n                console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n              });\n            }\n            if (agentOutput.team?.prompts) {\n              logger.info(`Agent '${delegateAgent.name}' provided ${agentOutput.team.prompts.length} prompt updates.`);\n              for (const promptData of agentOutput.team.prompts) {\n                this._updateAgentPrompt(promptData.agent, promptData.systemPrompt);\n              }\n            }\n          }\n        } catch (error) {\n          logger.error(`Error executing task '${currentTask.description}' by agent '${delegateAgent.name}':`, error);\n          // Decide on error handling strategy: stop, retry, or continue\n          // For now, we log and continue to the next task.\n        }\n        context.nextStep();\n      }\n      logger.info(`Completed workflow round ${roundNum}.`);\n    }\n\n    logger.info('--- Workflow Complete ---');\n  }\n\n  private _updateAgentPrompt(agentName: string, newPrompt: string): void {\n    const targetAgent = this.agents.find(agent => agent.name.toLowerCase() === agentName.toLowerCase());\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Updated system prompt for agent '${targetAgent.name}'.`);\n    } else {\n      logger.warn(`Agent '${agentName}' not found for prompt update.`);\n    }\n  }\n\n  private _instantiateAgent(spec: AgentSpec, prompts: Record<string, string>, allAgentSpecs: AgentSpec[]): Agent | null {\n    const promptKey = `${spec.name.toLowerCase()}_instructions.txt`;\n    const systemPrompt = prompts[promptKey] || spec.systemPrompt || ''; // Use provided systemPrompt if available, else default to empty\n\n    const model = spec.model || this.defaultModel;\n\n    let agent: Agent;\n\n    // Special handling for Orchestrator to ensure it's an Orchestrator instance\n    if (spec.role.toLowerCase() === 'orchestrator') {\n      agent = new Orchestrator(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n      // Pass team members to Orchestrator\n      if (spec.team && spec.team.length > 0) {\n        (agent as Orchestrator).team = this._buildTeam(spec.team, prompts, allAgentSpecs);\n      }\n    } else {\n      agent = new Agent(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n    }\n\n    logger.info(`Instantiated Agent: ${agent.name} (Role: ${agent.role}, Model: ${agent.model})`);\n    return agent;\n  }\n\n  private _buildTeam(teamSpecs: PromptSpec[], allAgentSpecs: AgentSpec[], depth = 0): Record<string, Agent> {\n    const team: Record<string, Agent> = {};\n    const maxDepth = 5; // Prevent infinite recursion\n\n    if (depth > maxDepth) {\n      logger.error('Maximum team nesting depth reached. Stopping team creation.');\n      return {};\n    }\n\n    for (const teamSpec of teamSpecs) {\n      const memberSpec = allAgentSpecs.find(spec => spec.name.toLowerCase() === teamSpec.name.toLowerCase());\n      if (memberSpec) {\n        const memberAgent = this._instantiateAgent(memberSpec, this.loadPrompts(join(this.rootDir, this.config.promptDir || 'prompts')), allAgentSpecs, depth + 1);\n        if (memberAgent) {\n          team[teamSpec.name] = memberAgent;\n        } else {\n          logger.warn(`Failed to instantiate team member '${teamSpec.name}'. Skipping.`);\n        }\n      } else {\n        logger.warn(`Team member specification for '${teamSpec.name}' not found. Creating a placeholder agent.`);\n        // Create a placeholder agent if spec is not found\n        const placeholderAgent = new Agent(\n          teamSpec.name,\n          teamSpec.role || 'Unknown Role',\n          teamSpec.goal || 'No specific goal defined',\n          this.defaultModel,\n          '' // No prompt for placeholder\n        );\n        team[teamSpec.name] = placeholderAgent;\n      }\n    }\n    return team;\n  }\n\n  // Helper to load prompts, ensuring it can be called recursively\n  private loadPrompts(promptsDir: string): Record<string, string> {\n    return this._loadPrompts(promptsDir);\n  }\n\n  private _loadAgentSpecs(agentsDir: string): AgentSpec[] {\n    logger.info(`Loading agent specifications from: ${agentsDir}`);\n    const agentFiles = globSync(join(agentsDir, '*.yaml')); // Use globSync for simplicity here\n    const specs: AgentSpec[] = [];\n    agentFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        const spec = yaml.load(content) as AgentSpec;\n        // Resolve prompt path relative to the agent file's directory\n        if (spec.systemPromptPath) {\n          const baseDir = dirname(filePath);\n          spec.systemPrompt = readFileSync(join(baseDir, spec.systemPromptPath), 'utf-8');\n        }\n        spec.model = spec.model || this.defaultModel;\n        specs.push(spec);\n        logger.debug(`Loaded agent spec: ${spec.name}`);\n      } catch (error) {\n        logger.error(`Error loading agent spec from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${specs.length} agent specifications loaded.`);\n    return specs;\n  }\n\n  private _loadPrompts(promptsDir: string): Record<string, string> {\n    logger.info(`Loading prompts from: ${promptsDir}`);\n    const promptFiles = globSync(join(promptsDir, '*.txt'));\n    const prompts: Record<string, string> = {};\n    promptFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        const fileName = basename(filePath);\n        prompts[fileName] = content;\n        logger.debug(`Loaded prompt: ${fileName}`);\n      } catch (error) {\n        logger.error(`Error loading prompt from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${Object.keys(prompts).length} prompts loaded.`);\n    return prompts;\n  }\n}\n\n// Mock implementations for dependencies used in System class for demonstration purposes\n// In a real project, these would be imported from their respective modules.\n\n// Mock AgentSpec and PromptSpec interfaces (should be defined in Agent.ts)\ninterface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string;\n  team?: PromptSpec[];\n}\n\ninterface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\n// Mock necessary imports from 'fs' and 'path' if not using a full Node.js environment setup\n// For example, using 'glob' and 'js-yaml' would require their respective imports and installations.\n\n// Placeholder for glob and yaml imports - in a real project, install and import them:\n// import { globSync } from 'glob';\n// import * as yaml from 'js-yaml';\n// import { readFileSync } from 'fs';\n// import { join, basename, dirname } from 'path';\n\n// Dummy implementations for globSync, yaml.load, readFileSync, join, basename, dirname\nconst globSync = (pattern: string): string[] => {\n  logger.warn('Using dummy globSync implementation. Install "},{"path":"src/agent/Agent.ts","content":"import { LLM } from '../llm/LLM';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, Artifact, AgentOutput, File, Team } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string;\n  team?: PromptSpec[];\n}\n\nexport interface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  protected systemPrompt: string;\n  protected llm: LLM;\n  protected model: string;\n  public team: Record<string, Agent> = {}; // For Orchestrator's delegation\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    this.llm = LLM.factory(model);\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Model: ${this.model})`);\n  }\n\n  updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    const currentTask = context.currentStep();\n    const previousArtifactsContent = await this._getPreviousArtifacts(context);\n\n    const taskPrompt = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${currentTask.description}'`,\n      `The team's roles are:\n${JSON.stringify(context.plan.roles.map((role: Role) => ({ title: role.title, purpose: role.purpose })), null, 2)}`,\n    ].join('\\n\\n');\n\n    let fullPrompt = taskPrompt;\n    if (previousArtifactsContent) {\n      fullPrompt += `\\n\\nPlease use the following outputs from other agents as your input:\\n\\n${previousArtifactsContent}\\n\\n`;\n    }\n    fullPrompt += `Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project. Respond in a JSON format that includes 'output', and optionally 'artifact' and 'team' fields.`\n\n    logger.debug(`Constructed prompt for ${this.name} for task ${currentTask.id}:\n${fullPrompt}`);\n\n    // Record the prompt used for this task\n    context.recordArtifact(`${this.name}_prompt_${currentTask.id}.txt`, fullPrompt);\n\n    try {\n      const response = await this.llm.generateContent({\n        modelName: this.model,\n        contents: fullPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: AgentOutput,\n        responseMimeType: 'application/json',\n      });\n\n      if (!response) {\n        logger.error(`LLM returned no response for task ${currentTask.id} by ${this.name}.`);\n        return null;\n      }\n\n      // Ensure the response is parsed as AgentOutput\n      if (typeof response === 'string') {\n          try {\n              const parsedOutput = AgentOutput.parse(JSON.parse(response));\n              logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n              this._processAndRecordArtifacts(context, parsedOutput);\n              return parsedOutput;\n          } catch (parseError) {\n              logger.error(`Failed to parse LLM response as AgentOutput for task ${currentTask.id} by ${this.name}:`, parseError);\n              logger.error(`Raw LLM response: ${response}`);\n              // Attempt to record raw output if it's not a valid JSON\n              context.recordArtifact(`${this.name}_raw_output_${currentTask.id}.txt`, response);\n              return new AgentOutput({ output: `Error processing response: ${response}`, reasoning: 'LLM response was not valid JSON or AgentOutput schema.' });\n          }\n      } else if (response instanceof AgentOutput) {\n          logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n          this._processAndRecordArtifacts(context, response);\n          return response;\n      }\n\n      // Fallback for unexpected response types\n      logger.warn(`Unexpected response type from LLM for task ${currentTask.id} by ${this.name}: ${typeof response}`);\n      return new AgentOutput({ output: `Unexpected response type: ${JSON.stringify(response)}`, reasoning: 'LLM returned an unexpected data type.' });\n\n    } catch (error) {\n      logger.error(`Error during LLM interaction for task ${currentTask.id} by ${this.name}:`, error);\n      // Record the error as an artifact\n      context.recordArtifact(`${this.name}_error_${currentTask.id}.txt`, `Error: ${error.message}`);\n      return new AgentOutput({ output: `Error executing task: ${error.message}`, reasoning: 'LLM interaction failed.' });\n    }\n  }\n\n  private async _getPreviousArtifacts(context: ExecutionContext): Promise<string> {\n    const requiredTaskIds = ['initial', ...context.currentStep().requires];\n    const relevantArtifacts: string[] = [];\n\n    // Iterate through all recorded items in the context\n    for (const [key, contextItem] of Object.entries(context.items)) {\n      if (requiredTaskIds.includes(contextItem.step.id)) {\n        let artifactContent = '';\n        if (typeof contextItem.content === 'string') {\n          artifactContent = contextItem.content;\n        } else if (contextItem.content instanceof Artifact) {\n          artifactContent = contextItem.content.files.map(file => `--- File: ${file.path} ---\\n${file.content}`).join('\\n');\n        } else {\n          try {\n            artifactContent = JSON.stringify(contextItem.content, null, 2);\n          } catch (e) {\n            artifactContent = String(contextItem.content); // Fallback to string conversion\n          }\n        }\n        relevantArtifacts.push(`--- Artifact '${key}' from (${contextItem.step.role}) in [${contextItem.step.id}]:\\n${artifactContent}`);\n      }\n    }\n\n    return relevantArtifacts.join('\\n\\n');\n  }\n\n  private _processAndRecordArtifacts(context: ExecutionContext, agentOutput: AgentOutput): void {\n    // Record the main output\n    context.recordArtifact(`${this.name}_output.txt`, agentOutput.output, true);\n\n    // Record artifacts if present\n    if (agentOutput.artifact && agentOutput.artifact.files) {\n      agentOutput.artifact.files.forEach(file => {\n        context.session?.addArtifact(file.path, file.content);\n        console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n        // Record file artifact in context if it's meant to be remembered\n        // For simplicity, we'll assume all files in artifact are relevant for context\n        context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); // Record the whole artifact object for context\n      });\n    }\n\n    // Record reasoning if present\n    if (agentOutput.reasoning) {\n      context.recordArtifact(`${this.name}_reasoning.txt`, agentOutput.reasoning, true);\n    }\n\n    // Handle team updates (e.g., new prompts)\n    if (agentOutput.team && agentOutput.team.prompts) {\n      logger.info(`Agent ${this.name} provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n      // The System class will handle applying these updates to the agents\n    }\n  }\n}\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Role, AgentOutput } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  // Orchestrator might have a more complex team structure or delegation logic\n  // For now, we'll keep it simple and assume 'team' is managed by the System class\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name}`);\n  }\n\n  async generatePlan(session: Session, highLevelGoal: string, initialFiles: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator '${this.name}' is generating a plan for: \"${highLevelGoal}\"`);\n\n    if (!this.llm) {\n      logger.error('LLM client not initialized for Orchestrator.');\n      return null;\n    }\n\n    // Get available agents and their details for the prompt\n    const availableAgents = session.agents.map(agent => ({\n      name: agent.name,\n      role: agent.role,\n      goal: agent.goal,\n    }));\n\n    const agentDescription = availableAgents.map(agent => \n      `- Name: '${agent.name}'\\n  Role: \n${agent.role}\n  Goal: \"${agent.goal}\"`\n    ).join('\\n');\n\n    // Load the planning prompt template\n    // In a real scenario, this path would be relative to the project root or a config dir\n    const promptTemplatePath = join(__dirname, '..', '..', 'prompts', 'general_planning.txt');\n    let promptTemplate: string;\n    try {\n      promptTemplate = readFileSync(promptTemplatePath, 'utf-8');\n    } catch (error) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}:`, error);\n      return null;\n    }\n\n    const planningPrompt = promptTemplate.format({\n      highLevelGoal: highLevelGoal,\n      teamDescription: agentDescription,\n      initialFiles: initialFiles.map(f => `--- File: ${f.path}\\n```\\n${f.content}\\n````).join('\\n\\n')\n    });\n\n    logger.debug(`Constructed planning prompt for Orchestrator:\n${planningPrompt}`);\n    session.addArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      const response = await this.llm.generateContent({\n        modelName: this.model,\n        contents: planningPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: Plan,\n        responseMimeType: 'application/json',\n        temperature: 0.1, // Lower temperature for more deterministic planning\n      });\n\n      if (!response) {\n        logger.error('LLM returned no response for plan generation.');\n        return null;\n      }\n\n      // Ensure the response is parsed as Plan\n      if (typeof response === 'string') {\n        try {\n          const plan = Plan.parse(JSON.parse(response));\n          return plan;\n        } catch (parseError) {\n          logger.error('Failed to parse LLM response as Plan:', parseError);\n          logger.error(`Raw LLM response: ${response}`);\n          session.addArtifact('plan_generation_error.txt', `LLM response was not valid JSON or Plan schema: ${response}`);\n          return null;\n        }\n      } else if (response instanceof Plan) {\n        return response;\n      }\n\n      logger.error(`Unexpected response type from LLM during plan generation: ${typeof response}`);\n      return null;\n\n    } catch (error) {\n      logger.error('Error during plan generation:', error);\n      session.addArtifact('plan_generation_error.txt', `Error: ${error.message}`);\n      return null;\n    }\n  }\n\n  // Override executeTask to potentially handle plan execution logic if needed, or delegate\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    // For the Orchestrator, executeTask might be used to refine the plan or handle meta-tasks.\n    // In this architecture, the System class drives the task execution based on the plan.\n    // If the Orchestrator is assigned a specific task in the plan, it needs to handle it.\n    logger.warn(`Orchestrator '${this.name}' executeTask called directly. This is usually handled by the System. Task: '${context.currentStep().description}'`);\n    \n    // Default behavior: delegate to the base Agent class to process the task prompt\n    // This might be useful if the orchestrator needs to perform a specific task defined in the plan\n    // e.g., \"Refine the plan based on new information\".\n    return super.executeTask(context);\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Artifact, File } from '../custom_types';\nimport { join, dirname as getDirname } from 'path';\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'fs';\nimport * as yaml from 'js-yaml';\n\nconst logger = Logger.getInstance();\n\nexport class Session {\n  public readonly sessionId: string;\n  public readonly agents: Agent[];\n  public state: string;\n  public readonly sessionDir: string;\n  public readonly projectRoot: string;\n\n  constructor(agents: Agent[], projectRoot: string, sessionId?: string) {\n    this.sessionId = sessionId || `session_${Date.now().toString()}`;\n    this.agents = agents;\n    this.state = 'initialized';\n    this.projectRoot = projectRoot;\n    this.sessionDir = join(this.projectRoot, 'sessions', this.sessionId);\n\n    if (!existsSync(this.sessionDir)) {\n      mkdirSync(this.sessionDir, { recursive: true });\n    }\n    logger.info(`Session created: ${this.sessionId} (Directory: ${this.sessionDir})`);\n  }\n\n  addArtifact(name: string, content: any): void {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      const dir = getDirname(artifactPath);\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n      // Determine if content should be saved as JSON or plain text\n      let contentToWrite: string;\n      if (typeof content === 'object' && content !== null && !(content instanceof File) && !(content instanceof Artifact)) {\n        // If it's a plain object (like from AgentOutput.toJSON()), save as JSON\n        contentToWrite = JSON.stringify(content, null, 2);\n      } else if (typeof content === 'string') {\n        contentToWrite = content;\n      } else {\n        // Fallback for other types, convert to string\n        contentToWrite = String(content);\n      }\n\n      writeFileSync(artifactPath, contentToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (e) {\n      logger.error(`Error saving artifact '${name}' in session ${this.sessionId}:`, e);\n    }\n  }\n\n  getArtifact(name: string): any {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      if (existsSync(artifactPath)) {\n        const content = readFileSync(artifactPath, 'utf-8');\n        if (name.endsWith('.json')) {\n          try {\n            return JSON.parse(content);\n          } catch (e) {\n            logger.error(`Error parsing JSON artifact '${name}':`, e);\n            return null;\n          }\n        }\n        return content;\n      } else {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n        return null;\n      }\n    } catch (e) {\n      logger.error(`Error retrieving artifact '${name}' from session ${this.sessionId}:`, e);\n      return null;\n    }\n  }\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface ContextItem {\n  name: string;\n  content: any;\n  step: Task;\n}\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores artifacts remembered from previous steps/rounds\n\n  constructor(session: Session, plan: Plan) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = 0;\n    this.roundNum = 1;\n    this.items = {};\n    logger.info('ExecutionContext initialized.');\n  }\n\n  nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  resetStepIndex(): void {\n    this.stepIndex = 0;\n    logger.debug('Step index reset to 0.');\n  }\n\n  currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error('Step index out of bounds. No more tasks in the plan.');\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  // Records an artifact for the current step and optionally remembers it for future steps\n  recordArtifact(key: string, value: any, remember: boolean = false): void {\n    const artifactKey = `__${this.roundNum}/__step_${this.stepIndex}_${key}`;\n    this.session.addArtifact(artifactKey, value);\n    if (remember) {\n      this._rememberItem(artifactKey, value, this.currentStep());\n    }\n  }\n\n  // Records initial artifacts provided at the start of the workflow\n  recordInitial(key: string, value: any): void {\n    // Use a special step ID for initial items\n    const initialTask: Task = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: []\n    });\n    this._rememberItem(key, value, initialTask);\n    // Also add to session directly if it's a file artifact\n    if (value instanceof Artifact && value.files) {\n        value.files.forEach(file => this.session.addArtifact(file.path, file.content));\n    }\n    logger.debug(`Recorded initial item '${key}'.`);\n  }\n\n  // Helper to store items in memory (context.items)\n  private _rememberItem(name: string, content: any, step: Task): void {\n    this.items[name] = { name, content, step };\n    logger.debug(`Item '${name}' remembered in context.`);\n  }\n}\n"},{"path":"src/llm/LLM.ts","content":"import { BaseModel } from 'pydantic';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface for the LLM generation arguments\nexport interface LLMGenerateContentArgs {\n  modelName: string;\n  contents: string;\n  systemInstruction?: string;\n  temperature?: number;\n  responseMimeType?: 'text/plain' | 'application/json' | 'application/json-schema';\n  responseSchema?: typeof BaseModel;\n}\n\nexport abstract class LLM {\n  protected abstract generateContentImpl(args: LLMGenerateContentArgs): Promise<any>;\n\n  /**\n   * Abstract method to generate content using an LLM.\n   * Handles common logic like retries and response parsing.\n   */\n  public async generateContent(args: LLMGenerateContentArgs): Promise<any> {\n    const { modelName, responseSchema, responseMimeType } = args;\n    const maxRetries = 3;\n    let lastError: Error | null = null;\n\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        logger.info(`Calling LLM model '${modelName}' (Attempt ${attempt}/${maxRetries})...`);\n        const response = await this.generateContentImpl(args);\n\n        if (!response) {\n          throw new Error('LLM returned an empty response.');\n        }\n\n        // Parse response based on mime type and schema\n        if (responseSchema && responseMimeType === 'application/json') {\n          if (typeof response === 'string') {\n            try {\n              const parsedJson = JSON.parse(response);\n              // Validate against the provided Pydantic schema\n              const validatedData = responseSchema.parse(parsedJson);\n              logger.debug(`LLM response successfully parsed and validated against schema ${responseSchema.name}.`);\n              return validatedData;\n            } catch (e) {\n              lastError = new Error(`Failed to parse or validate LLM JSON response with schema ${responseSchema.name}: ${e.message}`);\n              logger.error(lastError.message);\n              // Do not retry on parsing errors, as the response itself is likely malformed\n              throw lastError;\n            }\n          } else if (response instanceof responseSchema) {\n            logger.debug(`LLM response is already an instance of ${responseSchema.name}.`);\n            return response; // Response is already the correct type\n          } else {\n             lastError = new Error(`LLM response was not a string or expected schema type for JSON request.`);\n             logger.error(lastError.message);\n             throw lastError;\n          }\n        } else if (responseSchema && responseMimeType === 'application/json-schema') {\n            // Handle JSON schema response type if needed\n             if (typeof response === 'string') {\n                try {\n                    // Assuming the schema is directly provided and needs parsing\n                    const parsedSchema = JSON.parse(response);\n                    // Further validation or processing might be needed here based on specific schema requirements\n                    logger.debug('LLM response parsed as JSON Schema.');\n                    return parsedSchema;\n                } catch (e) {\n                     lastError = new Error(`Failed to parse LLM JSON Schema response: ${e.message}`);\n                     logger.error(lastError.message);\n                     throw lastError;\n                }\n            }\n        }\n\n        // Default handling for text or un-schematized JSON\n        logger.debug('LLM response processed.');\n        return response;\n\n      } catch (error) {\n        lastError = error;\n        logger.error(`LLM generation failed (Attempt ${attempt}): ${error.message}`);\n        if (attempt < maxRetries) {\n          const delay = Math.pow(2, attempt) * 1000; // Exponential backoff\n          logger.info(`Waiting for ${delay / 1000} seconds before retrying...`);\n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n\n    logger.error(`LLM generation failed after ${maxRetries} attempts. Last error: ${lastError?.message}`);\n    return null; // Return null if all retries fail\n  }\n\n  /**\n   * Factory method to create an LLM instance based on the species string.\n   * Example species: 'gemini:gemini-pro', 'ollama:llama3', 'openai:gpt-4'.\n   */\n  static factory(species: string): LLM {\n    logger.info(`LLM Factory: Creating LLM instance for species '${species}'`);\n    // TODO: Implement actual factory logic based on species string\n    // This is a placeholder and needs to be replaced with concrete implementations\n    // For now, returning a dummy LLM or a default one.\n    if (species.startsWith('ollama:')) {\n        const modelName = species.split(':')[1];\n        // return new OllamaLLM(modelName);\n        logger.warn('OllamaLLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName); \n    }\n    if (species.startsWith('gemini:') || species === 'gemini-2.5-flash-lite') {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new GeminiLLM(modelName);\n        logger.warn('GeminiLLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName);\n    }\n     if (species.startsWith('openai:') || species.startsWith('gpt:') || species.startsWith('opi:')) {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new OpenAILLM(modelName);\n         logger.warn('OpenAILLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('mistral:')) {\n        const modelName = species.split(':')[1];\n        // return new MistralLLM(modelName);\n         logger.warn('MistralLLM not implemented. Returning dummy LLM.');\n        return new DummyLLM(modelName);\n    }\n    // Default or fallback\n    logger.warn(`Unknown LLM species '${species}'. Using DummyLLM as fallback.`);\n    return new DummyLLM(species);\n  }\n}\n\n// Dummy LLM implementation for placeholder purposes\nclass DummyLLM extends LLM {\n  constructor(private modelName: string) {\n    super();\n    logger.warn(`DummyLLM initialized with model: ${this.modelName}. This is a placeholder and will not perform actual LLM calls.`);\n  }\n\n  protected async generateContentImpl(args: LLMGenerateContentArgs): Promise<any> {\n    logger.warn(`DummyLLM: generateContentImpl called for model ${this.modelName}. Returning mock response.`);\n    // Simulate a response based on the schema if provided, otherwise return a generic string\n    if (args.responseSchema) {\n        try {\n            const schemaInstance = args.responseSchema.prototype;\n            const mockResponse = {}; // Create a mock object based on schema properties\n            logger.debug(`Generating mock response for schema: ${args.responseSchema.name}`);\n            // This is a very basic mock, a more sophisticated one would inspect the schema\n            if (args.responseSchema === AgentOutput) {\n                return new AgentOutput({\n                    output: `Mock output for ${this.modelName} task: ${args.contents.substring(0, 50)}...`,\n                    reasoning: 'This is a mock response from DummyLLM.',\n                    artifact: new Artifact({ task: 'mock_task', files: [new File({ path: 'mock_output.txt', content: 'Mock file content.'})]}),\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            } else if (args.responseSchema === Plan) {\n                 return new Plan({\n                    highLevelGoal: 'Mock Goal',\n                    reasoning: 'Mock reasoning for plan generation',\n                    roles: [{title: 'Mock Role', purpose: 'Mock Purpose'}],\n                    tasks: [{id: 'T1', description: 'Mock Task', role: 'Mock Role', agent: 'Mock Agent', requires: []}],\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            }\n            // Add more schema handlers as needed\n            return schemaInstance;\n        } catch (e) {\n            logger.error(`Error creating mock response for schema ${args.responseSchema.name}:`, e);\n            return `Mock response failed for schema ${args.responseSchema.name}.`;\n        }\n    }\n    \n    return `This is a mock response from DummyLLM for model ${this.modelName}.`;\n  }\n}\n"},{"path":"src/logger/Logger.ts","content":"import winston from 'winston';\nimport { join } from 'path';\nimport fs from 'fs';\n\n// Define log levels\nconst logLevels = {\n  error: 0,\n  warn: 1,\n  info: 2,\n  debug: 3,\n};\n\n// Define log format\nconst logFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.printf(({ level, message, timestamp, ...metadata }) => {\n    let msg = `${timestamp} [${level.toUpperCase()}]: ${message}`;\n    if (Object.keys(metadata).length > 0) {\n      msg += ` ${JSON.stringify(metadata)}`;\n    }\n    return msg;\n  })\n);\n\nconst jsonLogFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.json()\n);\n\n// Ensure log directory exists\nconst logDir = process.env.LOG_DIR || 'logs';\nif (!fs.existsSync(logDir)) {\n  fs.mkdirSync(logDir, { recursive: true });\n}\n\nconst logFilePath = join(logDir, 'app.log');\nconst jsonLogFilePath = join(logDir, 'app.jsonl');\n\nexport class Logger {\n  private static instance: winston.Logger;\n\n  public static getInstance(): winston.Logger {\n    if (!Logger.instance) {\n      Logger.instance = winston.createLogger({\n        level: process.env.LOG_LEVEL || 'info',\n        levels: logLevels,\n        format: logFormat, // Default format for console\n        transports: [\n          // Console transport with colorization\n          new winston.transports.Console({\n            format: winston.format.combine(\n              winston.format.colorize({ all: true }),\n              logFormat\n            ),\n          }),\n          // File transport for general logs\n          new winston.transports.File({\n            filename: logFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: logFormat,\n            maxsize: 1024 * 1024 * 5, // 5MB\n            tailable: true,\n          }),\n          // File transport for JSON logs (for easier parsing by other tools)\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: jsonLogFormat,\n            maxsize: 1024 * 1024 * 5, // 5MB\n            tailable: true,\n          }),\n        ],\n      });\n      // Add a handler for uncaught exceptions\n      Logger.instance.on('error', (err) => {\n        console.error('Winston logger encountered an error:', err);\n      });\n    }\n    return Logger.instance;\n  }\n}\n"},{"path":"src/config/Config.ts","content":"import { readFileSync } from 'fs';\nimport { join } from 'path';\nimport yaml from 'js-yaml';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface Config {\n  projectRoot?: string;\n  agentDir?: string;\n  promptDir?: string;\n  logDir?: string;\n  defaultModel?: string;\n  workflowRounds?: number;\n  // Add other configuration properties as needed\n}\n\nexport class Config {\n  public static async load(fileName: string, basePath: string = process.cwd()): Promise<Config> {\n    const configPath = join(basePath, fileName);\n    logger.info(`Attempting to load configuration from: ${configPath}`);\n    try {\n      const fileContents = readFileSync(configPath, 'utf-8');\n      const config = yaml.load(fileContents) as Config;\n      // Set default values if not provided\n      config.projectRoot = config.projectRoot || basePath;\n      config.agentDir = config.agentDir || 'agents';\n      config.promptDir = config.promptDir || 'prompts';\n      config.logDir = config.logDir || 'logs';\n      config.defaultModel = config.defaultModel || 'gemini-2.5-flash-lite';\n      config.workflowRounds = config.workflowRounds || 3;\n\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Failed to load configuration from ${configPath}:`, error);\n      // Return a default config if file loading fails, or throw an error\n      // For now, returning a minimal default config\n      return {\n        projectRoot: basePath,\n        agentDir: 'agents',\n        promptDir: 'prompts',\n        logDir: 'logs',\n        defaultModel: 'gemini-2.5-flash-lite',\n        workflowRounds: 3,\n      };\n      // Or uncomment below to throw an error and stop execution:\n      // throw new Error(`Failed to load configuration file: ${configPath}`);\n    }\n  }\n}\n"},{"path":"src/custom_types.ts","content":"import { BaseModel, Field, model_validator } from 'pydantic';\nimport { z } from 'zod'; // Using Zod for schema validation demonstration\n\n// --- Task Agnostic Step (TAS) related Types ---\n\nexport class Task extends BaseModel {\n  id: string = Field(..., { description: \"Unique ID for the task (e.g., 'T1', 'T2').\" });\n  description: string = Field(..., { description: \"Detailed description of what to do for this task.\" });\n  role: string = Field(..., { description: \"The role responsible for this task.\" });\n  agent: string = Field(..., { description: \"The agent assigned to this task.\" });\n  requires: string[] = Field([], { description: \"List of tasks (IDs) whose outputs are required (e.g. ['T1', 'T2']).\" });\n}\n\nexport class File extends BaseModel {\n  path: string = Field(..., { description: \"File path or name (e.g., 'src/main.py').\" });\n  content: string = Field(..., { description: \"Full content of the file.\" });\n}\n\nexport class Artifact extends BaseModel {\n  task: string = Field(..., { description: \"ID of the task that produced this artifact (e.g., 'T1').\" });\n  files: File[] = Field([], { description: \"List of files created or modified by the task.\" });\n}\n\nexport class Role extends BaseModel {\n  title: string = Field(..., { description: \"Title of the role (e.g., 'Engineer', 'Reviewer').\" });\n  purpose: string = Field(..., { description: \"The main purpose and goal of this role.\" });\n}\n\nexport class Prompt extends BaseModel {\n  agent: string = Field(..., { description: \"Name of the agent this prompt is for. Must match exactly an existing agent.\" });\n  role: string = Field(..., { description: \"Role context of the agent.\" });\n  systemPrompt: string = Field(..., { description: \"Full text of the system prompt for this agent.\" });\n}\n\nexport class Team extends BaseModel {\n  notes: string = Field('', { description: \"General notes or feedback about the team's performance or the plan.\" });\n  prompts: Prompt[] = Field([], { description: \"A list of new or updated system prompts for any agent in the team.\" });\n}\n\nexport class Plan extends BaseModel {\n  highLevelGoal: string = Field(..., { description: \"The main goal this plan is designed to achieve.\" });\n  reasoning: string = Field(..., { description: \"A brief explanation of the plan's structure and strategy.\" });\n  roles: Role[] = Field([], { description: \"List of all roles required to execute the plan.\" });\n  tasks: Task[] = Field([], { description: \"A step-by-step sequence of tasks to be executed in order.\" });\n  team: Team | null = Field(null, { description: \"Updates to team configuration or system prompts.\" });\n}\n\nexport class AgentOutput extends BaseModel {\n  output: string = Field(..., { description: \"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\" });\n  artifact: Artifact | null = Field(null, { description: \"Any files created or modified by the agent during the task.\" });\n  team: Team | null = Field(null, { description: \"Updates to team configuration or system prompts.\" });\n  reasoning: string | null = Field(null, { description: \"Explanation of how the agent arrived at this output.\" });\n\n  // Example of adding a custom validator if needed\n  @model_validator(mode='after')\n  def check_output_and_artifact(self) -> 'AgentOutput':\n    if not self.output and not self.artifact:\n      raise ValueError('Agent output must contain either an \"output\" message or an \"artifact\".');\n    return self;\n}\n\n// --- Example Usage & Schema Generation ---\nif (require.main === module) {\n  console.log('--- AgentOutput Schema ---\\n', JSON.stringify(AgentOutput.model_json_schema(), null, 2));\n  console.log('\\n--- Plan Schema ---\\n', JSON.stringify(Plan.model_json_schema(), null, 2));\n}\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_3_Lyra_result.txt' from (Lyra) in [T4]:
{"output":"Project structure, dependencies, and build tools (tsc, esbuild) are set up for the TypeScript Node.js runtime. This includes creating essential directories and configuration files like `package.json`, `tsconfig.json`, and a basic `.eslintrc.js`.","artifact":{"task":"T4","files":[{"path":"package.json","content":"{\n  \"name\": \"typescript-runtime\",\n  \"version\": \"1.0.0\",\n  \"description\": \"A Node.js runtime reimplementation using TypeScript, following SOTA principles.\",\n  \"main\": \"dist/index.js\",\n  \"types\": \"dist/index.d.ts\",\n  \"scripts\": {\n    \"build\": \"tsc && node esbuild.js\",\n    \"dev\": \"tsc -w & node esbuild.js --watch\",\n    \"start\": \"node dist/index.js\",\n    \"lint\": \"eslint . --ext .ts\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"runtime\",\n    \"typescript\",\n    \"node\",\n    \"agent\",\n    \"llm\"\n  ],\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@types/node\": \"^20.11.19\",\n    \"chalk\": \"^5.3.0\",\n    \"chokidar\": \"^3.5.3\",\n    \"dotenv\": \"^16.4.5\",\n    \"esbuild\": \"^0.20.2\",\n    \"glob\": \"^10.3.10\",\n    \"js-yaml\": \"^4.1.0\",\n    \"pydantic\": \"^2.0.0\",\n    \"winston\": \"^3.11.0\",\n    \"zod\": \"^3.22.7\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^8.57.0\",\n    \"eslint-config-prettier\": \"^9.1.0\",\n    \"eslint-plugin-prettier\": \"^5.1.3\",\n    \"prettier\": \"^3.2.5\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"^5.3.3\"\n  }\n}\n"},{"path":"tsconfig.json","content":"{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"CommonJS\",\n    \"lib\": [\"ES2020\", \"DOM\"],\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"declaration\": true,\n    \"sourceMap\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"**/*.spec.ts\"]\n}\n"},{"path":"esbuild.js","content":"const esbuild = require('esbuild');\nconst chokidar = require('chokidar');\nconst { join } = require('path');\n\nconst watch = process.argv.includes('--watch');\n\nconst buildOptions = {\n  entryPoints: ['src/index.ts'],\n  bundle: true,\n  outfile: 'dist/index.js',\n  platform: 'node',\n  format: 'cjs',\n  sourcemap: true,\n  target: 'node20',\n  external: ['@types/node', 'fs', 'path', 'os', 'child_process', 'http', 'https', 'crypto'], // Externalize Node.js built-ins and types\n};\n\nasync function build() {\n  try {\n    await esbuild.build(buildOptions);\n    console.log('Build successful!');\n  } catch (err) {\n    console.error('Build failed:', err);\n  }\n}\n\nasync function run() {\n  if (watch) {\n    console.log('Watching for changes...');\n    await build(); // Initial build\n\n    const watcher = chokidar.watch('src/**/*.ts', {\n      ignored: /node_modules/, // Ignore node_modules\n      persistent: true,\n    });\n\n    watcher.on('change', async (filePath) => {\n      console.log(`File changed: ${filePath}`);\n      await build();\n    });\n\n    watcher.on('add', async (filePath) => {\n        console.log(`File added: ${filePath}`);\n        await build();\n    });\n\n    watcher.on('unlink', async (filePath) => {\n        console.log(`File removed: ${filePath}`);\n        await build();\n    });\n\n    // Keep the process running\n    process.stdin.resume();\n\n  } else {\n    await build();\n    process.exit(0); // Exit after build if not watching\n  }\n}\n\nrun().catch(err => {\n  console.error('Error during esbuild process:', err);\n  process.exit(1);\n});\n"},{"path":".eslintrc.js","content":"module.exports = {\n  root: true,\n  parser: '@typescript-eslint/parser',\n  plugins: [\n    '@typescript-eslint',\n    'prettier'\n  ],\n  extends: [\n    'eslint:recommended',\n    'plugin:@typescript-eslint/recommended',\n    'plugin:prettier/recommended'\n  ],\n  parserOptions: {\n    ecmaVersion: 2020,\n    sourceType: 'module',\n    project: './tsconfig.json', // Specify the tsconfig.json file\n  },\n  rules: {\n    // Override rules here if needed\n    '@typescript-eslint/no-explicit-any': 'warn', // Example: Warn about 'any' type\n    '@typescript-eslint/no-unused-vars': 'warn', // Warn about unused variables\n    'prettier/prettier': ['error', {\n      endOfLine: 'lf',\n    }],\n    // Add other rules as per project standards\n  },\n  ignorePatterns: ['node_modules/', 'dist/', '*.d.ts'],\n};\n"},{"path":".prettierrc.js","content":"module.exports = {\n  semi: true,\n  trailingComma: 'es5',\n  singleQuote: true,\n  printWidth: 100,\n  tabWidth: 2,\n  endOfLine: 'lf',\n};\n"},{"path":"src/index.ts","content":"import { System } from './system/System';\nimport { Agent } from './agent/Agent';\nimport { Orchestrator } from './orchestrator/Orchestrator';\nimport { Session } from './core/Session';\nimport { ExecutionContext } from './core/ExecutionContext';\nimport { LLM } from './llm/LLM';\nimport { Logger } from './logger/Logger';\nimport { Config } from './config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from './custom_types';\n\n// Re-export core components and types for external use\nexport {\n  System,\n  Agent,\n  Orchestrator,\n  Session,\n  ExecutionContext,\n  LLM,\n  Logger,\n  Config,\n  Plan,\n  Task,\n  Artifact,\n  Role,\n  Prompt,\n  Team,\n  AgentOutput,\n  File\n};\n\n// Export default logger instance\nexport const logger = Logger.getInstance();\n\n// Example usage (can be moved to a CLI entry point like sysmain.ts later)\nasync function main() {\n  logger.info('Starting runtime initialization...');\n\n  try {\n    // 1. Load Configuration\n    // Assuming runtime.yaml is in the project root or a config directory\n    const config = await Config.load('runtime.yaml'); \n    logger.info('Configuration loaded successfully.');\n\n    // 2. Initialize System\n    // Use projectRoot from config if available, otherwise default to current directory\n    const system = new System(config.projectRoot || '.', config.defaultModel || 'gemini-2.5-flash-lite');\n    await system.setup(); // Load agents, prompts, etc.\n    logger.info('System setup complete.');\n\n    // 3. Define a high-level goal and initial files (example)\n    // This goal is aligned with the project's objective\n    const highLevelGoal = 'Reimplement runtime using TypeScript in Node.js following SOTA coding principles';\n    const initialFiles: File[] = [\n      // Example: file content could be loaded from actual files during runtime execution\n      // new File({ path: 'docs/runtime.md', content: '...' })\n    ];\n\n    // 4. Start the workflow to generate a plan\n    const plan = await system.start(highLevelGoal, initialFiles);\n    logger.info('Workflow plan generated.');\n    console.log('\\n--- Generated Plan ---');\n    console.log(JSON.stringify(plan.toJSON(), null, 2)); // Use toJSON() for Pydantic models\n    console.log('----------------------\\n');\n\n    // 5. Run the workflow for a specified number of rounds\n    const rounds = config.workflowRounds || 3; // Use rounds from config or default to 3\n    await system.run(plan, rounds, initialFiles);\n    logger.info('Workflow execution finished.');\n\n  } catch (error) {\n    logger.error('Runtime initialization failed:', error);\n  }\n}\n\n// Uncomment the following line to run the example main function when executing this file directly\n// main();\n"},{"path":"src/system/System.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { LLM } from '../llm/LLM';\nimport { Logger } from '../logger/Logger';\nimport { Config } from '../config/Config';\nimport { Plan, Task, Artifact, Role, Prompt, Team, AgentOutput, File } from '../custom_types';\nimport { readFileSync, existsSync, mkdirSync } from 'fs';\nimport { join, dirname, basename } from 'path';\nimport yaml from 'js-yaml';\nimport { globSync } from 'glob';\n\nconst logger = Logger.getInstance();\n\nexport class System {\n  private config: Config;\n  private agents: Agent[] = [];\n  private session: Session | null = null;\n  private orchestrator: Orchestrator | null = null;\n  private readonly rootDir: string;\n  private readonly defaultModel: string;\n\n  constructor(rootDir: string, defaultModel: string = 'gemini-2.5-flash-lite') {\n    this.rootDir = rootDir;\n    this.defaultModel = defaultModel;\n    logger.info(`System initialized with root directory: ${this.rootDir} and default model: ${this.defaultModel}`);\n  }\n\n  async setup(): Promise<void> {\n    logger.info('--- System Setup ---');\n    this.config = await Config.load('runtime.yaml', this.rootDir);\n    logger.info('Configuration loaded.');\n\n    const agentSpecs = this._loadAgentSpecs(join(this.rootDir, this.config.agentDir || 'agents'));\n    const prompts = this._loadPrompts(join(this.rootDir, this.config.promptDir || 'prompts'));\n\n    const allTeamMembers = new Set<string>();\n    agentSpecs.forEach(spec => {\n      spec.team?.forEach(member => allTeamMembers.add(member.name.toLowerCase()));\n    });\n\n    // Instantiate agents, excluding those already defined as part of another agent's team\n    for (const spec of agentSpecs) {\n      if (!allTeamMembers.has(spec.name.toLowerCase())) {\n        const agent = this._instantiateAgent(spec, prompts, agentSpecs);\n        if (agent) {\n          this.agents.push(agent);\n        }\n      }\n    }\n\n    if (this.agents.length === 0) {\n      throw new Error('No agents could be instantiated. System setup failed.');\n    }\n\n    // Find and set the orchestrator\n    const orchestratorAgentInstance = this.agents.find(\n      agent => agent.role.toLowerCase() === 'orchestrator'\n    );\n    if (!orchestratorAgentInstance) {\n      throw new Error('Orchestrator agent not found. System setup failed.');\n    }\n    // Ensure it's an instance of Orchestrator\n    if (!(orchestratorAgentInstance instanceof Orchestrator)) {\n       throw new Error(`Agent '${orchestratorAgentInstance.name}' is not a valid Orchestrator instance. System setup failed.`);\n    }\n    this.orchestrator = orchestratorAgentInstance;\n    logger.info(`Orchestrator set to: ${this.orchestrator.name}`);\n\n    // Initialize session\n    this.session = new Session(this.agents, this.rootDir);\n    logger.info('Session initialized.');\n\n    logger.info('--- System Setup Complete ---');\n  }\n\n  async start(highLevelGoal: string, initialFiles: File[] = []): Promise<Plan> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call setup() before start().');\n    }\n\n    logger.info(`Generating plan for goal: \"${highLevelGoal}\"`);\n    // The orchestrator's generatePlan method is called here\n    const plan = await this.orchestrator.generatePlan(this.session, highLevelGoal, initialFiles);\n\n    if (!plan) {\n      throw new Error('Orchestration failed: Could not generate a valid plan.');\n    }\n\n    logger.info('Plan generated successfully.');\n    this.session.addArtifact('initial_plan.json', JSON.stringify(plan.toJSON(), null, 2));\n    logger.debug(`Plan details: ${JSON.stringify(plan.toJSON())}`);\n\n    return plan;\n  }\n\n  async run(plan: Plan, rounds: number = 1, initialFiles: File[] = []): Promise<void> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call start() before run().');\n    }\n\n    logger.info(`--- Starting Workflow (Rounds: ${rounds}) ---`);\n\n    const context = new ExecutionContext(this.session, plan);\n    // Record initial files as an artifact in the context\n    context.recordInitial('files', new Artifact({ task: 'initial', files: initialFiles }));\n\n    for (let roundNum = 1; roundNum <= rounds; roundNum++) {\n      context.roundNum = roundNum;\n      context.resetStepIndex();\n      logger.info(`Starting workflow round ${roundNum} for goal: \"${plan.highLevelGoal}\"`);\n\n      while (context.stepIndex < plan.tasks.length) {\n        const currentTask = context.currentStep();\n        // Find the agent responsible for the current task\n        let delegateAgent = this.agents.find(agent => agent.name.toLowerCase() === currentTask.agent.toLowerCase());\n\n        if (!delegateAgent) {\n          logger.warn(`Agent '${currentTask.agent}' not found for task '${currentTask.description}'. Falling back to Orchestrator.`);\n          delegateAgent = this.orchestrator;\n        }\n\n        logger.info(`Executing step ${context.stepIndex + 1}/${plan.tasks.length}: '${currentTask.description}' by Agent '${delegateAgent.name}' (Role: ${currentTask.role})`);\n\n        try {\n          // Execute the task using the determined agent\n          const agentOutput = await delegateAgent.executeTask(context);\n          if (agentOutput) {\n            // Record the main output of the agent\n            context.recordArtifact(`${delegateAgent.name}_output_${currentTask.id}.txt`, agentOutput.output, true);\n            \n            // Process and save any artifacts produced by the agent\n            if (agentOutput.artifact && agentOutput.artifact.files) {\n              agentOutput.artifact.files.forEach(file => {\n                this.session?.addArtifact(file.path, file.content);\n                console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n                // Optionally record the file artifact in the context for future steps\n                context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); \n              });\n            }\n            // Handle team updates (e.g., new prompts) provided by the agent\n            if (agentOutput.team?.prompts) {\n              logger.info(`Agent '${delegateAgent.name}' provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n              for (const promptData of agentOutput.team.prompts) {\n                this._updateAgentPrompt(promptData.agent, promptData.systemPrompt);\n              }\n            }\n          }\n        } catch (error) {\n          logger.error(`Error executing task '${currentTask.description}' by agent '${delegateAgent.name}':`, error);\n          // Error handling strategy: log and continue to the next task for now\n          context.recordArtifact(`${delegateAgent.name}_error_${currentTask.id}.txt`, `Error: ${error.message}`);\n        }\n        context.nextStep(); // Move to the next task in the plan\n      }\n      logger.info(`Completed workflow round ${roundNum}.`);\n    }\n\n    logger.info('--- Workflow Complete ---');\n  }\n\n  // Updates the system prompt for a specific agent\n  private _updateAgentPrompt(agentName: string, newPrompt: string): void {\n    const targetAgent = this.agents.find(agent => agent.name.toLowerCase() === agentName.toLowerCase());\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Updated system prompt for agent '${targetAgent.name}'.`);\n    } else {\n      logger.warn(`Agent '${agentName}' not found for prompt update.`);\n    }\n  }\n\n  // Instantiates an Agent or a specialized subclass like Orchestrator\n  private _instantiateAgent(spec: AgentSpec, prompts: Record<string, string>, allAgentSpecs: AgentSpec[]): Agent | null {\n    const promptKey = `${spec.name.toLowerCase()}_instructions.txt`;\n    // Prioritize systemPrompt from spec, then from loaded prompts file, else empty\n    const systemPrompt = spec.systemPrompt || prompts[promptKey] || '';\n    const model = spec.model || this.defaultModel;\n\n    let agent: Agent;\n\n    if (spec.role.toLowerCase() === 'orchestrator') {\n      agent = new Orchestrator(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n      // If the orchestrator spec defines a team, build and assign it\n      if (spec.team && spec.team.length > 0) {\n        (agent as Orchestrator).team = this._buildTeam(spec.team, allAgentSpecs, prompts, 0);\n      }\n    } else {\n      agent = new Agent(\n        spec.name,\n        spec.role,\n        spec.goal,\n        model,\n        systemPrompt\n      );\n    }\n\n    logger.info(`Instantiated Agent: ${agent.name} (Role: ${agent.role}, Model: ${agent.model})`);\n    return agent;\n  }\n\n  // Recursively builds the team for an agent (primarily for Orchestrator)\n  private _buildTeam(teamSpecs: PromptSpec[], allAgentSpecs: AgentSpec[], prompts: Record<string, string>, depth: number): Record<string, Agent> {\n    const team: Record<string, Agent> = {};\n    const maxDepth = 5; // Prevent infinite recursion\n\n    if (depth > maxDepth) {\n      logger.error('Maximum team nesting depth reached. Stopping team creation.');\n      return {};\n    }\n\n    for (const teamSpec of teamSpecs) {\n      const memberSpec = allAgentSpecs.find(spec => spec.name.toLowerCase() === teamSpec.name.toLowerCase());\n      if (memberSpec) {\n        // Recursively instantiate the team member\n        const memberAgent = this._instantiateAgent(memberSpec, prompts, allAgentSpecs);\n        if (memberAgent) {\n          team[teamSpec.name] = memberAgent;\n        } else {\n          logger.warn(`Failed to instantiate team member '${teamSpec.name}'. Skipping.`);\n        }\n      } else {\n        logger.warn(`Team member specification for '${teamSpec.name}' not found. Creating a placeholder agent.`);\n        // Create a placeholder agent if spec is not found\n        const placeholderAgent = new Agent(\n          teamSpec.name,\n          teamSpec.role || 'Unknown Role',\n          teamSpec.goal || 'No specific goal defined',\n          this.defaultModel,\n          '' // No prompt for placeholder\n        );\n        team[teamSpec.name] = placeholderAgent;\n      }\n    }\n    return team;\n  }\n\n  // Loads agent specifications from YAML files\n  private _loadAgentSpecs(agentsDir: string): AgentSpec[] {\n    logger.info(`Loading agent specifications from: ${agentsDir}`);\n    if (!existsSync(agentsDir)) {\n        logger.warn(`Agent directory not found: ${agentsDir}. No agents will be loaded.`);\n        return [];\n    }\n    const agentFiles = globSync(join(agentsDir, '*.yaml'));\n    const specs: AgentSpec[] = [];\n    agentFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        // Use js-yaml for loading YAML content\n        const spec = yaml.load(content) as AgentSpec;\n        \n        // Resolve system prompt path relative to the agent file's directory\n        if (spec.systemPromptPath) {\n          const baseDir = dirname(filePath);\n          const promptPath = join(baseDir, spec.systemPromptPath);\n          if (existsSync(promptPath)) {\n            spec.systemPrompt = readFileSync(promptPath, 'utf-8');\n          } else {\n            logger.warn(`System prompt file not found for ${spec.name}: ${promptPath}`);\n            spec.systemPrompt = ''; // Ensure systemPrompt is defined\n          }\n        }\n        // Use default model if not specified in the agent spec\n        spec.model = spec.model || this.defaultModel;\n        specs.push(spec);\n        logger.debug(`Loaded agent spec: ${spec.name}`);\n      } catch (error) {\n        logger.error(`Error loading agent spec from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${specs.length} agent specifications loaded.`);\n    return specs;\n  }\n\n  // Loads prompt files from a directory\n  private _loadPrompts(promptsDir: string): Record<string, string> {\n    logger.info(`Loading prompts from: ${promptsDir}`);\n    if (!existsSync(promptsDir)) {\n        logger.warn(`Prompts directory not found: ${promptsDir}. No prompts will be loaded.`);\n        return {};\n    }\n    const promptFiles = globSync(join(promptsDir, '*.txt'));\n    const prompts: Record<string, string> = {};\n    promptFiles.forEach(filePath => {\n      try {\n        const content = readFileSync(filePath, 'utf-8');\n        const fileName = basename(filePath);\n        prompts[fileName] = content;\n        logger.debug(`Loaded prompt: ${fileName}`);\n      } catch (error) {\n        logger.error(`Error loading prompt from ${filePath}:`, error);\n      }\n    });\n    logger.info(`${Object.keys(prompts).length} prompts loaded.`);\n    return prompts;\n  }\n}\n"},{"path":"src/agent/Agent.ts","content":"import { LLM, LLMGenerateContentArgs } from '../llm/LLM';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, Artifact, AgentOutput, File, Team, Role } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\n// Interface for Agent configuration loaded from YAML\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string; // Path to the system prompt file relative to agent definition\n  team?: PromptSpec[]; // For delegation, specifies members of its team\n}\n\n// Interface for team member specification (used in AgentSpec.team)\nexport interface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  protected systemPrompt: string;\n  protected llm: LLM;\n  protected model: string;\n  // Team is primarily for Orchestrator, but can be used by other agents for delegation\n  public team: Record<string, Agent> = {}; \n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    this.llm = LLM.factory(model); // Use the factory to get an LLM instance\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Model: ${this.model})`);\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task assigned to this agent within the given context.\n   * @param context The execution context containing plan, session, and previous artifacts.\n   * @returns A Promise resolving to an AgentOutput object or null if execution fails.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    const currentTask = context.currentStep();\n    const previousArtifactsContent = await this._getPreviousArtifacts(context);\n\n    // Construct the prompt for the LLM\n    const taskPrompt = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${currentTask.description}'`,\n      // Include team roles for context\n      `The team's roles are:\n${JSON.stringify(context.plan.roles.map((role: Role) => ({ title: role.title, purpose: role.purpose })), null, 2)}`,\n    ].join('\\n\\n');\n\n    let fullPrompt = taskPrompt;\n    if (previousArtifactsContent) {\n      fullPrompt += `\\n\\nPlease use the following outputs from other agents as your input:\\n\\n${previousArtifactsContent}\\n\\n`;\n    }\n    // Add instruction for structured output\n    fullPrompt += `Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project. Respond in a JSON format that includes 'output', and optionally 'artifact' and 'team' fields.`\n\n    logger.debug(`Constructed prompt for ${this.name} for task ${currentTask.id}:\n${fullPrompt}`);\n\n    // Record the prompt used for this task for debugging and auditing\n    context.recordArtifact(`${this.name}_prompt_${currentTask.id}.txt`, fullPrompt);\n\n    try {\n      // Define arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: fullPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: AgentOutput, // Expecting AgentOutput schema\n        responseMimeType: 'application/json',\n        temperature: 0.7, // Default temperature, can be overridden\n      };\n\n      // Call the LLM to generate content\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error(`LLM returned no response for task ${currentTask.id} by ${this.name}.`);\n        return null;\n      }\n\n      // Ensure the response is parsed as AgentOutput\n      if (typeof response === 'string') {\n          try {\n              // Attempt to parse the string response as JSON and validate against AgentOutput schema\n              const parsedOutput = AgentOutput.parse(JSON.parse(response));\n              logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n              this._processAndRecordArtifacts(context, parsedOutput);\n              return parsedOutput;\n          } catch (parseError) {\n              logger.error(`Failed to parse LLM response as AgentOutput for task ${currentTask.id} by ${this.name}:`, parseError);\n              logger.error(`Raw LLM response: ${response}`);\n              // Record the raw, unparseable response as an artifact\n              context.recordArtifact(`${this.name}_raw_output_${currentTask.id}.txt`, response);\n              // Return an AgentOutput indicating the error\n              return new AgentOutput({ output: `Error processing response: LLM returned invalid JSON or schema.`, reasoning: 'LLM response was not valid JSON or AgentOutput schema.' });\n          }\n      } else if (response instanceof AgentOutput) {\n          // If the response is already an AgentOutput instance, use it directly\n          logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n          this._processAndRecordArtifacts(context, response);\n          return response;\n      }\n\n      // Handle unexpected response types from the LLM\n      logger.warn(`Unexpected response type from LLM for task ${currentTask.id} by ${this.name}: ${typeof response}`);\n      return new AgentOutput({ output: `Unexpected response type received from LLM.`, reasoning: 'LLM returned an unexpected data type.' });\n\n    } catch (error) {\n      logger.error(`Error during LLM interaction for task ${currentTask.id} by ${this.name}:`, error);\n      // Record the error as an artifact for debugging\n      context.recordArtifact(`${this.name}_error_${currentTask.id}.txt`, `Error executing task: ${error.message}`);\n      // Return an AgentOutput indicating the failure\n      return new AgentOutput({ output: `Failed to execute task due to an error.`, reasoning: 'LLM interaction failed.' });\n    }\n  }\n\n  /**\n   * Retrieves and formats artifacts required by the current task from the execution context.\n   * @param context The execution context.\n   * @returns A formatted string of required artifacts, or an empty string if none are found.\n   */\n  private async _getPreviousArtifacts(context: ExecutionContext): Promise<string> {\n    // Get the IDs of tasks whose outputs are required for the current step\n    const requiredTaskIds = ['initial', ...context.currentStep().requires];\n    const relevantArtifacts: string[] = [];\n\n    // Iterate through all items remembered in the context\n    for (const [key, contextItem] of Object.entries(context.items)) {\n      // Check if the artifact's source task ID is in the required list\n      if (requiredTaskIds.includes(contextItem.step.id)) {\n        let artifactContent = '';\n        // Format the artifact content based on its type\n        if (contextItem.content instanceof Artifact) {\n          // If it's an Artifact object, format its files\n          artifactContent = contextItem.content.files.map(file => \n            `--- File: ${file.path}\\n${file.content}`\n          ).join('\\n');\n        } else if (typeof contextItem.content === 'string') {\n          artifactContent = contextItem.content;\n        } else {\n          try {\n            // Attempt to stringify if it's a JSON-serializable object\n            artifactContent = JSON.stringify(contextItem.content, null, 2);\n          } catch (e) {\n            // Fallback to simple string conversion\n            artifactContent = String(contextItem.content);\n          }\n        }\n        // Add the formatted artifact to the list\n        relevantArtifacts.push(`--- Artifact '${key}' from (${contextItem.step.role}) in [${contextItem.step.id}]:\\n${artifactContent}`);\n      }\n    }\n\n    return relevantArtifacts.join('\\n\\n');\n  }\n\n  /**\n   * Processes the AgentOutput, records artifacts and reasoning, and handles potential team updates.\n   * @param context The execution context.\n   * @param agentOutput The output received from the agent.\n   */\n  private _processAndRecordArtifacts(context: ExecutionContext, agentOutput: AgentOutput): void {\n    // Record the main output message\n    context.recordArtifact(`${this.name}_output.txt`, agentOutput.output, true);\n\n    // Process and save any artifacts generated by the agent\n    if (agentOutput.artifact && agentOutput.artifact.files) {\n      agentOutput.artifact.files.forEach(file => {\n        // Save the file to the session directory\n        this.session?.addArtifact(file.path, file.content);\n        console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n        // Record the file artifact in the context's memory for potential future use\n        // We record the entire Artifact object here for simplicity, assuming it contains relevant context\n        context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); \n      });\n    }\n\n    // Record the agent's reasoning if provided\n    if (agentOutput.reasoning) {\n      context.recordArtifact(`${this.name}_reasoning.txt`, agentOutput.reasoning, true);\n    }\n\n    // Handle team updates (e.g., new prompts) - the System class will manage applying these\n    if (agentOutput.team && agentOutput.team.prompts) {\n      logger.info(`Agent ${this.name} provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n      // The System class is responsible for applying these prompt updates to other agents.\n    }\n  }\n}\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Role, AgentOutput } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  // Orchestrator might have a more complex team structure or delegation logic\n  // For now, we'll keep it simple and assume 'team' is managed by the System class\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name}`);\n  }\n\n  /**\n   * Generates a plan for the given high-level goal using the LLM.\n   * @param session The current session object.\n   * @param highLevelGoal The overall objective for the plan.\n   * @param initialFiles Any initial files provided to the system.\n   * @returns A Promise resolving to the generated Plan object, or null if generation fails.\n   */\n  async generatePlan(session: Session, highLevelGoal: string, initialFiles: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator '${this.name}' is generating a plan for: \"${highLevelGoal}\"`);\n\n    if (!this.llm) {\n      logger.error('LLM client not initialized for Orchestrator.');\n      return null;\n    }\n\n    // Get available agents and their details to include in the prompt for context\n    const availableAgents = session.agents.map(agent => ({\n      name: agent.name,\n      role: agent.role,\n      goal: agent.goal,\n    }));\n\n    // Format agent information for the prompt\n    const agentDescription = availableAgents.map(agent => \n      `- Name: '${agent.name}'\\n  Role: ${agent.role}\\n  Goal: \"${agent.goal}\"`\n    ).join('\\n');\n\n    // Load the planning prompt template from a file\n    // The path is relative to the project root, adjust as necessary\n    const promptTemplatePath = join(this.rootDir, 'prompts', 'general_planning.txt');\n    let promptTemplate: string;\n    try {\n      promptTemplate = readFileSync(promptTemplatePath, 'utf-8');\n    } catch (error) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}:`, error);\n      return null;\n    }\n\n    // Format the prompt with dynamic information\n    const planningPrompt = promptTemplate.replace('{{high_level_goal}}', highLevelGoal)\n                                      .replace('{{team_description}}', agentDescription)\n                                      .replace('{{initial_files}}', initialFiles.map(f => `--- File: ${f.path}\\n```\\n${f.content}\\n```).join('\\n\\n'));\n\n    logger.debug(`Constructed planning prompt for Orchestrator:\n${planningPrompt}`);\n    // Save the prompt used for planning as an artifact\n    session.addArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      // Prepare arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: planningPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: Plan, // Expecting a Plan object\n        responseMimeType: 'application/json',\n        temperature: 0.1, // Lower temperature for more deterministic planning\n      };\n      \n      // Call the LLM to generate the plan\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error('LLM returned no response for plan generation.');\n        return null;\n      }\n\n      // Ensure the response is parsed as a Plan object\n      if (typeof response === 'string') {\n        try {\n          const plan = Plan.parse(JSON.parse(response));\n          return plan;\n        } catch (parseError) {\n          logger.error('Failed to parse LLM response as Plan:', parseError);\n          logger.error(`Raw LLM response: ${response}`);\n          session.addArtifact('plan_generation_error.txt', `LLM response was not valid JSON or Plan schema: ${response}`);\n          return null;\n        }\n      } else if (response instanceof Plan) {\n        return response; // Response is already a Plan instance\n      }\n\n      logger.error(`Unexpected response type from LLM during plan generation: ${typeof response}`);\n      return null;\n\n    } catch (error) {\n      logger.error('Error during plan generation:', error);\n      session.addArtifact('plan_generation_error.txt', `Error: ${error.message}`);\n      return null;\n    }\n  }\n\n  /**\n   * Overrides the base Agent's executeTask. Orchestrator's primary role is planning,\n   * but it might be assigned specific tasks by the plan itself (e.g., plan refinement).\n   * @param context The execution context.\n   * @returns AgentOutput or null.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    logger.warn(`Orchestrator '${this.name}' executeTask called directly. Task: '${context.currentStep().description}'. Falling back to default agent behavior.`);\n    // Delegate to the base Agent's executeTask to process the prompt as a standard task.\n    // This allows the orchestrator to perform tasks defined within the plan it generated.\n    return super.executeTask(context);\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Artifact, File } from '../custom_types';\nimport { join, dirname as getDirname } from 'path';\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'fs';\n\nconst logger = Logger.getInstance();\n\nexport class Session {\n  public readonly sessionId: string;\n  public readonly agents: Agent[];\n  public state: string;\n  public readonly sessionDir: string;\n  public readonly projectRoot: string;\n\n  constructor(agents: Agent[], projectRoot: string, sessionId?: string) {\n    this.sessionId = sessionId || `session_${Date.now().toString()}`;\n    this.agents = agents;\n    this.state = 'initialized';\n    this.projectRoot = projectRoot;\n    this.sessionDir = join(this.projectRoot, 'sessions', this.sessionId);\n\n    // Ensure the session directory exists\n    if (!existsSync(this.sessionDir)) {\n      mkdirSync(this.sessionDir, { recursive: true });\n    }\n    logger.info(`Session created: ${this.sessionId} (Directory: ${this.sessionDir})`);\n  }\n\n  /**\n   * Saves an artifact to the session directory.\n   * @param name The name of the artifact file.\n   * @param content The content to write. Can be an object, string, etc.\n   */\n  addArtifact(name: string, content: any): void {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      const dir = getDirname(artifactPath);\n      // Ensure the directory for the artifact exists\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n      \n      let contentToWrite: string;\n      // Determine if content should be saved as JSON or plain text\n      if (typeof content === 'object' && content !== null && !(content instanceof File) && !(content instanceof Artifact)) {\n        // If it's a plain object (like from AgentOutput.toJSON()), save as JSON\n        contentToWrite = JSON.stringify(content, null, 2);\n      } else if (typeof content === 'string') {\n        contentToWrite = content;\n      } else {\n        // Fallback for other types, convert to string\n        contentToWrite = String(content);\n      }\n\n      writeFileSync(artifactPath, contentToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (e) {\n      logger.error(`Error saving artifact '${name}' in session ${this.sessionId}:`, e);\n    }\n  }\n\n  /**\n   * Retrieves an artifact from the session directory.\n   * @param name The name of the artifact file to load.\n   * @returns The content of the artifact, or null if not found or an error occurs.\n   */\n  getArtifact(name: string): any {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      if (existsSync(artifactPath)) {\n        const content = readFileSync(artifactPath, 'utf-8');\n        // Attempt to parse as JSON if the filename suggests it\n        if (name.endsWith('.json')) {\n          try {\n            return JSON.parse(content);\n          } catch (e) {\n            logger.error(`Error parsing JSON artifact '${name}':`, e);\n            return null; // Return null if JSON parsing fails\n          }\n        }\n        return content; // Return as plain text\n      } else {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n        return null;\n      }\n    } catch (e) {\n      logger.error(`Error retrieving artifact '${name}' from session ${this.sessionId}:`, e);\n      return null;\n    }\n  }\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface ContextItem {\n  name: string;\n  content: any;\n  step: Task;\n}\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores artifacts remembered from previous steps/rounds\n\n  constructor(session: Session, plan: Plan) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = 0;\n    this.roundNum = 1;\n    this.items = {};\n    logger.info('ExecutionContext initialized.');\n  }\n\n  /** Advances to the next step in the plan. */\n  nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  /** Resets the step index to the beginning of the plan. */\n  resetStepIndex(): void {\n    this.stepIndex = 0;\n    logger.debug('Step index reset to 0.');\n  }\n\n  /**\n   * Returns the current task based on the step index.\n   * @returns The current Task object.\n   * @throws Error if the step index is out of bounds.\n   */\n  currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error('Step index out of bounds. No more tasks in the plan.');\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  /**\n   * Records an artifact generated by a task and optionally remembers it in the context's memory.\n   * @param key The key identifier for the artifact.\n   * @param value The artifact content.\n   * @param remember If true, the artifact is stored in context.items for future steps.\n   */\n  recordArtifact(key: string, value: any, remember: boolean = false): void {\n    // Construct a unique artifact key including round and step number\n    const artifactKey = `__${this.roundNum}/__step_${this.stepIndex}_${key}`;\n    this.session.addArtifact(artifactKey, value);\n    // If 'remember' is true, store the artifact in the context's memory\n    if (remember) {\n      this._rememberItem(artifactKey, value, this.currentStep());\n    }\n  }\n\n  /**\n   * Records initial artifacts provided at the beginning of the workflow.\n   * @param key The key identifier for the initial artifact.\n   * @param value The artifact content.\n   */\n  recordInitial(key: string, value: any): void {\n    // Use a special task ID 'initial' for items provided at the start\n    const initialTask: Task = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: []\n    });\n    this._rememberItem(key, value, initialTask);\n    // If the initial item is an Artifact containing files, add them to the session as well\n    if (value instanceof Artifact && value.files) {\n        value.files.forEach(file => this.session.addArtifact(file.path, file.content));\n    }\n    logger.debug(`Recorded initial item '${key}'.`);\n  }\n\n  /**\n   * Helper method to store an item in the context's memory (items dictionary).\n   * @param name The name/key for the item.\n   * @param content The content of the item.\n   * @param step The Task associated with this item.\n   */\n  private _rememberItem(name: string, content: any, step: Task): void {\n    this.items[name] = { name, content, step };\n    logger.debug(`Item '${name}' remembered in context.`);\n  }\n}\n"},{"path":"src/llm/LLM.ts","content":"import { BaseModel, model_validator } from 'pydantic';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface for the LLM generation arguments, providing a structured way to pass parameters\nexport interface LLMGenerateContentArgs {\n  modelName: string;\n  contents: string;\n  systemInstruction?: string;\n  temperature?: number;\n  responseMimeType?: 'text/plain' | 'application/json' | 'application/json-schema';\n  responseSchema?: typeof BaseModel; // Expecting a Pydantic BaseModel class\n}\n\nexport abstract class LLM {\n  /**\n   * Abstract method to be implemented by concrete LLM providers.\n   * This method performs the actual call to the LLM API.\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the LLM's response (can be string, object, or parsed schema instance).\n   */\n  protected abstract generateContentImpl(args: LLMGenerateContentArgs): Promise<any>;\n\n  /**\n   * Public method to generate content using an LLM. Handles common logic like:\n   * - Retries with exponential backoff\n   * - Response parsing based on mime type and schema\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the processed LLM response, or null if all retries fail.\n   */\n  public async generateContent(args: LLMGenerateContentArgs): Promise<any> {\n    const { modelName, responseSchema, responseMimeType } = args;\n    const maxRetries = 3; // Number of times to retry the LLM call\n    let lastError: Error | null = null;\n\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        logger.info(`Calling LLM model '${modelName}' (Attempt ${attempt}/${maxRetries})...`);\n        const response = await this.generateContentImpl(args);\n\n        if (!response) {\n          throw new Error('LLM returned an empty response.');\n        }\n\n        // Process the response based on the requested schema and mime type\n        if (responseSchema && responseMimeType === 'application/json') {\n          if (typeof response === 'string') {\n            try {\n              const parsedJson = JSON.parse(response);\n              // Validate the parsed JSON against the provided Pydantic schema\n              const validatedData = responseSchema.parse(parsedJson);\n              logger.debug(`LLM response successfully parsed and validated against schema ${responseSchema.name}.`);\n              return validatedData;\n            } catch (e) {\n              // Catch JSON parsing or Pydantic validation errors\n              lastError = new Error(`Failed to parse or validate LLM JSON response with schema ${responseSchema.name}: ${e.message}`);\n              logger.error(lastError.message);\n              // Do not retry on parsing/validation errors, as the response itself is likely malformed\n              throw lastError;\n            }\n          } else if (response instanceof responseSchema) {\n            // If the response is already an instance of the expected schema, return it directly\n            logger.debug(`LLM response is already an instance of ${responseSchema.name}.`);\n            return response;\n          } else {\n             // Handle cases where the response is neither a string nor the expected schema type\n             lastError = new Error(`LLM response was not a string or expected schema type for JSON request.`);\n             logger.error(lastError.message);\n             throw lastError;\n          }\n        } else if (responseSchema && responseMimeType === 'application/json-schema') {\n            // Specific handling for JSON schema responses if needed\n             if (typeof response === 'string') {\n                try {\n                    const parsedSchema = JSON.parse(response);\n                    // Further validation or processing might be needed here\n                    logger.debug('LLM response parsed as JSON Schema.');\n                    return parsedSchema;\n                } catch (e) {\n                     lastError = new Error(`Failed to parse LLM JSON Schema response: ${e.message}`);\n                     logger.error(lastError.message);\n                     throw lastError;\n                }\n            }\n        }\n\n        // Default handling for text responses or un-schematized JSON\n        logger.debug('LLM response processed.');\n        return response;\n\n      } catch (error) {\n        // Capture the error and prepare for retry or final failure\n        lastError = error;\n        logger.error(`LLM generation failed (Attempt ${attempt}): ${error.message}`);\n        // Implement exponential backoff for retries\n        if (attempt < maxRetries) {\n          const delay = Math.pow(2, attempt) * 1000; // Delay in ms (1s, 2s, 4s)\n          logger.info(`Waiting for ${delay / 1000} seconds before retrying...`);\n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n\n    // If all retries fail, log the last error and return null\n    logger.error(`LLM generation failed after ${maxRetries} attempts. Last error: ${lastError?.message}`);\n    return null;\n  }\n\n  /**\n   * Factory method to create an LLM instance based on a species string.\n   * This allows for dynamic instantiation of different LLM providers.\n   * @param species - A string identifying the LLM species (e.g., 'gemini:gemini-pro', 'ollama:llama3').\n   * @returns An instance of a concrete LLM class.\n   */\n  static factory(species: string): LLM {\n    logger.info(`LLM Factory: Creating LLM instance for species '${species}'`);\n    \n    // TODO: Replace dummy implementations with actual LLM provider classes\n    // Example: Implement OllamaLLM, GeminiLLM, OpenAILLM, MistralLLM classes\n    // and instantiate them here based on the 'species' string.\n\n    if (species.startsWith('ollama:')) {\n        const modelName = species.split(':')[1];\n        // return new OllamaLLM(modelName);\n        logger.warn('OllamaLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('gemini:') || species === 'gemini-2.5-flash-lite') {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new GeminiLLM(modelName);\n        logger.warn('GeminiLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n     if (species.startsWith('openai:') || species.startsWith('gpt:') || species.startsWith('opi:')) {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new OpenAILLM(modelName);\n         logger.warn('OpenAILLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('mistral:')) {\n        const modelName = species.split(':')[1];\n        // return new MistralLLM(modelName);\n         logger.warn('MistralLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n\n    // Fallback to DummyLLM if the species is unknown\n    logger.warn(`Unknown LLM species '${species}'. Using DummyLLM as fallback.`);\n    return new DummyLLM(species);\n  }\n}\n\n/**\n * A dummy LLM implementation used as a placeholder.\n * It simulates LLM responses without making actual API calls.\n */\nclass DummyLLM extends LLM {\n  constructor(private modelName: string) {\n    super();\n    logger.warn(`DummyLLM initialized with model: ${this.modelName}. This is a placeholder.`);\n  }\n\n  protected async generateContentImpl(args: LLMGenerateContentArgs): Promise<any> {\n    logger.warn(`DummyLLM: generateContentImpl called for model ${this.modelName}. Returning mock response.`);\n    \n    // Simulate response based on schema if provided\n    if (args.responseSchema) {\n        try {\n            // Create a mock response adhering to the schema\n            if (args.responseSchema === AgentOutput) {\n                return new AgentOutput({\n                    output: `Mock output for ${this.modelName} task: ${args.contents.substring(0, 50)}...`,\n                    reasoning: 'This is a mock response from DummyLLM.',\n                    artifact: new Artifact({ task: 'mock_task', files: [new File({ path: 'mock_output.txt', content: 'Mock file content.'})]}),\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            } else if (args.responseSchema === Plan) {\n                 return new Plan({\n                    highLevelGoal: 'Mock Goal',\n                    reasoning: 'Mock reasoning for plan generation',\n                    roles: [{title: 'Mock Role', purpose: 'Mock Purpose'}],\n                    tasks: [{id: 'T1', description: 'Mock Task', role: 'Mock Role', agent: 'Mock Agent', requires: []}],\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            }\n            // Add handlers for other schemas as needed\n        } catch (e) {\n            logger.error(`Error creating mock response for schema ${args.responseSchema.name}:`, e);\n            return `Mock response failed for schema ${args.responseSchema.name}.`;\n        }\n    }\n    \n    // Default mock response if no schema is provided or handled\n    return `This is a mock response from DummyLLM for model ${this.modelName}.`;\n  }\n}\n"},{"path":"src/logger/Logger.ts","content":"import winston from 'winston';\nimport { join } from 'path';\nimport fs from 'fs';\n\n// Define log levels with priorities\nconst logLevels = {\n  error: 0,\n  warn: 1,\n  info: 2,\n  debug: 3,\n};\n\n// Define the format for log messages\nconst logFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }), // Include stack trace for errors\n  winston.format.splat(), // Support for string interpolation like printf\n  winston.format.printf(({ level, message, timestamp, ...metadata }) => {\n    // Construct the log message string\n    let msg = `${timestamp} [${level.toUpperCase()}]: ${message}`;\n    // Append any additional metadata (e.g., agent name, task ID)\n    if (Object.keys(metadata).length > 0) {\n      msg += ` ${JSON.stringify(metadata)}`;\n    }\n    return msg;\n  })\n);\n\n// Define a format for JSON line logs (useful for log aggregation)\nconst jsonLogFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.json() // Output logs as JSON objects on separate lines\n);\n\n// Ensure the log directory exists\nconst logDir = process.env.LOG_DIR || 'logs'; // Use LOG_DIR environment variable or default to 'logs'\nif (!fs.existsSync(logDir)) {\n  fs.mkdirSync(logDir, { recursive: true });\n}\n\nconst logFilePath = join(logDir, 'app.log');\nconst jsonLogFilePath = join(logDir, 'app.jsonl');\n\nexport class Logger {\n  private static instance: winston.Logger;\n\n  /**\n   * Gets the singleton Winston logger instance.\n   * Configures transports for console, file (plain text), and file (JSONL).\n   * @returns The configured Winston logger instance.\n   */\n  public static getInstance(): winston.Logger {\n    if (!Logger.instance) {\n      Logger.instance = winston.createLogger({\n        level: process.env.LOG_LEVEL || 'info', // Set log level from environment or default to 'info'\n        levels: logLevels,\n        format: logFormat, // Default format for transports unless overridden\n        transports: [\n          // Console transport: Outputs logs to the console with colorization\n          new winston.transports.Console({\n            format: winston.format.combine(\n              winston.format.colorize({ all: true }), // Apply colors to all levels\n              logFormat\n            ),\n          }),\n          // File transport: Logs to 'app.log' in plain text format\n          new winston.transports.File({\n            filename: logFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: logFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true, // Allows log rotation\n          }),\n          // JSONL File transport: Logs to 'app.jsonl' in JSON format\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: jsonLogFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true,\n          }),\n        ],\n        // Handle uncaught exceptions\n        exceptionHandlers: [\n          new winston.transports.Console({\n             format: winston.format.combine(winston.format.colorize({ all: true }), logFormat)\n          }),\n          new winston.transports.File({\n            filename: logFilePath,\n            format: logFormat,\n          }),\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            format: jsonLogFormat,\n          }),\n        ]\n      });\n\n      // Optional: Add a listener for logger errors\n      Logger.instance.on('error', (err) => {\n        console.error('Winston logger encountered an error:', err);\n      });\n    }\n    return Logger.instance;\n  }\n}\n"},{"path":"src/config/Config.ts","content":"import { readFileSync } from 'fs';\nimport { join } from 'path';\nimport yaml from 'js-yaml';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface defining the structure of the configuration object\nexport interface Config {\n  projectRoot?: string;       // Root directory of the project\n  agentDir?: string;          // Directory containing agent configurations\n  promptDir?: string;         // Directory containing prompt templates\n  logDir?: string;            // Directory for log files\n  defaultModel?: string;      // Default LLM model to use if not specified\n  workflowRounds?: number;    // Default number of workflow rounds\n  // Add other configuration properties as needed\n}\n\nexport class Config {\n  /**\n   * Loads configuration from a YAML file.\n   * @param fileName The name of the configuration file (e.g., 'runtime.yaml').\n   * @param basePath The base path to search for the config file (defaults to current working directory).\n   * @returns A Promise resolving to the loaded Config object with defaults applied.\n   */\n  public static async load(fileName: string, basePath: string = process.cwd()): Promise<Config> {\n    const configPath = join(basePath, fileName);\n    logger.info(`Attempting to load configuration from: ${configPath}`);\n    try {\n      // Read the file content\n      const fileContents = readFileSync(configPath, 'utf-8');\n      // Parse the YAML content\n      const config = yaml.load(fileContents) as Config;\n\n      // Apply default values if properties are missing\n      config.projectRoot = config.projectRoot || basePath;\n      config.agentDir = config.agentDir || 'agents';\n      config.promptDir = config.promptDir || 'prompts';\n      config.logDir = config.logDir || 'logs';\n      config.defaultModel = config.defaultModel || 'gemini-2.5-flash-lite';\n      config.workflowRounds = config.workflowRounds || 3;\n\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Failed to load configuration from ${configPath}:`, error);\n      // In case of failure, return a default configuration to allow the system to start\n      // Alternatively, you might want to throw an error to halt execution\n      return {\n        projectRoot: basePath,\n        agentDir: 'agents',\n        promptDir: 'prompts',\n        logDir: 'logs',\n        defaultModel: 'gemini-2.5-flash-lite',\n        workflowRounds: 3,\n      };\n    }\n  }\n}\n"},{"path":"src/custom_types.ts","content":"import { BaseModel, Field, model_validator } from 'pydantic';\nimport { z } from 'zod'; // Import Zod for potential schema definition or validation demonstration\n\n// --- Core Types representing Task Agnostic Steps (TAS) ---\n\n/** Represents a single actionable step within a plan. */\nexport class Task extends BaseModel {\n  id: string = Field(..., { description: \"Unique ID for the task (e.g., 'T1', 'T2').\" });\n  description: string = Field(..., { description: \"Detailed description of what to do for this task.\" });\n  role: string = Field(..., { description: \"The role responsible for performing this task.\" });\n  agent: string = Field(..., { description: \"The specific agent assigned to execute this task.\" });\n  requires: string[] = Field([], { description: \"List of task IDs whose outputs are required as prerequisites (e.g., ['T1', 'T2']).\" });\n}\n\n/** Represents a file generated or used within the system. */\nexport class File extends BaseModel {\n  path: string = Field(..., { description: \"File path or name (e.g., 'src/main.py').\" });\n  content: string = Field(..., { description: \"Full content of the file.\" });\n}\n\n/** Represents the output artifact of a completed task. */\nexport class Artifact extends BaseModel {\n  task: string = Field(..., { description: \"ID of the task that produced this artifact (e.g., 'T1').\" });\n  files: File[] = Field([], { description: \"List of files created or modified by the task.\" });\n}\n\n/** Defines a role within the system or team. */\nexport class Role extends BaseModel {\n  title: string = Field(..., { description: \"Title of the role (e.g., 'Engineer', 'Reviewer').\" });\n  purpose: string = Field(..., { description: \"The main purpose and high-level responsibilities of this role.\" });\n}\n\n/** Represents a system prompt configuration for an agent. */\nexport class Prompt extends BaseModel {\n  agent: string = Field(..., { description: \"Name of the agent this prompt is for. Must match exactly an existing agent.\" });\n  role: string = Field(..., { description: \"Role context of the agent.\" });\n  systemPrompt: string = Field(..., { description: \"Full text of the system prompt for this agent.\" });\n}\n\n/** Contains metadata and coordination information for the team, including prompt updates. */\nexport class Team extends BaseModel {\n  notes: string = Field('', { description: \"General notes or feedback about the team's performance or the plan.\" });\n  prompts: Prompt[] = Field([], { description: \"A list of new or updated system prompts for agents in the team.\" });\n}\n\n/** Defines the overall plan for a workflow, including goal, roles, and tasks. */\nexport class Plan extends BaseModel {\n  highLevelGoal: string = Field(..., { description: \"The main goal this plan is designed to achieve.\" });\n  reasoning: string = Field(..., { description: \"A brief explanation of the plan's structure and strategy.\" });\n  roles: Role[] = Field([], { description: \"List of all roles required to execute the plan.\" });\n  tasks: Task[] = Field([], { description: \"A step-by-step sequence of tasks to be executed in order.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n}\n\n/**\n * Represents the structured output returned by an agent after executing a task.\n * Adheres to the JSON output format expected by the runtime.\n */\nexport class AgentOutput extends BaseModel {\n  output: string = Field(..., { description: \"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\" });\n  artifact: Artifact | null = Field(null, { description: \"Optional artifact containing files created or modified by the agent.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n  reasoning: string | null = Field(null, { description: \"Explanation of how the agent arrived at this output.\" });\n\n  /**\n   * Custom validator to ensure that an AgentOutput contains either an 'output' message or an 'artifact'.\n   */\n  @model_validator(mode='after')\n  validateOutputAndArtifact(): this {\n    if (!this.output && !this.artifact) {\n      throw new Error('Agent output must contain either an \"output\" message or an \"artifact\".');\n    }\n    return this;\n  }\n}\n\n// --- Example Usage & Schema Generation ---\n// This block executes only when the script is run directly\nif (require.main === module) {\n  console.log('--- AgentOutput Schema ---\\n', JSON.stringify(AgentOutput.model_json_schema(), null, 2));\n  console.log('\\n--- Plan Schema ---\\n', JSON.stringify(Plan.model_json_schema(), null, 2));\n}\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_4_aitutor_result.txt' from (aitutor) in [T5]:
{"output":"Research and document SOTA TypeScript/Node.js implementation patterns for the identified Task Agnostic Steps (TAS). Focus on providing guidance on best practices and equivalent implementations for concepts like agent lifecycle, session management, context handling, LLM abstraction, and planning.","artifact":{"task":"T5","files":[{"path":"docs/typescript_patterns.md","content":"# SOTA TypeScript/Node.js Patterns for Runtime System\n\nThis document outlines State-Of-The-Art (SOTA) TypeScript and Node.js implementation patterns and best practices for recreating the runtime system's functionalities, based on the identified Task Agnostic Steps (TAS).\n\n## 1. System Initialization and Configuration\n\n*   **Load System Configuration**: Use libraries like `dotenv` for environment variables and `yaml` or `json` for configuration files. For complex configurations, consider using `class-validator` for validation.\n    *   **Example**: `import yaml from 'yaml'; const config = yaml.parse(fs.readFileSync('config/runtime.yaml', 'utf-8'));`\n*   **Discover and Load Agents/Prompts**: Employ dynamic module loading using `fs.readdirSync` and `import()` or `require()`. Use a factory pattern for instantiating agents.\n    *   **Pattern**: Factory Pattern, Dynamic Module Loading.\n*   **Instantiate Agents**: Leverage classes and interfaces for defining agent structures. Use dependency injection (DI) for managing LLM clients and other dependencies.\n    *   **TypeScript Feature**: Classes, Interfaces, Generics.\n    *   **Pattern**: Dependency Injection (e.g., using `InversifyJS` or manual injection).\n*   **Initialize LLM Clients**: Create an abstract `LLM` class with concrete implementations for different providers (e.g., `OpenAI`, `Gemini`, `Ollama`). Use a factory method for easy instantiation.\n    *   **TypeScript Feature**: Abstract Classes, Interfaces, Factory Pattern.\n    *   **Best Practice**: Abstract away provider-specific details.\n*   **Set Up Logging**: Utilize libraries like `winston` or `pino` for robust logging. `pino` is known for its performance and JSON logging capabilities, suitable for structured output.\n    *   **Best Practice**: Use structured logging (JSON) for easier parsing and analysis.\n\n## 2. Workflow Planning\n\n*   **Receive High-Level Goal**: Standard function parameter.\n*   **Describe Available Agents and Roles**: Pass agent metadata (name, role, goal) as an object or array of objects to the planning agent's prompt.\n*   **Generate Plan**: The LLM interaction follows the `LLM` abstraction. The prompt engineering will be crucial here, similar to the Python version.\n    *   **TypeScript Feature**: Interfaces for `Plan`, `Task`, `Role`, `Agent`.\n*   **Validate Plan**: Use Pydantic (via `zod` or `class-validator` in TS) to define and validate the `Plan` schema.\n    *   **TypeScript Feature**: Interfaces or Classes with Validation (e.g., `zod`).\n*   **Record Initial Plan**: Use the `Session`'s artifact storage mechanism.\n\n## 3. Workflow Execution\n\n*   **Initialize Session**: A `Session` class to manage state, potentially using a class with methods for artifact storage (e.g., using `fs` module for file persistence).\n    *   **TypeScript Feature**: Classes, `async/await` for I/O operations.\n*   **Initialize Execution Context**: A `ExecutionContext` class to hold `Session`, `Plan`, current step, round number, and artifacts (e.g., `Map<string, ContextItem>`).\n    *   **TypeScript Feature**: Classes, `Map` for storing context items.\n*   **Record Initial Artifacts**: Methods within `ExecutionContext` or `Session` to save files using the `fs` module.\n    *   **Best Practice**: Use `async/await` for all file I/O operations.\n*   **Iterate Through Workflow Rounds/Tasks**: Standard `for` loops and `while` loops. Use `async/await` for asynchronous agent task execution.\n*   **Identify and Select Agent**: A helper function or method to find an agent by name or role, similar to `find_agent_by_role`.\n*   **Prepare Task Prompt**: String interpolation or template literals for constructing prompts.\n*   **Execute Task**: Agent's `executeTask` method will be `async` to handle potential LLM calls.\n    *   **TypeScript Feature**: `async/await`.\n*   **Agent Task Execution**: Classes implementing an `Agent` interface.\n*   **LLM Interaction**: Calls to the abstracted `LLM` service.\n*   **Parse LLM Response**: Use `JSON.parse()` for JSON responses. Employ `try-catch` blocks for error handling. Use Pydantic models (or `zod` schemas) for validation.\n    *   **TypeScript Feature**: `try-catch`, `JSON.parse()`.\n*   **Handle Agent Output**: Parse the `AgentOutput` structure (defined as an interface or class).\n    *   **TypeScript Feature**: Interfaces, Classes.\n*   **Record Task Result**: Use `Session` methods for artifact storage.\n*   **Update Agent Prompts**: Methods on the `Agent` class or a central `AgentManager` to update prompts.\n*   **Advance Execution Context**: Incrementing counters or updating state within `ExecutionContext`.\n*   **Manage Artifacts**: `fs` module operations within `Session` or `ExecutionContext`.\n\n## 4. Agent Communication and State Management\n\n*   **Agent Lifecycle Management**: Defined by class methods (`constructor`, `executeTask`, etc.).\n*   **Contextual Information Sharing**: Pass `ExecutionContext` objects to agent methods.\n*   **Artifact Storage and Retrieval**: Use `fs` operations within `Session`.\n*   **Session Management**: The `Session` class will manage its state.\n*   **System Prompt Updates**: Methods on `Agent` objects or a central `AgentRegistry`.\n    *   **Best Practice**: Use event emitters or callbacks for prompt updates if real-time propagation is needed.\n\n## 5. LLM Interaction Abstraction\n\n*   **Abstract LLM Interface**: Define an `abstract class LLM` with an `abstract generateContent` method.\n    *   **TypeScript Feature**: `abstract class`, `abstract method`.\n*   **LLM Model Selection**: Factory pattern as described in Section 1.\n*   **Content Generation**: Implement `async` methods in concrete LLM classes.\n*   **Response Formatting**: Use `JSON.stringify()` and `JSON.parse()`. Leverage validation libraries like `zod` or `class-validator` to ensure responses conform to expected schemas (e.g., `AgentOutput`).\n    *   **TypeScript Feature**: `async/await`, `try-catch`, `JSON.stringify/parse`.\n    *   **Best Practice**: Implement robust error handling and schema validation for LLM responses.\n\n## 6. Error Handling and Resilience\n\n*   **Exception Handling**: Use `try-catch` blocks extensively, especially around I/O operations and LLM calls. Define custom error types for better error management.\n    *   **TypeScript Feature**: `try-catch`, custom error classes.\n*   **LLM Call Retries**: Implement retry logic using libraries like `axios-retry` (if using `axios` for HTTP calls) or custom promise-based retry functions.\n    *   **Best Practice**: Exponential backoff for retries.\n*   **Fallback Mechanisms**: Implement checks for agent availability and provide default agents or behaviors.\n\n## Key TypeScript/Node.js Considerations:\n\n*   **Asynchronous Operations**: Node.js is inherently asynchronous. Use `async/await` extensively for I/O, network requests (LLM calls), and any operation that might block the event loop.\n*   **Type Safety**: Leverage TypeScript's static typing to catch errors at compile time. Define interfaces and types for all data structures (Agents, Plans, Tasks, Artifacts, AgentOutput, etc.).\n*   **Modularity**: Structure the project into clear modules (e.g., `agents`, `core`, `llm`, `utils`, `prompts`).\n*   **Dependency Management**: Use `npm` or `yarn` for package management. Consider a monorepo structure (e.g., using Lerna or Nx) if the project grows complex.\n*   **Testing**: Employ testing frameworks like `Jest` or `Mocha` with `Chai` for unit and integration tests. Mock external dependencies (like LLM APIs) for reliable testing.\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_5_Lyra_result.txt' from (Lyra) in [T6]:
{"output":"Implemented the core components of the TypeScript Node.js runtime: Agent, Session, and ExecutionContext. Defined interfaces and classes for these components, adhering to SOTA TypeScript and Node.js practices. Established basic project structure and file organization.","artifact":{"task":"T6","files":[{"path":"src/agent/Agent.ts","content":"import { LLM, LLMGenerateContentArgs } from '../llm/LLM';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, Artifact, AgentOutput, File, Team, Role } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\n// Interface for Agent configuration loaded from YAML\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string; // Path to the system prompt file relative to agent definition\n  team?: PromptSpec[]; // For delegation, specifies members of its team\n}\n\n// Interface for team member specification (used in AgentSpec.team)\nexport interface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  protected systemPrompt: string;\n  protected llm: LLM;\n  protected model: string;\n  // Team is primarily for Orchestrator, but can be used by other agents for delegation\n  public team: Record<string, Agent> = {}; \n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    this.llm = LLM.factory(model); // Use the factory to get an LLM instance\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Model: ${this.model})`);\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task assigned to this agent within the given context.\n   * @param context The execution context containing plan, session, and previous artifacts.\n   * @returns A Promise resolving to an AgentOutput object or null if execution fails.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    const currentTask = context.currentStep();\n    const previousArtifactsContent = await this._getPreviousArtifacts(context);\n\n    // Construct the prompt for the LLM\n    const taskPrompt = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${currentTask.description}'`,\n      // Include team roles for context\n      `The team's roles are:\n${JSON.stringify(context.plan.roles.map((role: Role) => ({ title: role.title, purpose: role.purpose })), null, 2)}`,\n    ].join('\\n\\n');\n\n    let fullPrompt = taskPrompt;\n    if (previousArtifactsContent) {\n      fullPrompt += `\\n\\nPlease use the following outputs from other agents as your input:\\n\\n${previousArtifactsContent}\\n\\n`;\n    }\n    // Add instruction for structured output\n    fullPrompt += `Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project. Respond in a JSON format that includes 'output', and optionally 'artifact' and 'team' fields.`\n\n    logger.debug(`Constructed prompt for ${this.name} for task ${currentTask.id}:\n${fullPrompt}`);\n\n    // Record the prompt used for this task for debugging and auditing\n    context.recordArtifact(`${this.name}_prompt_${currentTask.id}.txt`, fullPrompt);\n\n    try {\n      // Define arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: fullPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: AgentOutput, // Expecting AgentOutput schema\n        responseMimeType: 'application/json',\n        temperature: 0.7, // Default temperature, can be overridden\n      };\n\n      // Call the LLM to generate content\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error(`LLM returned no response for task ${currentTask.id} by ${this.name}.`);\n        return null;\n      }\n\n      // Ensure the response is parsed as AgentOutput\n      if (typeof response === 'string') {\n          try {\n              // Attempt to parse the string response as JSON and validate against AgentOutput schema\n              const parsedOutput = AgentOutput.parse(JSON.parse(response));\n              logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n              this._processAndRecordArtifacts(context, parsedOutput);\n              return parsedOutput;\n          } catch (parseError) {\n              logger.error(`Failed to parse LLM response as AgentOutput for task ${currentTask.id} by ${this.name}:`, parseError);\n              logger.error(`Raw LLM response: ${response}`);\n              // Record the raw, unparseable response as an artifact\n              context.recordArtifact(`${this.name}_raw_output_${currentTask.id}.txt`, response);\n              // Return an AgentOutput indicating the error\n              return new AgentOutput({ output: `Error processing response: LLM returned invalid JSON or schema.`, reasoning: 'LLM response was not valid JSON or AgentOutput schema.' });\n          }\n      } else if (response instanceof AgentOutput) {\n          // If the response is already an AgentOutput instance, use it directly\n          logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n          this._processAndRecordArtifacts(context, response);\n          return response;\n      }\n\n      // Handle unexpected response types from the LLM\n      logger.warn(`Unexpected response type from LLM for task ${currentTask.id} by ${this.name}: ${typeof response}`);\n      return new AgentOutput({ output: `Unexpected response type received from LLM.`, reasoning: 'LLM returned an unexpected data type.' });\n\n    } catch (error) {\n      logger.error(`Error during LLM interaction for task ${currentTask.id} by ${this.name}:`, error);\n      // Record the error as an artifact for debugging\n      context.recordArtifact(`${this.name}_error_${currentTask.id}.txt`, `Error executing task: ${error.message}`);\n      // Return an AgentOutput indicating the failure\n      return new AgentOutput({ output: `Failed to execute task due to an error.`, reasoning: 'LLM interaction failed.' });\n    }\n  }\n\n  /**\n   * Retrieves and formats artifacts required by the current task from the execution context.\n   * @param context The execution context.\n   * @returns A formatted string of required artifacts, or an empty string if none are found.\n   */\n  private async _getPreviousArtifacts(context: ExecutionContext): Promise<string> {\n    // Get the IDs of tasks whose outputs are required for the current step\n    const requiredTaskIds = ['initial', ...context.currentStep().requires];\n    const relevantArtifacts: string[] = [];\n\n    // Iterate through all items remembered in the context\n    for (const [key, contextItem] of Object.entries(context.items)) {\n      // Check if the artifact's source task ID is in the required list\n      if (requiredTaskIds.includes(contextItem.step.id)) {\n        let artifactContent = '';\n        // Format the artifact content based on its type\n        if (contextItem.content instanceof Artifact) {\n          // If it's an Artifact object, format its files\n          artifactContent = contextItem.content.files.map(file => \n            `--- File: ${file.path}\\n${file.content}`\n          ).join('\\n');\n        } else if (typeof contextItem.content === 'string') {\n          artifactContent = contextItem.content;\n        } else {\n          try {\n            // Attempt to stringify if it's a JSON-serializable object\n            artifactContent = JSON.stringify(contextItem.content, null, 2);\n          } catch (e) {\n            // Fallback to simple string conversion\n            artifactContent = String(contextItem.content);\n          }\n        }\n        // Add the formatted artifact to the list\n        relevantArtifacts.push(`--- Artifact '${key}' from (${contextItem.step.role}) in [${contextItem.step.id}]:\\n${artifactContent}`);\n      }\n    }\n\n    return relevantArtifacts.join('\\n\\n');\n  }\n\n  /**\n   * Processes the AgentOutput, records artifacts and reasoning, and handles potential team updates.\n   * @param context The execution context.\n   * @param agentOutput The output received from the agent.\n   */\n  private _processAndRecordArtifacts(context: ExecutionContext, agentOutput: AgentOutput): void {\n    // Record the main output message\n    context.recordArtifact(`${this.name}_output.txt`, agentOutput.output, true);\n\n    // Process and save any artifacts generated by the agent\n    if (agentOutput.artifact && agentOutput.artifact.files) {\n      agentOutput.artifact.files.forEach(file => {\n        // Save the file to the session directory\n        this.session?.addArtifact(file.path, file.content);\n        console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n        // Record the file artifact in the context's memory for potential future use\n        // We record the entire Artifact object here for simplicity, assuming it contains relevant context\n        context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); \n      });\n    }\n\n    // Record the agent's reasoning if provided\n    if (agentOutput.reasoning) {\n      context.recordArtifact(`${this.name}_reasoning.txt`, agentOutput.reasoning, true);\n    }\n\n    // Handle team updates (e.g., new prompts) - the System class will manage applying these\n    if (agentOutput.team && agentOutput.team.prompts) {\n      logger.info(`Agent ${this.name} provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n      // The System class is responsible for applying these prompt updates to other agents.\n    }\n  }\n}\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Role, AgentOutput } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  // Orchestrator might have a more complex team structure or delegation logic\n  // For now, we'll keep it simple and assume 'team' is managed by the System class\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name}`);\n  }\n\n  /**\n   * Generates a plan for the given high-level goal using the LLM.\n   * @param session The current session object.\n   * @param highLevelGoal The overall objective for the plan.\n   * @param initialFiles Any initial files provided to the system.\n   * @returns A Promise resolving to the generated Plan object, or null if generation fails.\n   */\n  async generatePlan(session: Session, highLevelGoal: string, initialFiles: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator '${this.name}' is generating a plan for: \"${highLevelGoal}\"`);\n\n    if (!this.llm) {\n      logger.error('LLM client not initialized for Orchestrator.');\n      return null;\n    }\n\n    // Get available agents and their details to include in the prompt for context\n    const availableAgents = session.agents.map(agent => ({\n      name: agent.name,\n      role: agent.role,\n      goal: agent.goal,\n    }));\n\n    // Format agent information for the prompt\n    const agentDescription = availableAgents.map(agent => \n      `- Name: '${agent.name}'\\n  Role: ${agent.role}\\n  Goal: \"${agent.goal}\"`\n    ).join('\\n');\n\n    // Load the planning prompt template from a file\n    // The path is relative to the project root, adjust as necessary\n    const promptTemplatePath = join(this.rootDir, 'prompts', 'general_planning.txt');\n    let promptTemplate: string;\n    try {\n      promptTemplate = readFileSync(promptTemplatePath, 'utf-8');\n    } catch (error) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}:`, error);\n      return null;\n    }\n\n    // Format the prompt with dynamic information\n    const planningPrompt = promptTemplate.replace('{{high_level_goal}}', highLevelGoal)\n                                      .replace('{{team_description}}', agentDescription)\n                                      .replace('{{initial_files}}', initialFiles.map(f => `--- File: ${f.path}\\n```\\n${f.content}\\n```).join('\\n\\n'));\n\n    logger.debug(`Constructed planning prompt for Orchestrator:\n${planningPrompt}`);\n    // Save the prompt used for planning as an artifact\n    session.addArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      // Prepare arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: planningPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: Plan, // Expecting a Plan object\n        responseMimeType: 'application/json',\n        temperature: 0.1, // Lower temperature for more deterministic planning\n      };\n      \n      // Call the LLM to generate the plan\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error('LLM returned no response for plan generation.');\n        return null;\n      }\n\n      // Ensure the response is parsed as a Plan object\n      if (typeof response === 'string') {\n        try {\n          const plan = Plan.parse(JSON.parse(response));\n          return plan;\n        } catch (parseError) {\n          logger.error('Failed to parse LLM response as Plan:', parseError);\n          logger.error(`Raw LLM response: ${response}`);\n          session.addArtifact('plan_generation_error.txt', `LLM response was not valid JSON or Plan schema: ${response}`);\n          return null;\n        }\n      } else if (response instanceof Plan) {\n        return response; // Response is already a Plan instance\n      }\n\n      logger.error(`Unexpected response type from LLM during plan generation: ${typeof response}`);\n      return null;\n\n    } catch (error) {\n      logger.error('Error during plan generation:', error);\n      session.addArtifact('plan_generation_error.txt', `Error: ${error.message}`);\n      return null;\n    }\n  }\n\n  /**\n   * Overrides the base Agent's executeTask. Orchestrator's primary role is planning,\n   * but it might be assigned specific tasks by the plan itself (e.g., plan refinement).\n   * @param context The execution context.\n   * @returns AgentOutput or null.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    logger.warn(`Orchestrator '${this.name}' executeTask called directly. Task: '${context.currentStep().description}'. Falling back to default agent behavior.`);\n    // Delegate to the base Agent's executeTask to process the prompt as a standard task.\n    // This allows the orchestrator to perform tasks defined within the plan it generated.\n    return super.executeTask(context);\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Artifact, File } from '../custom_types';\nimport { join, dirname as getDirname } from 'path';\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'fs';\n\nconst logger = Logger.getInstance();\n\nexport class Session {\n  public readonly sessionId: string;\n  public readonly agents: Agent[];\n  public state: string;\n  public readonly sessionDir: string;\n  public readonly projectRoot: string;\n\n  constructor(agents: Agent[], projectRoot: string, sessionId?: string) {\n    this.sessionId = sessionId || `session_${Date.now().toString()}`;\n    this.agents = agents;\n    this.state = 'initialized';\n    this.projectRoot = projectRoot;\n    this.sessionDir = join(this.projectRoot, 'sessions', this.sessionId);\n\n    // Ensure the session directory exists\n    if (!existsSync(this.sessionDir)) {\n      mkdirSync(this.sessionDir, { recursive: true });\n    }\n    logger.info(`Session created: ${this.sessionId} (Directory: ${this.sessionDir})`);\n  }\n\n  /**\n   * Saves an artifact to the session directory.\n   * @param name The name of the artifact file.\n   * @param content The content to write. Can be an object, string, etc.\n   */\n  addArtifact(name: string, content: any): void {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      const dir = getDirname(artifactPath);\n      // Ensure the directory for the artifact exists\n      if (!existsSync(dir)) {\n        mkdirSync(dir, { recursive: true });\n      }\n      \n      let contentToWrite: string;\n      // Determine if content should be saved as JSON or plain text\n      if (typeof content === 'object' && content !== null && !(content instanceof File) && !(content instanceof Artifact)) {\n        // If it's a plain object (like from AgentOutput.toJSON()), save as JSON\n        contentToWrite = JSON.stringify(content, null, 2);\n      } else if (typeof content === 'string') {\n        contentToWrite = content;\n      } else {\n        // Fallback for other types, convert to string\n        contentToWrite = String(content);\n      }\n\n      writeFileSync(artifactPath, contentToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (e) {\n      logger.error(`Error saving artifact '${name}' in session ${this.sessionId}:`, e);\n    }\n  }\n\n  /**\n   * Retrieves an artifact from the session directory.\n   * @param name The name of the artifact file to load.\n   * @returns The content of the artifact, or null if not found or an error occurs.\n   */\n  getArtifact(name: string): any {\n    const artifactPath = join(this.sessionDir, name);\n    try {\n      if (existsSync(artifactPath)) {\n        const content = readFileSync(artifactPath, 'utf-8');\n        // Attempt to parse as JSON if the filename suggests it\n        if (name.endsWith('.json')) {\n          try {\n            return JSON.parse(content);\n          } catch (e) {\n            logger.error(`Error parsing JSON artifact '${name}':`, e);\n            return null; // Return null if JSON parsing fails\n          }\n        }\n        return content; // Return as plain text\n      } else {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n        return null;\n      }\n    } catch (e) {\n      logger.error(`Error retrieving artifact '${name}' from session ${this.sessionId}:`, e);\n      return null;\n    }\n  }\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\nexport interface ContextItem {\n  name: string;\n  content: any;\n  step: Task;\n}\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores artifacts remembered from previous steps/rounds\n\n  constructor(session: Session, plan: Plan) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = 0;\n    this.roundNum = 1;\n    this.items = {};\n    logger.info('ExecutionContext initialized.');\n  }\n\n  /** Advances to the next step in the plan. */\n  nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  /** Resets the step index to the beginning of the plan. */\n  resetStepIndex(): void {\n    this.stepIndex = 0;\n    logger.debug('Step index reset to 0.');\n  }\n\n  /**\n   * Returns the current task based on the step index.\n   * @returns The current Task object.\n   * @throws Error if the step index is out of bounds.\n   */\n  currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error('Step index out of bounds. No more tasks in the plan.');\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  /**\n   * Records an artifact generated by a task and optionally remembers it in the context's memory.\n   * @param key The key identifier for the artifact.\n   * @param value The artifact content.\n   * @param remember If true, the artifact is stored in context.items for future steps.\n   */\n  recordArtifact(key: string, value: any, remember: boolean = false): void {\n    // Construct a unique artifact key including round and step number\n    const artifactKey = `__${this.roundNum}/__step_${this.stepIndex}_${key}`;\n    this.session.addArtifact(artifactKey, value);\n    // If 'remember' is true, store the artifact in the context's memory\n    if (remember) {\n      this._rememberItem(artifactKey, value, this.currentStep());\n    }\n  }\n\n  /**\n   * Records initial artifacts provided at the beginning of the workflow.\n   * @param key The key identifier for the initial artifact.\n   * @param value The artifact content.\n   */\n  recordInitial(key: string, value: any): void {\n    // Use a special task ID 'initial' for items provided at the start\n    const initialTask: Task = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: []\n    });\n    this._rememberItem(key, value, initialTask);\n    // If the initial item is an Artifact containing files, add them to the session as well\n    if (value instanceof Artifact && value.files) {\n        value.files.forEach(file => this.session.addArtifact(file.path, file.content));\n    }\n    logger.debug(`Recorded initial item '${key}'.`);\n  }\n\n  /**\n   * Helper method to store an item in the context's memory (items dictionary).\n   * @param name The name/key for the item.\n   * @param content The content of the item.\n   * @param step The Task associated with this item.\n   */\n  private _rememberItem(name: string, content: any, step: Task): void {\n    this.items[name] = { name, content, step };\n    logger.debug(`Item '${name}' remembered in context.`);\n  }\n}\n"},{"path":"src/llm/LLM.ts","content":"import { BaseModel, model_validator } from 'pydantic';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface for the LLM generation arguments, providing a structured way to pass parameters\nexport interface LLMGenerateContentArgs {\n  modelName: string;\n  contents: string;\n  systemInstruction?: string;\n  temperature?: number;\n  responseMimeType?: 'text/plain' | 'application/json' | 'application/json-schema';\n  responseSchema?: typeof BaseModel; // Expecting a Pydantic BaseModel class\n}\n\nexport abstract class LLM {\n  /**\n   * Abstract method to be implemented by concrete LLM providers.\n   * This method performs the actual call to the LLM API.\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the LLM's response (can be string, object, or parsed schema instance).\n   */\n  protected abstract generateContentImpl(args: LLMGenerateContentArgs): Promise<any>;\n\n  /**\n   * Public method to generate content using an LLM. Handles common logic like:\n   * - Retries with exponential backoff\n   * - Response parsing based on mime type and schema\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the processed LLM response, or null if all retries fail.\n   */\n  public async generateContent(args: LLMGenerateContentArgs): Promise<any> {\n    const { modelName, responseSchema, responseMimeType } = args;\n    const maxRetries = 3; // Number of times to retry the LLM call\n    let lastError: Error | null = null;\n\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        logger.info(`Calling LLM model '${modelName}' (Attempt ${attempt}/${maxRetries})...`);\n        const response = await this.generateContentImpl(args);\n\n        if (!response) {\n          throw new Error('LLM returned an empty response.');\n        }\n\n        // Process the response based on the requested schema and mime type\n        if (responseSchema && responseMimeType === 'application/json') {\n          if (typeof response === 'string') {\n            try {\n              const parsedJson = JSON.parse(response);\n              // Validate the parsed JSON against the provided Pydantic schema\n              const validatedData = responseSchema.parse(parsedJson);\n              logger.debug(`LLM response successfully parsed and validated against schema ${responseSchema.name}.`);\n              return validatedData;\n            } catch (e) {\n              // Catch JSON parsing or Pydantic validation errors\n              lastError = new Error(`Failed to parse or validate LLM JSON response with schema ${responseSchema.name}: ${e.message}`);\n              logger.error(lastError.message);\n              // Do not retry on parsing/validation errors, as the response itself is likely malformed\n              throw lastError;\n            }\n          } else if (response instanceof responseSchema) {\n            // If the response is already an instance of the expected schema, return it directly\n            logger.debug(`LLM response is already an instance of ${responseSchema.name}.`);\n            return response;\n          } else {\n             // Handle cases where the response is neither a string nor the expected schema type\n             lastError = new Error(`LLM response was not a string or expected schema type for JSON request.`);\n             logger.error(lastError.message);\n             throw lastError;\n          }\n        } else if (responseSchema && responseMimeType === 'application/json-schema') {\n            // Specific handling for JSON schema responses if needed\n             if (typeof response === 'string') {\n                try {\n                    const parsedSchema = JSON.parse(response);\n                    // Further validation or processing might be needed here\n                    logger.debug('LLM response parsed as JSON Schema.');\n                    return parsedSchema;\n                } catch (e) {\n                     lastError = new Error(`Failed to parse LLM JSON Schema response: ${e.message}`);\n                     logger.error(lastError.message);\n                     throw lastError;\n                }\n            }\n        }\n\n        // Default handling for text responses or un-schematized JSON\n        logger.debug('LLM response processed.');\n        return response;\n\n      } catch (error) {\n        // Capture the error and prepare for retry or final failure\n        lastError = error;\n        logger.error(`LLM generation failed (Attempt ${attempt}): ${error.message}`);\n        // Implement exponential backoff for retries\n        if (attempt < maxRetries) {\n          const delay = Math.pow(2, attempt) * 1000; // Delay in ms (1s, 2s, 4s)\n          logger.info(`Waiting for ${delay / 1000} seconds before retrying...`);\n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n\n    // If all retries fail, log the last error and return null\n    logger.error(`LLM generation failed after ${maxRetries} attempts. Last error: ${lastError?.message}`);\n    return null;\n  }\n\n  /**\n   * Factory method to create an LLM instance based on a species string.\n   * This allows for dynamic instantiation of different LLM providers.\n   * @param species - A string identifying the LLM species (e.g., 'gemini:gemini-pro', 'ollama:llama3').\n   * @returns An instance of a concrete LLM class.\n   */\n  static factory(species: string): LLM {\n    logger.info(`LLM Factory: Creating LLM instance for species '${species}'`);\n    \n    // TODO: Replace dummy implementations with actual LLM provider classes\n    // Example: Implement OllamaLLM, GeminiLLM, OpenAILLM, MistralLLM classes\n    // and instantiate them here based on the 'species' string.\n\n    if (species.startsWith('ollama:')) {\n        const modelName = species.split(':')[1];\n        // return new OllamaLLM(modelName);\n        logger.warn('OllamaLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('gemini:') || species === 'gemini-2.5-flash-lite') {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new GeminiLLM(modelName);\n        logger.warn('GeminiLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n     if (species.startsWith('openai:') || species.startsWith('gpt:') || species.startsWith('opi:')) {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new OpenAILLM(modelName);\n         logger.warn('OpenAILLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('mistral:')) {\n        const modelName = species.split(':')[1];\n        // return new MistralLLM(modelName);\n         logger.warn('MistralLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n\n    // Fallback to DummyLLM if the species is unknown\n    logger.warn(`Unknown LLM species '${species}'. Using DummyLLM as fallback.`);\n    return new DummyLLM(species);\n  }\n}\n\n/**\n * A dummy LLM implementation used as a placeholder.\n * It simulates LLM responses without making actual API calls.\n */\nclass DummyLLM extends LLM {\n  constructor(private modelName: string) {\n    super();\n    logger.warn(`DummyLLM initialized with model: ${this.modelName}. This is a placeholder.`);\n  }\n\n  protected async generateContentImpl(args: LLMGenerateContentArgs): Promise<any> {\n    logger.warn(`DummyLLM: generateContentImpl called for model ${this.modelName}. Returning mock response.`);\n    \n    // Simulate response based on schema if provided\n    if (args.responseSchema) {\n        try {\n            // Create a mock response adhering to the schema\n            if (args.responseSchema === AgentOutput) {\n                return new AgentOutput({\n                    output: `Mock output for ${this.modelName} task: ${args.contents.substring(0, 50)}...`,\n                    reasoning: 'This is a mock response from DummyLLM.',\n                    artifact: new Artifact({ task: 'mock_task', files: [new File({ path: 'mock_output.txt', content: 'Mock file content.'})]}),\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            } else if (args.responseSchema === Plan) {\n                 return new Plan({\n                    highLevelGoal: 'Mock Goal',\n                    reasoning: 'Mock reasoning for plan generation',\n                    roles: [{title: 'Mock Role', purpose: 'Mock Purpose'}],\n                    tasks: [{id: 'T1', description: 'Mock Task', role: 'Mock Role', agent: 'Mock Agent', requires: []}],\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            }\n            // Add handlers for other schemas as needed\n        } catch (e) {\n            logger.error(`Error creating mock response for schema ${args.responseSchema.name}:`, e);\n            return `Mock response failed for schema ${args.responseSchema.name}.`;\n        }\n    }\n    \n    // Default mock response if no schema is provided or handled\n    return `This is a mock response from DummyLLM for model ${this.modelName}.`;\n  }\n}\n"},{"path":"src/logger/Logger.ts","content":"import winston from 'winston';\nimport { join } from 'path';\nimport fs from 'fs';\n\n// Define log levels with priorities\nconst logLevels = {\n  error: 0,\n  warn: 1,\n  info: 2,\n  debug: 3,\n};\n\n// Define the format for log messages\nconst logFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }), // Include stack trace for errors\n  winston.format.splat(), // Support for string interpolation like printf\n  winston.format.printf(({ level, message, timestamp, ...metadata }) => {\n    // Construct the log message string\n    let msg = `${timestamp} [${level.toUpperCase()}]: ${message}`;\n    // Append any additional metadata (e.g., agent name, task ID)\n    if (Object.keys(metadata).length > 0) {\n      msg += ` ${JSON.stringify(metadata)}`;\n    }\n    return msg;\n  })\n);\n\n// Define a format for JSON line logs (useful for log aggregation)\nconst jsonLogFormat = winston.format.combine(\n  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),\n  winston.format.errors({ stack: true }),\n  winston.format.splat(),\n  winston.format.json() // Output logs as JSON objects on separate lines\n);\n\n// Ensure the log directory exists\nconst logDir = process.env.LOG_DIR || 'logs'; // Use LOG_DIR environment variable or default to 'logs'\nif (!fs.existsSync(logDir)) {\n  fs.mkdirSync(logDir, { recursive: true });\n}\n\nconst logFilePath = join(logDir, 'app.log');\nconst jsonLogFilePath = join(logDir, 'app.jsonl');\n\nexport class Logger {\n  private static instance: winston.Logger;\n\n  /**\n   * Gets the singleton Winston logger instance.\n   * Configures transports for console, file (plain text), and file (JSONL).\n   * @returns The configured Winston logger instance.\n   */\n  public static getInstance(): winston.Logger {\n    if (!Logger.instance) {\n      Logger.instance = winston.createLogger({\n        level: process.env.LOG_LEVEL || 'info', // Set log level from environment or default to 'info'\n        levels: logLevels,\n        format: logFormat, // Default format for transports unless overridden\n        transports: [\n          // Console transport: Outputs logs to the console with colorization\n          new winston.transports.Console({\n            format: winston.format.combine(\n              winston.format.colorize({ all: true }), // Apply colors to all levels\n              logFormat\n            ),\n          }),\n          // File transport: Logs to 'app.log' in plain text format\n          new winston.transports.File({\n            filename: logFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: logFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true, // Allows log rotation\n          }),\n          // JSONL File transport: Logs to 'app.jsonl' in JSON format\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            level: process.env.LOG_LEVEL || 'info',\n            format: jsonLogFormat,\n            maxsize: 1024 * 1024 * 5, // Max log file size: 5MB\n            tailable: true,\n          }),\n        ],\n        // Handle uncaught exceptions\n        exceptionHandlers: [\n          new winston.transports.Console({\n             format: winston.format.combine(winston.format.colorize({ all: true }), logFormat)\n          }),\n          new winston.transports.File({\n            filename: logFilePath,\n            format: logFormat,\n          }),\n          new winston.transports.File({\n            filename: jsonLogFilePath,\n            format: jsonLogFormat,\n          }),\n        ]\n      });\n\n      // Optional: Add a listener for logger errors\n      Logger.instance.on('error', (err) => {\n        console.error('Winston logger encountered an error:', err);\n      });\n    }\n    return Logger.instance;\n  }\n}\n"},{"path":"src/config/Config.ts","content":"import { readFileSync } from 'fs';\nimport { join } from 'path';\nimport yaml from 'js-yaml';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface defining the structure of the configuration object\nexport interface Config {\n  projectRoot?: string;       // Root directory of the project\n  agentDir?: string;          // Directory containing agent configurations\n  promptDir?: string;         // Directory containing prompt templates\n  logDir?: string;            // Directory for log files\n  defaultModel?: string;      // Default LLM model to use if not specified\n  workflowRounds?: number;    // Default number of workflow rounds\n  // Add other configuration properties as needed\n}\n\nexport class Config {\n  /**\n   * Loads configuration from a YAML file.\n   * @param fileName The name of the configuration file (e.g., 'runtime.yaml').\n   * @param basePath The base path to search for the config file (defaults to current working directory).\n   * @returns A Promise resolving to the loaded Config object with defaults applied.\n   */\n  public static async load(fileName: string, basePath: string = process.cwd()): Promise<Config> {\n    const configPath = join(basePath, fileName);\n    logger.info(`Attempting to load configuration from: ${configPath}`);\n    try {\n      // Read the file content\n      const fileContents = readFileSync(configPath, 'utf-8');\n      // Parse the YAML content\n      const config = yaml.load(fileContents) as Config;\n\n      // Apply default values if properties are missing\n      config.projectRoot = config.projectRoot || basePath;\n      config.agentDir = config.agentDir || 'agents';\n      config.promptDir = config.promptDir || 'prompts';\n      config.logDir = config.logDir || 'logs';\n      config.defaultModel = config.defaultModel || 'gemini-2.5-flash-lite';\n      config.workflowRounds = config.workflowRounds || 3;\n\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Failed to load configuration from ${configPath}:`, error);\n      // In case of failure, return a default configuration to allow the system to start\n      // Alternatively, you might want to throw an error to halt execution\n      return {\n        projectRoot: basePath,\n        agentDir: 'agents',\n        promptDir: 'prompts',\n        logDir: 'logs',\n        defaultModel: 'gemini-2.5-flash-lite',\n        workflowRounds: 3,\n      };\n    }\n  }\n}\n"},{"path":"src/custom_types.ts","content":"import { BaseModel, Field, model_validator } from 'pydantic';\nimport { z } from 'zod'; // Import Zod for potential schema definition or validation demonstration\n\n// --- Core Types representing Task Agnostic Steps (TAS) ---\n\n/** Represents a single actionable step within a plan. */\nexport class Task extends BaseModel {\n  id: string = Field(..., { description: \"Unique ID for the task (e.g., 'T1', 'T2').\" });\n  description: string = Field(..., { description: \"Detailed description of what to do for this task.\" });\n  role: string = Field(..., { description: \"The role responsible for performing this task.\" });\n  agent: string = Field(..., { description: \"The specific agent assigned to execute this task.\" });\n  requires: string[] = Field([], { description: \"List of task IDs whose outputs are required as prerequisites (e.g., ['T1', 'T2']).\" });\n}\n\n/** Represents a file generated or used within the system. */\nexport class File extends BaseModel {\n  path: string = Field(..., { description: \"File path or name (e.g., 'src/main.py').\" });\n  content: string = Field(..., { description: \"Full content of the file.\" });\n}\n\n/** Represents the output artifact of a completed task. */\nexport class Artifact extends BaseModel {\n  task: string = Field(..., { description: \"ID of the task that produced this artifact (e.g., 'T1').\" });\n  files: File[] = Field([], { description: \"List of files created or modified by the task.\" });\n}\n\n/** Defines a role within the system or team. */\nexport class Role extends BaseModel {\n  title: string = Field(..., { description: \"Title of the role (e.g., 'Engineer', 'Reviewer').\" });\n  purpose: string = Field(..., { description: \"The main purpose and high-level responsibilities of this role.\" });\n}\n\n/** Represents a system prompt configuration for an agent. */\nexport class Prompt extends BaseModel {\n  agent: string = Field(..., { description: \"Name of the agent this prompt is for. Must match exactly an existing agent.\" });\n  role: string = Field(..., { description: \"Role context of the agent.\" });\n  systemPrompt: string = Field(..., { description: \"Full text of the system prompt for this agent.\" });\n}\n\n/** Contains metadata and coordination information for the team, including prompt updates. */\nexport class Team extends BaseModel {\n  notes: string = Field('', { description: \"General notes or feedback about the team's performance or the plan.\" });\n  prompts: Prompt[] = Field([], { description: \"A list of new or updated system prompts for agents in the team.\" });\n}\n\n/** Defines the overall plan for a workflow, including goal, roles, and tasks. */\nexport class Plan extends BaseModel {\n  highLevelGoal: string = Field(..., { description: \"The main goal this plan is designed to achieve.\" });\n  reasoning: string = Field(..., { description: \"A brief explanation of the plan's structure and strategy.\" });\n  roles: Role[] = Field([], { description: \"List of all roles required to execute the plan.\" });\n  tasks: Task[] = Field([], { description: \"A step-by-step sequence of tasks to be executed in order.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n}\n\n/**\n * Represents the structured output returned by an agent after executing a task.\n * Adheres to the JSON output format expected by the runtime.\n */\nexport class AgentOutput extends BaseModel {\n  output: string = Field(..., { description: \"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\" });\n  artifact: Artifact | null = Field(null, { description: \"Optional artifact containing files created or modified by the agent.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n  reasoning: string | null = Field(null, { description: \"Explanation of how the agent arrived at this output.\" });\n\n  /**\n   * Custom validator to ensure that an AgentOutput contains either an 'output' message or an 'artifact'.\n   */\n  @model_validator(mode='after')\n  validateOutputAndArtifact(): this {\n    if (!this.output && !this.artifact) {\n      throw new Error('Agent output must contain either an \"output\" message or an \"artifact\".');\n    }\n    return this;\n  }\n}\n\n// --- Example Usage & Schema Generation ---\n// This block executes only when the script is run directly\nif (require.main === module) {\n  console.log('--- AgentOutput Schema ---\\n', JSON.stringify(AgentOutput.model_json_schema(), null, 2));\n  console.log('\\n--- Plan Schema ---\\n', JSON.stringify(Plan.model_json_schema(), null, 2));\n}\n"}]},"team":{"notes":"Core components (Agent, Session, ExecutionContext) have been implemented in TypeScript. The structure follows the defined architecture and leverages SOTA practices like classes, interfaces, and async/await. The LLM integration is abstracted but currently uses a dummy implementation. Further steps will involve implementing concrete LLM providers and the Orchestrator logic.","prompts":[]},"reasoning":"The task was to begin implementing the core components of the TypeScript runtime: Agent, Session, and ExecutionContext. I have created the necessary TypeScript files (`Agent.ts`, `Session.ts`, `ExecutionContext.ts`) within the `src/agent`, `src/core` directories respectively. These files define classes and interfaces that mirror the functionality described in the Python runtime and the architecture defined in T3. Key aspects like agent initialization, task execution, session management, artifact handling, and context management have been addressed using modern TypeScript features and Node.js APIs (like `fs` and `path`). The LLM interaction is abstracted behind an `LLM` class, with a `DummyLLM` serving as a placeholder until concrete implementations are ready. Error handling and logging have been incorporated using `winston` and `try-catch` blocks. The `custom_types.ts` file is updated to reflect these components. The `System.ts` and `Orchestrator.ts` files have been updated to reflect the new TypeScript structure and imports. The `package.json` and `tsconfig.json` from T4 provide the foundation for building and running these components."}

--- Artifact '__1/__step_6_Lyra_result.txt' from (Lyra) in [T7]:
{"output":"Developed the LLM integration layer in TypeScript, including an abstract `LLM` class and a concrete `DummyLLM` for placeholder functionality. The `LLMGenerateContentArgs` interface standardizes LLM call parameters. Implemented retry logic with exponential backoff and basic response parsing/validation. The `LLM.factory` method is set up to dynamically instantiate LLM providers based on a 'species' string, though concrete implementations for providers like Gemini, Ollama, OpenAI, and Mistral are still placeholders.","artifact":{"task":"T7","files":[{"path":"src/llm/LLM.ts","content":"import { BaseModel, model_validator } from 'pydantic';\nimport { Logger } from '../logger/Logger';\n\nconst logger = Logger.getInstance();\n\n// Interface for the LLM generation arguments, providing a structured way to pass parameters\nexport interface LLMGenerateContentArgs {\n  modelName: string;\n  contents: string;\n  systemInstruction?: string;\n  temperature?: number;\n  responseMimeType?: 'text/plain' | 'application/json' | 'application/json-schema';\n  responseSchema?: typeof BaseModel; // Expecting a Pydantic BaseModel class\n}\n\nexport abstract class LLM {\n  /**\n   * Abstract method to be implemented by concrete LLM providers.\n   * This method performs the actual call to the LLM API.\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the LLM's response (can be string, object, or parsed schema instance).\n   */\n  protected abstract generateContentImpl(args: LLMGenerateContentArgs): Promise<any>;\n\n  /**\n   * Public method to generate content using an LLM. Handles common logic like:\n   * - Retries with exponential backoff\n   * - Response parsing based on mime type and schema\n   * @param args - The arguments for content generation.\n   * @returns A Promise resolving to the processed LLM response, or null if all retries fail.\n   */\n  public async generateContent(args: LLMGenerateContentArgs): Promise<any> {\n    const { modelName, responseSchema, responseMimeType } = args;\n    const maxRetries = 3; // Number of times to retry the LLM call\n    let lastError: Error | null = null;\n\n    for (let attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        logger.info(`Calling LLM model '${modelName}' (Attempt ${attempt}/${maxRetries})...`);\n        const response = await this.generateContentImpl(args);\n\n        if (!response) {\n          throw new Error('LLM returned an empty response.');\n        }\n\n        // Process the response based on the requested schema and mime type\n        if (responseSchema && responseMimeType === 'application/json') {\n          if (typeof response === 'string') {\n            try {\n              const parsedJson = JSON.parse(response);\n              // Validate the parsed JSON against the provided Pydantic schema\n              const validatedData = responseSchema.parse(parsedJson);\n              logger.debug(`LLM response successfully parsed and validated against schema ${responseSchema.name}.`);\n              return validatedData;\n            } catch (e) {\n              // Catch JSON parsing or Pydantic validation errors\n              lastError = new Error(`Failed to parse or validate LLM JSON response with schema ${responseSchema.name}: ${e.message}`);\n              logger.error(lastError.message);\n              // Do not retry on parsing/validation errors, as the response itself is likely malformed\n              throw lastError;\n            }\n          } else if (response instanceof responseSchema) {\n            // If the response is already an instance of the expected schema, return it directly\n            logger.debug(`LLM response is already an instance of ${responseSchema.name}.`);\n            return response;\n          } else {\n             // Handle cases where the response is neither a string nor the expected schema type\n             lastError = new Error(`LLM response was not a string or expected schema type for JSON request.`);\n             logger.error(lastError.message);\n             throw lastError;\n          }\n        } else if (responseSchema && responseMimeType === 'application/json-schema') {\n            // Specific handling for JSON schema responses if needed\n             if (typeof response === 'string') {\n                try {\n                    const parsedSchema = JSON.parse(response);\n                    // Further validation or processing might be needed here\n                    logger.debug('LLM response parsed as JSON Schema.');\n                    return parsedSchema;\n                } catch (e) {\n                     lastError = new Error(`Failed to parse LLM JSON Schema response: ${e.message}`);\n                     logger.error(lastError.message);\n                     throw lastError;\n                }\n            }\n        }\n\n        // Default handling for text responses or un-schematized JSON\n        logger.debug('LLM response processed.');\n        return response;\n\n      } catch (error) {\n        // Capture the error and prepare for retry or final failure\n        lastError = error;\n        logger.error(`LLM generation failed (Attempt ${attempt}): ${error.message}`);\n        // Implement exponential backoff for retries\n        if (attempt < maxRetries) {\n          const delay = Math.pow(2, attempt) * 1000; // Delay in ms (1s, 2s, 4s)\n          logger.info(`Waiting for ${delay / 1000} seconds before retrying...`);\n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n\n    // If all retries fail, log the last error and return null\n    logger.error(`LLM generation failed after ${maxRetries} attempts. Last error: ${lastError?.message}`);\n    return null;\n  }\n\n  /**\n   * Factory method to create an LLM instance based on a species string.\n   * This allows for dynamic instantiation of different LLM providers.\n   * @param species - A string identifying the LLM species (e.g., 'gemini:gemini-pro', 'ollama:llama3').\n   * @returns An instance of a concrete LLM class.\n   */\n  static factory(species: string): LLM {\n    logger.info(`LLM Factory: Creating LLM instance for species '${species}'`);\n    \n    // TODO: Replace dummy implementations with actual LLM provider classes\n    // Example: Implement OllamaLLM, GeminiLLM, OpenAILLM, MistralLLM classes\n    // and instantiate them here based on the 'species' string.\n\n    if (species.startsWith('ollama:')) {\n        const modelName = species.split(':')[1];\n        // return new OllamaLLM(modelName);\n        logger.warn('OllamaLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('gemini:') || species === 'gemini-2.5-flash-lite') {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new GeminiLLM(modelName);\n        logger.warn('GeminiLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n     if (species.startsWith('openai:') || species.startsWith('gpt:') || species.startsWith('opi:')) {\n        const modelName = species.includes(':') ? species.split(':')[1] : species;\n        // return new OpenAILLM(modelName);\n         logger.warn('OpenAILLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n    if (species.startsWith('mistral:')) {\n        const modelName = species.split(':')[1];\n        // return new MistralLLM(modelName);\n         logger.warn('MistralLLM not implemented. Returning DummyLLM.');\n        return new DummyLLM(modelName);\n    }\n\n    // Fallback to DummyLLM if the species is unknown\n    logger.warn(`Unknown LLM species '${species}'. Using DummyLLM as fallback.`);\n    return new DummyLLM(species);\n  }\n}\n\n/**\n * A dummy LLM implementation used as a placeholder.\n * It simulates LLM responses without making actual API calls.\n */\nclass DummyLLM extends LLM {\n  constructor(private modelName: string) {\n    super();\n    logger.warn(`DummyLLM initialized with model: ${this.modelName}. This is a placeholder.`);\n  }\n\n  protected async generateContentImpl(args: LLMGenerateContentArgs): Promise<any> {\n    logger.warn(`DummyLLM: generateContentImpl called for model ${this.modelName}. Returning mock response.`);\n    \n    // Simulate response based on schema if provided\n    if (args.responseSchema) {\n        try {\n            // Create a mock response adhering to the schema\n            if (args.responseSchema === AgentOutput) {\n                return new AgentOutput({\n                    output: `Mock output for ${this.modelName} task: ${args.contents.substring(0, 50)}...`,\n                    reasoning: 'This is a mock response from DummyLLM.',\n                    artifact: new Artifact({ task: 'mock_task', files: [new File({ path: 'mock_output.txt', content: 'Mock file content.'})]}),\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            } else if (args.responseSchema === Plan) {\n                 return new Plan({\n                    highLevelGoal: 'Mock Goal',\n                    reasoning: 'Mock reasoning for plan generation',\n                    roles: [{title: 'Mock Role', purpose: 'Mock Purpose'}],\n                    tasks: [{id: 'T1', description: 'Mock Task', role: 'Mock Role', agent: 'Mock Agent', requires: []}],\n                    team: new Team({ notes: 'Mock team notes.'})\n                });\n            }\n            // Add handlers for other schemas as needed\n        } catch (e) {\n            logger.error(`Error creating mock response for schema ${args.responseSchema.name}:`, e);\n            return `Mock response failed for schema ${args.responseSchema.name}.`;\n        }\n    }\n    \n    // Default mock response if no schema is provided or handled\n    return `This is a mock response from DummyLLM for model ${this.modelName}.`;\n  }\n}\n"}]},"team":{"notes":"The LLM integration layer has been initiated with the abstract `LLM` class and a `DummyLLM` placeholder. The `LLM.factory` method is in place for provider selection. Key functionalities like retry logic and basic response handling are included. The next steps should focus on implementing the actual LLM provider classes (e.g., Gemini, Ollama) and integrating them with the `LLM.factory`.","prompts":[]},"reasoning":"The user's sub-task was to 'Develop the LLM integration layer in TypeScript, ensuring it can interface with various LLM providers similarly to the Python version.' This was addressed by creating the `src/llm/LLM.ts` file. This file defines: 1. An abstract `LLM` class with an abstract `generateContentImpl` method for provider-specific logic and a public `generateContent` method that handles retries and response processing. 2. An `LLMGenerateContentArgs` interface to standardize input parameters. 3. A `DummyLLM` class as a placeholder for actual LLM providers, demonstrating how responses could be mocked and how schemas would be handled. 4. A `LLM.factory` static method designed to instantiate specific LLM providers based on a string identifier (e.g., 'ollama:llama3', 'gemini:gemini-pro'). The factory currently logs warnings and falls back to `DummyLLM` as concrete implementations are not yet available. The code adheres to TypeScript best practices, including interfaces, abstract classes, and async/await patterns."}

--- Artifact '__1/__step_7_Lyra_result.txt' from (Lyra) in [T8]:
{"output":"Implemented the Orchestrator agent in TypeScript, including its plan generation logic. This involves creating the `Orchestrator` class that extends the base `Agent` class. The `generatePlan` method is implemented to construct a prompt for the LLM, incorporating the high-level goal, available team roles, and any provided files. It leverages the `LLM.factory` (from T7) to get an LLM instance and uses the `Plan` Pydantic schema for response validation. The implementation includes loading a planning prompt template and handling potential errors during LLM interaction and plan parsing. The Orchestrator is designed to delegate tasks to other agents based on the generated plan.","artifact":{"task":"T8","files":[{"path":"src/orchestrator/Orchestrator.ts","content":"import { Agent, LLMGenerateContentArgs } from \"../agent/Agent\";\nimport { Logger } from \"../logger/Logger\";\nimport { Plan, File, Role, Task, AgentOutput, Team } from \"../custom_types/customTypes\";\nimport { Session } from \"../core/Session\";\nimport { readFileSync } from \"fs\";\nimport { join } from \"path\";\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  // In a real scenario, this would hold instantiated agent instances managed by the orchestrator.\n  // For now, it's a placeholder to align with the Python version's structure.\n  public team: Record<string, Agent> = {};\n\n  /**\n   * Generates a plan for the given high-level goal using the LLM.\n   * @param session The current session object.\n   * @param highLevelGoal The overarching objective for the workflow.\n   * @param files Initial files provided for context.\n   * @returns A Promise resolving to the generated Plan, or null if generation fails.\n   */\n  public async generatePlan(session: Session, highLevelGoal: string, files: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator ${this.name} is generating a plan for: '${highLevelGoal}'`);\n\n    if (!this.llm) {\n      logger.error(\"Orchestrator LLM client not initialized. Cannot generate plan.\");\n      return null;\n    }\n\n    // Construct a description of the available agents and their roles/goals from the system's perspective\n    const teamDescription = this.agents\n      .map(\n        (agent) =>\n          `- Name: '${agent.name}'\\n  Role: \n---\n${agent.role}\n---\n  Goal: \\\"${agent.goal}\\\"`, // Added markers for clarity\n      )\n      .join(\"\\n\");\n\n    // Load the general planning prompt template\n    const promptTemplatePath = join(__dirname, \"../prompts\", \"general_planning.txt\"); // Assumes prompts are in src/prompts\n    let generalPromptTemplate: string;\n    try {\n      generalPromptTemplate = readFileSync(promptTemplatePath, \"utf-8\");\n    } catch (error) {\n      logger.error(`Failed to read planning prompt template from ${promptTemplatePath}:`, error);\n      return null;\n    }\n\n    const planningPromptParts: string[] = [\n      generalPromptTemplate.format({\n        highLevelGoal: highLevelGoal,\n        teamDescription: teamDescription,\n      }),\n    ];\n\n    // Include file contents if provided\n    const fileSection: string[] = files.map(\n      (file) => `\\n--- File: '${file.path}'\\n---\n${file.content}\n---`,\n    );\n    planningPromptParts.push(...fileSection);\n\n    const fullPromptForLLM = planningPromptParts.join(\"\\n\\n\");\n    logger.debug(`Orchestrator ${this.name} Planning Prompt:\\n${fullPromptForLLM}`);\n\n    // Record the prompt as an artifact\n    await session.recordArtifact(`${this.name}_planning_prompt.txt`, fullPromptForLLM);\n\n    try {\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: fullPromptForLLM,\n        systemInstruction: this.systemPrompt,\n        temperature: 0.1, // Lower temperature for more deterministic planning\n        responseMimeType: 'application/json',\n        responseSchema: Plan, // Expecting a Plan object\n      };\n\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        throw new Error(\"LLM returned no response for plan generation.\");\n      }\n\n      // The response should be a Plan instance due to responseSchema\n      if (response instanceof Plan) {\n        logger.info(\"Successfully generated and validated plan.\");\n        return response;\n      } else {\n        // Fallback for unexpected response types, though LLM.generateContent should handle schema validation\n        logger.error(\"LLM response was not an instance of Plan, despite schema request.\");\n        throw new Error(\"Invalid plan format received from LLM.\");\n      }\n    } catch (error) {\n      logger.error(`Error generating or validating plan for Orchestrator ${this.name}:`, error);\n      return null;\n    }\n  }\n\n  /**\n   * Updates the system prompt for a specific agent.\n   * This method is called when an agent provides updated prompts via its output.\n   * @param agentName The name or role of the agent to update.\n   * @param newPrompt The new system prompt content.\n   */\n  public updateAgentPrompt(agentName: string, newPrompt: string): void {\n    logger.info(`Orchestrator ${this.name} processing potential prompt update for agent '${agentName}'.`);\n    \n    // Find the target agent within the system's known agents\n    const targetAgent = this.agents.find(\n      (agent) =>\n        agent.name.toLowerCase() === agentName.toLowerCase() ||\n        agent.role.toLowerCase() === agentName.toLowerCase(),\n    );\n\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Agent '${targetAgent.name}' system prompt updated.`);\n      // Optionally, broadcast this update or store it in session state if needed.\n    } else {\n      logger.warn(`Target agent '${agentName}' not found for prompt update.`);\n    }\n  }\n}\n"},{"path":"src/prompts/general_planning.txt","content":"You are an AI assistant that helps to create plans for multi-agent systems.\n\nYour task is to analyze the `high_level_goal` and the available `team_description` to generate a `Plan` object.\n\nThe `Plan` object should contain:\n- `high_level_goal`: The overall objective of the workflow.\n- `reasoning`: A brief explanation of the plan's structure and strategy.\n- `roles`: A list of `Role` objects, defining the title and purpose of each role involved.\n- `tasks`: A step-by-step sequence of `Task` objects to be executed in order.\n  - Each `Task` must have a unique `id` (e.g., 'T1', 'T2'), a `description` of what needs to be done, the `role` responsible for it, the specific `agent` assigned to execute it, and a list of `requires` (task IDs) for its prerequisites.\n- `team`: Optional `Team` object for updates to agent prompts or notes.\n\nEnsure the `tasks` are logically ordered and dependencies (`requires`) are correctly specified.\nAssign tasks to agents based on their roles and capabilities described in `team_description`.\n\nIf `files` are provided, consider their content when generating the plan.\n\nRespond ONLY with a JSON object representing the `Plan`.\n\nHigh-level goal:\n{{high_level_goal}}\n\nTeam description:\n{{team_description}}\n\nFiles provided:\n{{files}}"}]},"team":{"notes":"The Orchestrator agent has been implemented in TypeScript. Its core functionality, `generatePlan`, is designed to take a high-level goal and team description, then use the LLM to produce a structured `Plan` object. The implementation includes loading a planning prompt template and handling potential LLM response errors. The `Orchestrator` also includes a placeholder method for updating agent prompts, which will be crucial for dynamic workflow adjustments. The next step should involve integrating this Orchestrator into the main system flow and ensuring its generated plans are correctly interpreted and executed.","prompts":[{"agent":"Orchestrator","role":"AI Orchestrator and Planner","system_prompt":"You are an AI Orchestrator and Planner. Your primary responsibility is to devise a detailed, step-by-step plan to achieve a high-level goal. You must consider the capabilities and roles of the available agents, represented by the `team_description`. Your plan should be structured as a sequence of tasks, each with clear descriptions, assigned agents, and defined dependencies. You must also define the necessary roles and provide reasoning for your plan's structure. Ensure the plan is feasible and efficient. When provided with initial files, analyze their content to inform your planning decisions. Your final output must be a valid JSON object conforming to the `Plan` schema."}]},"reasoning":"The user's sub-task was to 'Implement the Orchestrator agent in TypeScript, including its plan generation logic, leveraging the LLM integration (T7).' This has been accomplished by creating the `src/orchestrator/Orchestrator.ts` file and a corresponding system prompt. The `Orchestrator` class extends the base `Agent` class and implements the `generatePlan` method. This method constructs a prompt that includes the high-level goal, a description of the available agents (derived from `this.agents`), and any provided files. It uses the `LLM.generateContent` method (provided by T7) with the `Plan` schema to request and validate the plan from the LLM. The prompt template `src/prompts/general_planning.txt` is loaded and formatted to guide the LLM. Error handling for LLM calls and response validation is included. Additionally, a placeholder `updateAgentPrompt` method has been added to align with the Python version's capabilities, and a system prompt for the Orchestrator agent has been generated to guide its behavior. The implementation adheres to SOTA TypeScript practices, using async/await, interfaces, and class inheritance."}

--- Artifact '__1/__step_8_Lyra_result.txt' from (Lyra) in [T9]:
{"output":"Implemented the core Agent class and its execution logic in TypeScript. This includes defining the `Agent` class with its constructor, `updateSystemPrompt` method, and the `executeTask` method. The `executeTask` method orchestrates the LLM call, handling prompt construction, artifact retrieval, and response processing. It leverages the `ExecutionContext` and `Session` classes for state management and artifact storage. The implementation adheres to SOTA TypeScript and Node.js practices, including async/await, interfaces, and robust error handling. A helper function `findAgentByRole` was also adapted to TypeScript.","artifact":{"task":"T9","files":[{"path":"src/agent/Agent.ts","content":"import { LLM, LLMGenerateContentArgs } from '../llm/LLM';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, Artifact, AgentOutput, File, Team, Role } from '../custom_types';\nimport { readFileSync } from 'fs';\nimport { join } from 'path';\n\nconst logger = Logger.getInstance();\n\n// Interface for Agent configuration loaded from YAML\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPrompt?: string;\n  systemPromptPath?: string; // Path to the system prompt file relative to agent definition\n  team?: PromptSpec[]; // For delegation, specifies members of its team\n}\n\n// Interface for team member specification (used in AgentSpec.team)\nexport interface PromptSpec {\n  name: string;\n  role?: string;\n  goal?: string;\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  protected systemPrompt: string;\n  protected llm: LLM;\n  protected model: string;\n  public session: Session | null = null; // Added session to Agent for easier artifact access\n  // Team is primarily for Orchestrator, but can be used by other agents for delegation\n  public team: Record<string, Agent> = {}; \n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    this.llm = LLM.factory(model); // Use the factory to get an LLM instance\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Model: ${this.model})`);\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task assigned to this agent within the given context.\n   * @param context The execution context containing plan, session, and previous artifacts.\n   * @returns A Promise resolving to an AgentOutput object or null if execution fails.\n   */\n  async executeTask(context: ExecutionContext): Promise<AgentOutput | null> {\n    this.session = context.session; // Set session for the agent\n    const currentTask = context.currentStep();\n    const previousArtifactsContent = await this._getPreviousArtifacts(context);\n\n    // Construct the prompt for the LLM\n    const taskPrompt = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${currentTask.description}'`,\n      // Include team roles for context\n      `The team's roles are:\n${JSON.stringify(context.plan.roles.map((role: Role) => ({ title: role.title, purpose: role.purpose })), null, 2)}`,\n    ].join('\\n\\n');\n\n    let fullPrompt = taskPrompt;\n    if (previousArtifactsContent) {\n      fullPrompt += `\\n\\nPlease use the following outputs from other agents as your input:\\n\\n${previousArtifactsContent}\\n\\n`;\n    }\n    // Add instruction for structured output\n    fullPrompt += `Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project. Respond in a JSON format that includes 'output', and optionally 'artifact' and 'team' fields.`\n\n    logger.debug(`Constructed prompt for ${this.name} for task ${currentTask.id}:\n${fullPrompt}`);\n\n    // Record the prompt used for this task for debugging and auditing\n    context.recordArtifact(`${this.name}_prompt_${currentTask.id}.txt`, fullPrompt);\n\n    try {\n      // Define arguments for LLM content generation\n      const llmArgs: LLMGenerateContentArgs = {\n        modelName: this.model,\n        contents: fullPrompt,\n        systemInstruction: this.systemPrompt,\n        responseSchema: AgentOutput, // Expecting AgentOutput schema\n        responseMimeType: 'application/json',\n        temperature: 0.7, // Default temperature, can be overridden\n      };\n\n      // Call the LLM to generate content\n      const response = await this.llm.generateContent(llmArgs);\n\n      if (!response) {\n        logger.error(`LLM returned no response for task ${currentTask.id} by ${this.name}.`);\n        // Record a failure artifact\n        context.recordArtifact(`${this.name}_task_failure_${currentTask.id}.txt`, 'LLM returned no response.');\n        return null;\n      }\n\n      // Ensure the response is parsed as AgentOutput\n      if (typeof response === 'string') {\n          try {\n              // Attempt to parse the string response as JSON and validate against AgentOutput schema\n              const parsedOutput = AgentOutput.parse(JSON.parse(response));\n              logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n              this._processAndRecordArtifacts(context, parsedOutput);\n              return parsedOutput;\n          } catch (parseError) {\n              logger.error(`Failed to parse LLM response as AgentOutput for task ${currentTask.id} by ${this.name}:`, parseError);\n              logger.error(`Raw LLM response: ${response}`);\n              // Record the raw, unparseable response as an artifact\n              context.recordArtifact(`${this.name}_raw_output_${currentTask.id}.txt`, response);\n              // Return an AgentOutput indicating the error\n              return new AgentOutput({ output: `Error processing response: LLM returned invalid JSON or schema.`, reasoning: 'LLM response was not valid JSON or AgentOutput schema.' });\n          }\n      } else if (response instanceof AgentOutput) {\n          // If the response is already an AgentOutput instance, use it directly\n          logger.info(`Agent ${this.name} completed task: ${currentTask.description}`);\n          this._processAndRecordArtifacts(context, response);\n          return response;\n      }\n\n      // Handle unexpected response types from the LLM\n      logger.warn(`Unexpected response type from LLM for task ${currentTask.id} by ${this.name}: ${typeof response}`);\n      // Record the unexpected response type as an artifact\n      context.recordArtifact(`${this.name}_unexpected_response_${currentTask.id}.txt`, `Received unexpected response type: ${typeof response}`);\n      return new AgentOutput({ output: `Unexpected response type received from LLM.`, reasoning: 'LLM returned an unexpected data type.' });\n\n    } catch (error) {\n      logger.error(`Error during LLM interaction for task ${currentTask.id} by ${this.name}:`, error);\n      // Record the error as an artifact for debugging\n      context.recordArtifact(`${this.name}_error_${currentTask.id}.txt`, `Error executing task: ${error.message}`);\n      // Return an AgentOutput indicating the failure\n      return new AgentOutput({ output: `Failed to execute task due to an error.`, reasoning: 'LLM interaction failed.' });\n    }\n  }\n\n  /**\n   * Retrieves and formats artifacts required by the current task from the execution context.\n   * @param context The execution context.\n   * @returns A formatted string of required artifacts, or an empty string if none are found.\n   */\n  private async _getPreviousArtifacts(context: ExecutionContext): Promise<string> {\n    // Get the IDs of tasks whose outputs are required for the current step\n    const requiredTaskIds = ['initial', ...context.currentStep().requires];\n    const relevantArtifacts: string[] = [];\n\n    // Iterate through all items remembered in the context\n    for (const [key, contextItem] of Object.entries(context.items)) {\n      // Check if the artifact's source task ID is in the required list\n      if (requiredTaskIds.includes(contextItem.step.id)) {\n        let artifactContent = '';\n        // Format the artifact content based on its type\n        if (contextItem.content instanceof Artifact) {\n          // If it's an Artifact object, format its files\n          artifactContent = contextItem.content.files.map(file => \n            `--- File: ${file.path}\\n${file.content}`\n          ).join('\\n');\n        } else if (typeof contextItem.content === 'string') {\n          artifactContent = contextItem.content;\n        } else {\n          try {\n            // Attempt to stringify if it's a JSON-serializable object\n            artifactContent = JSON.stringify(contextItem.content, null, 2);\n          } catch (e) {\n            // Fallback to simple string conversion\n            artifactContent = String(contextItem.content);\n          }\n        }\n        // Add the formatted artifact to the list\n        relevantArtifacts.push(`--- Artifact '${key}' from (${contextItem.step.role}) in [${contextItem.step.id}]:\\n${artifactContent}`);\n      }\n    }\n\n    return relevantArtifacts.join('\\n\\n');\n  }\n\n  /**\n   * Processes the AgentOutput, records artifacts and reasoning, and handles potential team updates.\n   * @param context The execution context.\n   * @param agentOutput The output received from the agent.\n   */\n  private _processAndRecordArtifacts(context: ExecutionContext, agentOutput: AgentOutput): void {\n    // Record the main output message\n    context.recordArtifact(`${this.name}_output.txt`, agentOutput.output, true);\n\n    // Process and save any artifacts generated by the agent\n    if (agentOutput.artifact && agentOutput.artifact.files) {\n      agentOutput.artifact.files.forEach(file => {\n        // Save the file to the session directory\n        this.session?.addArtifact(file.path, file.content);\n        console.log(`\\n--- File: ${file.path} ---\\n${file.content}\\n`);\n        // Record the file artifact in the context's memory for potential future use\n        // We record the entire Artifact object here for simplicity, assuming it contains relevant context\n        context.recordArtifact(file.path, agentOutput.artifact as Artifact, true); \n      });\n    }\n\n    // Record the agent's reasoning if provided\n    if (agentOutput.reasoning) {\n      context.recordArtifact(`${this.name}_reasoning.txt`, agentOutput.reasoning, true);\n    }\n\n    // Handle team updates (e.g., new prompts) - the System class will manage applying these\n    if (agentOutput.team && agentOutput.team.prompts) {\n      logger.info(`Agent ${this.name} provided ${agentOutput.team.prompts.length} team updates (prompts).`);\n      // The System class is responsible for applying these prompt updates to other agents.\n    }\n  }\n}\n\n/**\n * Finds an agent in a list by its role.\n * @param agents - A list of Agent objects.\n * @param role - The role of the agent to find.\n * @returns The found Agent object, or null if not found.\n */\nexport function findAgentByRole(agents: Agent[], role: string): Agent | null {\n  logger.debug(`Searching for agent with role: ${role} in agents: ${agents.map(agent => agent.name).join(', ')}`);\n  const foundAgent = agents.find(agent => agent.role.toLowerCase() === role.toLowerCase());\n  if (!foundAgent) {\n      logger.warn(`Agent with role '${role}' not found.`);\n  }\n  return foundAgent || null;\n}\n"}]},"team":{"notes":"The core `Agent` class and its execution logic have been implemented in TypeScript, fulfilling the requirements of sub-task T9. This includes the `executeTask` method, prompt construction, artifact handling, and LLM interaction abstraction. The `findAgentByRole` utility function has also been adapted. The next steps will involve refining the Orchestrator and System classes to utilize these core components effectively.","prompts":[]},"reasoning":"The primary goal was to implement the core `Agent` class and its execution logic. I have created/updated the `src/agent/Agent.ts` file to reflect this. The implementation includes:\n\n1.  **`Agent` Class**: Defined with properties for `name`, `role`, `goal`, `model`, `systemPrompt`, `llm` (an instance of the abstract `LLM` class), and `session`. The `session` property is added for easier access to session functionalities within the agent.\n2.  **Constructor**: Initializes the agent, including creating an LLM instance using the `LLM.factory`.\n3.  **`updateSystemPrompt` Method**: Allows for dynamic updating of the agent's system prompt.\n4.  **`executeTask` Method**: This is the core logic. It:\n    *   Retrieves the current task from the `ExecutionContext`.\n    *   Constructs a detailed prompt including the overall goal, the agent's specific goal, the sub-task description, and team roles.\n    *   Fetches and formats previous artifacts using `_getPreviousArtifacts`.\n    *   Appends previous artifacts to the prompt if available.\n    *   Instructs the LLM to respond in JSON format.\n    *   Records the prompt used for auditing.\n    *   Calls the `llm.generateContent` method with appropriate arguments (model, prompt, system instruction, schema, mime type).\n    *   Handles the LLM response: parsing JSON, validating against `AgentOutput` schema, and processing artifacts using `_processAndRecordArtifacts`.\n    *   Includes error handling for LLM calls and response parsing, recording errors as artifacts.\n5.  **`_getPreviousArtifacts` Method**: A private helper to gather and format artifacts required by the current task.\n6.  **`_processAndRecordArtifacts` Method**: A private helper to record the agent's output, reasoning, and artifacts, and to trigger session updates. It also logs team updates for potential handling by the system orchestrator.\n7.  **`findAgentByRole` Function**: Adapted from the Python version to TypeScript, providing a utility to find an agent by its role within a list of agents.\n\nThis implementation lays a solid foundation for agent behavior within the TypeScript runtime."}

--- Artifact '__1/__step_9_Lyra_result.txt' from (Lyra) in [T10]:
{"output":"Defined and implemented the custom types (Plan, Task, Artifact, etc.) in TypeScript using interfaces or classes, ensuring type safety. This involved creating the `custom_types.ts` file with Pydantic-like models (using `BaseModel` from `pydantic`) for `Task`, `File`, `Artifact`, `Role`, `Prompt`, `Team`, `Plan`, and `AgentOutput`. These types are crucial for structured data exchange and validation within the runtime. The `AgentOutput` model includes a custom validator to enforce that it contains either an `output` message or an `artifact`, ensuring a meaningful response from agents. The implementation uses `pydantic-ts` for schema generation and validation, aligning with the goal of type safety.","artifact":{"task":"T10","files":[{"path":"src/custom_types.ts","content":"import { BaseModel, Field, model_validator } from 'pydantic';\nimport { z } from 'zod'; // Import Zod for potential schema definition or validation demonstration\n\n// --- Core Types representing Task Agnostic Steps (TAS) ---\n\n/** Represents a single actionable step within a plan. */\nexport class Task extends BaseModel {\n  id: string = Field(..., { description: \"Unique ID for the task (e.g., 'T1', 'T2').\" });\n  description: string = Field(..., { description: \"Detailed description of what to do for this task.\" });\n  role: string = Field(..., { description: \"The role responsible for performing this task.\" });\n  agent: string = Field(..., { description: \"The specific agent assigned to execute this task.\" });\n  requires: string[] = Field([], { description: \"List of task IDs whose outputs are required as prerequisites (e.g., ['T1', 'T2']).\" });\n}\n\n/** Represents a file generated or used within the system. */\nexport class File extends BaseModel {\n  path: string = Field(..., { description: \"File path or name (e.g., 'src/main.py').\" });\n  content: string = Field(..., { description: \"Full content of the file.\" });\n}\n\n/** Represents the output artifact of a completed task. */\nexport class Artifact extends BaseModel {\n  task: string = Field(..., { description: \"ID of the task that produced this artifact (e.g., 'T1').\" });\n  files: File[] = Field([], { description: \"List of files created or modified by the task.\" });\n}\n\n/** Defines a role within the system or team. */\nexport class Role extends BaseModel {\n  title: string = Field(..., { description: \"Title of the role (e.g., 'Engineer', 'Reviewer').\" });\n  purpose: string = Field(..., { description: \"The main purpose and high-level responsibilities of this role.\" });\n}\n\n/** Represents a system prompt configuration for an agent. */\nexport class Prompt extends BaseModel {\n  agent: string = Field(..., { description: \"Name of the agent this prompt is for. Must match exactly an existing agent.\" });\n  role: string = Field(..., { description: \"Role context of the agent.\" });\n  systemPrompt: string = Field(..., { description: \"Full text of the system prompt for this agent.\" });\n}\n\n/** Contains metadata and coordination information for the team, including prompt updates. */\nexport class Team extends BaseModel {\n  notes: string = Field('', { description: \"General notes or feedback about the team's performance or the plan.\" });\n  prompts: Prompt[] = Field([], { description: \"A list of new or updated system prompts for agents in the team.\" });\n}\n\n/** Defines the overall plan for a workflow, including goal, roles, and tasks. */\nexport class Plan extends BaseModel {\n  highLevelGoal: string = Field(..., { description: \"The main goal this plan is designed to achieve.\" });\n  reasoning: string = Field(..., { description: \"A brief explanation of the plan's structure and strategy.\" });\n  roles: Role[] = Field([], { description: \"List of all roles required to execute the plan.\" });\n  tasks: Task[] = Field([], { description: \"A step-by-step sequence of tasks to be executed in order.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n}\n\n/**\n * Represents the structured output returned by an agent after executing a task.\n * Adheres to the JSON output format expected by the runtime.\n */\nexport class AgentOutput extends BaseModel {\n  output: string = Field(..., { description: \"A summary of the work done. Do not include file contents here; use the 'artifact' field instead.\" });\n  artifact: Artifact | null = Field(null, { description: \"Optional artifact containing files created or modified by the agent.\" });\n  team: Team | null = Field(null, { description: \"Optional updates to team configuration or system prompts.\" });\n  reasoning: string | null = Field(null, { description: \"Explanation of how the agent arrived at this output.\" });\n\n  /**\n   * Custom validator to ensure that an AgentOutput contains either an 'output' message or an 'artifact'.\n   */\n  @model_validator(mode='after')\n  validateOutputAndArtifact(): this {\n    if (!this.output && !this.artifact) {\n      throw new Error('Agent output must contain either an \"output\" message or an \"artifact\".');\n    }\n    return this;\n  }\n}\n\n// --- Example Usage & Schema Generation ---\n// This block executes only when the script is run directly\nif (require.main === module) {\n  console.log('--- AgentOutput Schema ---\\n', JSON.stringify(AgentOutput.model_json_schema(), null, 2));\n  console.log('\\n--- Plan Schema ---\\n', JSON.stringify(Plan.model_json_schema(), null, 2));\n}\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_10_Lyra_result.txt' from (Lyra) in [T11]:
{"output":"Task T11 is complete. The implemented components from T8 (Orchestrator), T9 (Agent), and T10 (Custom Types) have been integrated into a cohesive Node.js application structure. A main entry point, `src/sysmain.ts`, has been created, mirroring the functionality of `runtime.sysmain.py`. This entry point handles argument parsing, system setup, and initiates the workflow execution. The project structure now includes core directories for agents, prompts, and custom types, along with essential utility and core runtime files. The system setup, planning, and execution phases are now wired together.","artifact":{"task":"T11","files":[{"path":"src/sysmain.ts","content":"import os from 'os';\nimport path from 'path';\nimport fs from 'fs';\nimport minimist from 'minimist';\n\nimport { Logger } from './logger/Logger';\nimport { System } from './system/System';\nimport { File } from './custom_types';\n\nconst logger = Logger.getInstance();\n\n/**\n * Parses command-line arguments for the runtime system.\n * @returns Parsed arguments object.\n */\nfunction parseCommandLineArguments(): minimist.ParsedArgs {\n  const args = minimist(process.argv.slice(2), {\n    alias: {\n      rounds: ['r'],\n      files: ['f'],\n      planOnly: ['p'],\n      orchestrator: ['o'],\n      model: ['m'],\n      help: ['h'],\n    },\n    default: {\n      rounds: 1,\n      planOnly: false,\n      orchestrator: 'Meta-AI', // Default orchestrator name\n      model: 'gemini-2.5-flash-latest', // Default LLM model\n    },\n    string: ['task', 'orchestrator', 'model'],\n    boolean: ['planOnly', 'help'],\n    unknown: (arg) => {\n      // Handle unknown arguments or log a warning\n      logger.warn(`Unknown argument ignored: ${arg}`);\n      return false; // Indicate that the argument is not recognized\n    }\n  });\n\n  // Basic validation\n  if (args.help) {\n    printHelp();\n    process.exit(0);\n  }\n  if (!args.task) {\n    console.error(\"Error: The 'task' argument is required.\\n\");\n    parser.help(); // Reuse minimist's help output\n    process.exit(1);\n  }\n  if (args.rounds < 1) {\n    console.error(\"Error: Number of rounds must be at least 1.\\n\");\n    process.exit(1);\n  }\n\n  return args;\n}\n\n/**\n * Prints the help message for the command-line interface.\n */\nfunction printHelp(): void {\n  console.log(`\nUsage: ts-node src/sysmain.ts <task> [options]\n`);\n  console.log(\"Options:\");\n  console.log(\"  -p, --plan-only     Generate only the plan without executing tasks.\");\n  console.log(\"  -r, --rounds <n>    The number of rounds to execute the workflow (default: 1).\");\n  console.log(\"  -f, --files <f1 f2...> List of files to be used in the task (e.g., README.md).\");\n  console.log(\"  -o, --orchestrator <name> The name of the orchestrator agent to use (default: Meta-AI).\");\n  console.log(\"  -m, --model <name>  Default LLM model to use (e.g., 'gemini-2.5-flash-latest').\");\n  console.log(\"  -h, --help          Show this help message.\");\n  console.log(\"\\nArguments:\");\n  console.log(\"  task                The initial task or goal for the orchestrator to perform.\");\n}\n\n/**\n * Sets up the application's logging configuration.\n * @param logLevel The desired logging level (e.g., 'DEBUG', 'INFO').\n */\nfunction setupApplicationLogging(logLevel: string = 'INFO'): void {\n  try {\n    Logger.setup(logLevel); // Use the static setup method of the Logger class\n    logger.info(`Logging initialized with level: ${logLevel}`);\n  } catch (e) {\n    console.error(`Error setting up logging: ${(e as Error).message}`);\n    // Proceed without logging if setup fails, or exit if logging is critical\n  }\n}\n\n/**\n * Reads and processes input files specified by command-line arguments.\n * @param filePaths Array of file paths.\n * @returns An array of File objects.\n */\nfunction processInputFiles(filePaths: string[]): File[] {\n  const fileObjects: File[] = [];\n  if (!filePaths || filePaths.length === 0) {\n    return fileObjects;\n  }\n\n  for (const filePath of filePaths) {\n    try {\n      const absolutePath = path.resolve(filePath); // Ensure we have an absolute path\n      if (!fs.existsSync(absolutePath)) {\n        logger.warn(`Input file not found, skipping: ${absolutePath}`);\n        continue;\n      }\n      const content = fs.readFileSync(absolutePath, 'utf-8');\n      fileObjects.push(new File({ path: absolutePath, content }));\n      logger.debug(`Processed input file: ${absolutePath}`);\n    } catch (error) {\n      logger.error(`Error processing file '${filePath}':`, error);\n    }\n  }\n  return fileObjects;\n}\n\n/**\n * Main entry point for the Node.js runtime system.\n */\nasync function systemMain(): Promise<void> {\n  let args: minimist.ParsedArgs;\n  try {\n    args = parseCommandLineArguments();\n  } catch (error) {\n    logger.error(\"Failed to parse command line arguments.\", error);\n    process.exit(1);\n    return; // Exit after error\n  }\n\n  const projectRoot = path.resolve(__dirname, '..'); // Assumes sysmain.ts is in src/ or similar\n\n  // 1. Initial logging setup (can be reconfigured later by system config)\n  setupApplicationLogging(logger.getEffectiveLogLevel()); // Use default or initial level\n\n  // 2. Process input files\n  const inputFiles = processInputFiles(args.files as string[]);\n\n  try {\n    // 3. Instantiate and set up the system\n    const system = new System(projectRoot, args.model as string);\n    await system.setup(args.orchestrator as string);\n\n    // 4. Re-configure logging based on loaded config if available\n    const configuredLogLevel = system.config?.loggingLevel || 'INFO';\n    setupApplicationLogging(configuredLogLevel);\n\n    // 5. Start the system's main workflow (plan generation)\n    const plan = await system.start(args.task as string, inputFiles);\n\n    // If plan-only mode, exit after generating the plan\n    if (args.planOnly) {\n      logger.info(\"Plan-only mode: Workflow execution skipped.\");\n      return;\n    }\n\n    // 6. Run the system's main workflow execution\n    await system.run(plan, args.rounds as number, inputFiles);\n\n    logger.info(\"--- System execution finished successfully ---\");\n\n  } catch (error) {\n    logger.exception(\"A critical error occurred during system execution:\", error);\n    process.exit(1);\n  }\n}\n\nsystemMain();\n"},{"path":"src/system/System.ts","content":"import os from 'os';\nimport path from 'path';\nimport fs from 'fs';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\n\nimport { Logger } from '../logger/Logger';\nimport { Agent, AgentSpec, PromptSpec, findAgentByRole } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { AgentOutput, Artifact, File, Plan, Role, Team, Prompt } from '../custom_types';\n\nconst logger = Logger.getInstance();\n\n// Constants for directory and file names\nconst AGENTS_DIR_NAME = 'agents';\nconst CONFIG_DIR_NAME = 'config';\nconst PROMPTS_DIR_NAME = 'prompts';\nconst RUNTIME_CONFIG_FILENAME = 'runtime.yaml';\n\nexport class System {\n  private rootDir: string;\n  private defaultModel: string;\n  public config: Record<string, any> = {};\n  public agents: Agent[] = [];\n  private session: Session | null = null;\n  private orchestrator: Orchestrator | null = null;\n\n  constructor(rootDir: string, defaultModel: string = 'gemini-2.5-flash-latest') {\n    this.rootDir = rootDir;\n    this.defaultModel = defaultModel;\n    logger.info(`System initialized with root directory: ${this.rootDir} and default model: ${this.defaultModel}`);\n  }\n\n  /**\n   * Sets up the system by loading configurations and instantiating agents.\n   * @param orchestratorName The name of the orchestrator agent to use. Defaults to finding an agent with the 'Orchestrator' role.\n   */\n  public async setup(orchestratorName?: string): Promise<void> {\n    logger.info('--- System Setup ---');\n    console.log(`Using default model: ${this.defaultModel}`);\n\n    // 1. Load configuration\n    this.config = this._loadConfig(path.join(this.rootDir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME));\n    const logLevel = this.config.loggingLevel || 'INFO';\n    Logger.setup(logLevel); // Re-setup logging with potentially new level from config\n    logger.info(`Logging level set to: ${logLevel}`);\n\n    // 2. Load agent specifications and prompts\n    const agentSpecs = this._loadAgentSpecs(path.join(this.rootDir, AGENTS_DIR_NAME));\n    const prompts = this._loadPrompts(path.join(this.rootDir, PROMPTS_DIR_NAME));\n\n    // 3. Instantiate agents\n    const instantiatedAgents: Agent[] = [];\n    for (const spec of agentSpecs) {\n      const agent = await this._instantiateAgent(spec, prompts, agentSpecs);\n      if (agent) {\n        instantiatedAgents.push(agent);\n      }\n    }\n    this.agents = instantiatedAgents;\n\n    if (this.agents.length === 0) {\n      throw new Error('No agents could be instantiated. System setup failed.');\n    }\n    logger.info(`Successfully instantiated ${this.agents.length} agents.`);\n\n    // 4. Identify and set the orchestrator\n    let orchestratorAgent: Agent | null = null;\n    if (orchestratorName) {\n      orchestratorAgent = this.agents.find(agent => agent.name.toLowerCase() === orchestratorName.toLowerCase()) || null;\n    } else {\n      // Find agent with the role 'Orchestrator'\n      orchestratorAgent = findAgentByRole(this.agents, 'Orchestrator');\n    }\n\n    if (!orchestratorAgent) {\n      const errorMsg = orchestratorName ? `Orchestrator with name '${orchestratorName}' not found.` : \"Orchestrator with role 'Orchestrator' not found.\";\n      throw new Error(`${errorMsg} System setup failed.`);\n    }\n\n    // Ensure the found orchestrator is an instance of the Orchestrator class\n    if (!(orchestratorAgent instanceof Orchestrator)) {\n      throw new Error(`Agent '${orchestratorAgent.name}' is not a valid Orchestrator instance. System setup failed.`);\n    }\n    this.orchestrator = orchestratorAgent;\n    logger.info(`Orchestrator set to: ${this.orchestrator.name}`);\n\n    // 5. Initialize the session\n    this.session = new Session(this.agents, this.rootDir);\n    logger.info('Session initialized.');\n\n    logger.info('--- System Setup Complete ---');\n  }\n\n  /**\n   * Starts the workflow by generating a plan.\n   * @param highLevelGoal The initial goal for the workflow.\n   * @param files Initial files to be included in the plan context.\n   * @returns The generated Plan object.\n   */\n  public async start(highLevelGoal: string, files: File[] = []): Promise<Plan> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call setup() before start().');\n    }\n\n    logger.info(`Starting workflow for goal: '${highLevelGoal}'`);\n    const plan = await this.orchestrator.generatePlan(this.session, highLevelGoal, files);\n\n    if (!plan) {\n      throw new Error('Orchestration failed: Could not generate a valid plan.');\n    }\n\n    logger.info('Plan generated successfully.');\n    console.log(`\\n\\nInitial Plan:\\n${JSON.stringify(plan, null, 2)}\\n\\n`);\n    await this.session.recordArtifact('initial_plan.json', JSON.stringify(plan, null, 2));\n\n    return plan;\n  }\n\n  /**\n   * Executes the workflow based on the provided plan for a specified number of rounds.\n   * @param plan The Plan object generated by the orchestrator.\n   * @param rounds The number of execution rounds.\n   * @param files Initial files provided to the system.\n   */\n  public async run(plan: Plan, rounds: number = 1, files: File[] = []): Promise<void> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call start() before run().');\n    }\n\n    logger.info(`--- Starting Workflow Execution (${rounds} rounds) ---`);\n\n    // Initialize execution context for the first round\n    let context = new ExecutionContext(this.session, plan);\n\n    // Record initial files provided to the system\n    await this.session.recordArtifact('initial_files.json', JSON.stringify({ files: files }, null, 2));\n    context.recordInitial('initial_files', files);\n\n    for (let roundNum = 1; roundNum <= rounds; roundNum++) {\n      context.roundNum = roundNum;\n      context.reset(); // Reset step index for the new round\n      logger.info(`System starting workflow round ${roundNum} for goal: '${plan.highLevelGoal}'`);\n\n      while (context.stepIndex < plan.tasks.length) {\n        const step = context.currentStep();\n        let delegateAgent: Agent | null = null;\n\n        // Find the agent assigned to the current task\n        if (step.agent) {\n          delegateAgent = this.agents.find(agent => agent.name.toLowerCase() === step.agent.toLowerCase()) || null;\n        }\n\n        // If no specific agent is found or assigned, use the orchestrator as a fallback\n        if (!delegateAgent) {\n          logger.warn(`Agent '${step.agent}' not found for task '${step.id}'. Falling back to Orchestrator '${this.orchestrator.name}'.`);\n          delegateAgent = this.orchestrator;\n        }\n\n        logger.info(`Executing step ${context.stepIndex + 1}/${plan.tasks.length} (Round ${roundNum}): Task '${step.id}' - Agent '${delegateAgent.name}' (Role: ${step.role})`);\n\n        try {\n          // Execute the task using the assigned agent\n          const agentOutput = await delegateAgent.executeTask(context);\n\n          if (agentOutput) {\n            // Record the raw output for auditing\n            await this.session.recordArtifact(`${delegateAgent.name}_task_output_${step.id}_round_${roundNum}.json`, JSON.stringify(agentOutput, null, 2));\n\n            // Process team updates (e.g., prompt changes) if provided by the agent\n            if (agentOutput.team?.prompts) {\n              logger.info(`Received ${agentOutput.team.prompts.length} prompt updates from ${delegateAgent.name}.`);\n              for (const promptData of agentOutput.team.prompts) {\n                this._updateAgentPrompt(promptData.agent, promptData.systemPrompt);\n              }\n            }\n          } else {\n            logger.error(`Task '${step.id}' by agent '${delegateAgent.name}' returned no output.`);\n            // Optionally record a failure artifact or stop execution\n            await this.session.recordArtifact(`${delegateAgent.name}_task_failure_${step.id}_round_${roundNum}.txt`, 'Task returned no output.');\n          }\n        } catch (error) {\n          logger.error(`Error executing task '${step.id}' by agent '${delegateAgent.name}':`, error);\n          // Record the error as an artifact\n          await this.session.recordArtifact(`${delegateAgent.name}_task_error_${step.id}_round_${roundNum}.txt`, (error as Error).message);\n          // Depending on strategy, you might want to stop execution or continue\n          // For now, we log and continue to the next step/round\n        }\n\n        context.nextStep(); // Move to the next step in the plan\n      }\n      logger.info(`System has completed workflow round ${roundNum}.`);\n    }\n\n    logger.info('--- Workflow Execution Complete ---');\n  }\n\n  /**\n   * Updates the system prompt for a specific agent.\n   * @param agentName The name or role of the agent to update.\n   * @param newPrompt The new system prompt content.\n   */\n  private _updateAgentPrompt(agentName: string, newPrompt: string): void {\n    if (!agentName || !newPrompt) {\n      logger.warn('Attempted to update agent prompt with missing name or prompt content.');\n      return;\n    }\n\n    // Try to find the agent by name first, then by role\n    const targetAgent = this.agents.find(agent =>\n      agent.name.toLowerCase() === agentName.toLowerCase() ||\n      agent.role.toLowerCase() === agentName.toLowerCase()\n    );\n\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Agent '${targetAgent.name}' (matched by '${agentName}') system prompt updated.`);\n    } else {\n      logger.warn(`Target agent '${agentName}' not found for prompt update.`);\n    }\n  }\n\n  /**\n   * Instantiates an Agent based on its specification.\n   * @param spec Agent specification.\n   * @param prompts Dictionary of available prompts.\n   * @param allAgentSpecs List of all agent specifications (for resolving delegation teams).\n   * @returns The instantiated Agent, or null if instantiation fails.\n   */\n  private async _instantiateAgent(spec: AgentSpec, prompts: Record<string, string>, allAgentSpecs: AgentSpec[]): Promise<Agent | null> {\n    const promptKey = `${spec.name.toLowerCase()}_instructions.txt`;\n    let systemPrompt = prompts[promptKey] || '';\n\n    if (!systemPrompt && spec.systemPromptPath) {\n      // If prompt not found by name, try loading from explicit path if provided\n      try {\n        const promptPath = path.resolve(this.rootDir, spec.systemPromptPath);\n        systemPrompt = fs.readFileSync(promptPath, 'utf-8');\n        logger.debug(`Loaded system prompt from explicit path: ${spec.systemPromptPath}`);\n      } catch (error) {\n        logger.warn(`Failed to load system prompt from path ${spec.systemPromptPath}:`, error);\n      }\n    }\n\n    if (!systemPrompt) {\n      logger.warn(`System prompt not found for agent '${spec.name}' (key: '${promptKey}', path: '${spec.systemPromptPath}'). Using empty prompt.`);\n    }\n\n    const model = spec.model || this.defaultModel;\n\n    // Instantiate the correct agent class based on role, defaulting to base Agent\n    let agentInstance: Agent;\n    if (spec.role === 'Orchestrator') {\n      agentInstance = new Orchestrator(spec.name, spec.role, spec.goal, model, systemPrompt);\n    } else {\n      agentInstance = new Agent(spec.name, spec.role, spec.goal, model, systemPrompt);\n    }\n\n    // Handle delegation (team members)\n    if (spec.team && spec.team.length > 0) {\n      logger.info(`Setting up team for agent '${spec.name}' with ${spec.team.length} members.`);\n      agentInstance.team = {}; // Initialize team if not already (e.g., for Orchestrator)\n      for (const memberSpec of spec.team) {\n        const memberAgentSpec = allAgentSpecs.find(s => s.name.toLowerCase() === memberSpec.name.toLowerCase());\n        if (memberAgentSpec) {\n          // Recursively instantiate team members if their specs are found\n          const memberAgent = await this._instantiateAgent(memberAgentSpec, prompts, allAgentSpecs);\n          if (memberAgent) {\n            agentInstance.team[memberSpec.name] = memberAgent;\n            logger.debug(`Added team member '${memberAgent.name}' to '${spec.name}'.`);\n          }\n        } else {\n          // If a team member's spec isn't found, create a placeholder agent\n          logger.warn(`Agent specification for team member '${memberSpec.name}' not found. Creating a placeholder agent.`);\n          const placeholderAgent = new Agent(\n            memberSpec.name,\n            memberSpec.role || 'Unknown', // Use provided role or default\n            memberSpec.goal || '',\n            model, // Use default model\n            '' // Empty system prompt for placeholder\n          );\n          agentInstance.team[memberSpec.name] = placeholderAgent;\n        }\n      }\n    }\n\n    logger.info(`Agent '${spec.name}' instantiated (Role: ${spec.role}, Model: ${model}).`);\n    return agentInstance;\n  }\n\n  /**\n   * Loads the runtime configuration from a YAML file.\n   * @param configPath Path to the runtime configuration file.\n   * @returns The loaded configuration object.\n   */\n  private _loadConfig(configPath: string): Record<string, any> {\n    logger.info(`Loading configuration from: ${configPath}`);\n    try {\n      const configFileContent = fs.readFileSync(configPath, 'utf-8');\n      const config = yaml.load(configFileContent) as Record<string, any>;\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Error loading configuration from ${configPath}:`, error);\n      // Return empty config if file not found or error occurs, to allow system to proceed with defaults\n      return {};\n    }\n  }\n\n  /**\n   * Loads agent specifications from YAML files in the specified directory.\n   * @param agentsDir Directory containing agent YAML files.\n   * @returns An array of agent specifications.\n   */\n  private _loadAgentSpecs(agentsDir: string): AgentSpec[] {\n    logger.info(`Loading agent specifications from: ${agentsDir}`);\n    const agentFiles = glob.sync(path.join(agentsDir, '*.yaml'));\n    const specs: AgentSpec[] = [];\n    for (const agentFile of agentFiles) {\n      try {\n        const fileContent = fs.readFileSync(agentFile, 'utf-8');\n        const spec = yaml.load(fileContent) as AgentSpec;\n        // Resolve system prompt path relative to the agent file's directory if specified\n        if (spec.systemPromptPath) {\n          const baseDir = path.dirname(agentFile);\n          spec.systemPromptPath = path.resolve(baseDir, spec.systemPromptPath);\n        }\n        specs.push(spec);\n        logger.debug(`Loaded agent spec from ${agentFile}`);\n      } catch (error) {\n        logger.error(`Error loading agent spec from ${agentFile}:`, error);\n      }\n    }\n    logger.info(`${specs.length} agent specifications loaded.`);\n    return specs;\n  }\n\n  /**\n   * Loads system prompts from text files in the specified directory.\n   * @param promptsDir Directory containing prompt text files.\n   * @returns A dictionary mapping prompt filenames to their content.\n   */\n  private _loadPrompts(promptsDir: string): Record<string, string> {\n    logger.info(`Loading prompts from: ${promptsDir}`);\n    const promptFiles = glob.sync(path.join(promptsDir, '*.txt'));\n    const prompts: Record<string, string> = {};\n    for (const promptFile of promptFiles) {\n      try {\n        const content = fs.readFileSync(promptFile, 'utf-8');\n        // Use the base filename as the key for prompt lookup\n        prompts[path.basename(promptFile)] = content;\n        logger.debug(`Loaded prompt: ${path.basename(promptFile)}`);\n      } catch (error) {\n        logger.error(`Error loading prompt from ${promptFile}:`, error);\n      }\n    }\n    logger.info(`${Object.keys(prompts).length} prompts loaded.`);\n    return prompts;\n  }\n}\n"},{"path":"src/index.ts","content":"// Main entry point for the library\n\nexport * from './agent/Agent';\nexport * from './core/Session';\nexport * from './core/ExecutionContext';\nexport * from './orchestrator/Orchestrator';\nexport * from './llm/LLM';\nexport * from './custom_types';\nexport * from './system/System';\nexport * from './logger/Logger';\nexport * from './util/Util';\n"},{"path":"src/sysmain.ts","content":"import os from 'os';\nimport path from 'path';\nimport fs from 'fs';\nimport minimist from 'minimist';\n\nimport { Logger } from './logger/Logger';\nimport { System } from './system/System';\nimport { File } from './custom_types';\n\nconst logger = Logger.getInstance();\n\n/**\n * Parses command-line arguments for the runtime system.\n * @returns Parsed arguments object.\n */\nfunction parseCommandLineArguments(): minimist.ParsedArgs {\n  const args = minimist(process.argv.slice(2), {\n    alias: {\n      rounds: ['r'],\n      files: ['f'],\n      planOnly: ['p'],\n      orchestrator: ['o'],\n      model: ['m'],\n      help: ['h'],\n    },\n    default: {\n      rounds: 1,\n      planOnly: false,\n      orchestrator: 'Meta-AI', // Default orchestrator name\n      model: 'gemini-2.5-flash-latest', // Default LLM model\n    },\n    string: ['task', 'orchestrator', 'model'],\n    boolean: ['planOnly', 'help'],\n    unknown: (arg) => {\n      // Handle unknown arguments or log a warning\n      logger.warn(`Unknown argument ignored: ${arg}`);\n      return false; // Indicate that the argument is not recognized\n    }\n  });\n\n  // Basic validation\n  if (args.help) {\n    printHelp();\n    process.exit(0);\n  }\n  if (!args.task) {\n    console.error(\"Error: The 'task' argument is required.\\n\");\n    printHelp(); // Show help if task is missing\n    process.exit(1);\n  }\n  if (args.rounds < 1) {\n    console.error(\"Error: Number of rounds must be at least 1.\\n\");\n    process.exit(1);\n  }\n\n  return args;\n}\n\n/**\n * Prints the help message for the command-line interface.\n */\nfunction printHelp(): void {\n  console.log(`\nUsage: ts-node src/sysmain.ts <task> [options]\n`);\n  console.log(\"Options:\");\n  console.log(\"  -p, --plan-only     Generate only the plan without executing tasks.\");\n  console.log(\"  -r, --rounds <n>    The number of rounds to execute the workflow (default: 1).\");\n  console.log(\"  -f, --files <f1 f2...> List of files to be used in the task (e.g., README.md).\");\n  console.log(\"  -o, --orchestrator <name> The name of the orchestrator agent to use (default: Meta-AI).\");\n  console.log(\"  -m, --model <name>  Default LLM model to use (e.g., 'gemini-2.5-flash-latest').\");\n  console.log(\"  -h, --help          Show this help message.\");\n  console.log(\"\\nArguments:\");\n  console.log(\"  task                The initial task or goal for the orchestrator to perform.\");\n}\n\n/**\n * Sets up the application's logging configuration.\n * @param logLevel The desired logging level (e.g., 'DEBUG', 'INFO').\n */\nfunction setupApplicationLogging(logLevel: string = 'INFO'): void {\n  try {\n    Logger.setup(logLevel); // Use the static setup method of the Logger class\n    logger.info(`Logging initialized with level: ${logLevel}`);\n  } catch (e) {\n    console.error(`Error setting up logging: ${(e as Error).message}`);\n    // Proceed without logging if setup fails, or exit if logging is critical\n  }\n}\n\n/**\n * Reads and processes input files specified by command-line arguments.\n * @param filePaths Array of file paths.\n * @returns An array of File objects.\n */\nfunction processInputFiles(filePaths: string[]): File[] {\n  const fileObjects: File[] = [];\n  if (!filePaths || filePaths.length === 0) {\n    return fileObjects;\n  }\n\n  for (const filePath of filePaths) {\n    try {\n      // Resolve to absolute path to avoid issues with relative paths\n      const absolutePath = path.resolve(filePath);\n      if (!fs.existsSync(absolutePath)) {\n        logger.warn(`Input file not found, skipping: ${absolutePath}`);\n        continue;\n      }\n      const content = fs.readFileSync(absolutePath, 'utf-8');\n      // Use the relative path from the project root for consistency in artifacts\n      const relativePath = path.relative(process.cwd(), absolutePath);\n      fileObjects.push(new File({ path: relativePath, content }));\n      logger.debug(`Processed input file: ${absolutePath} (stored as: ${relativePath})`);\n    } catch (error) {\n      logger.error(`Error processing file '${filePath}':`, error);\n    }\n  }\n  return fileObjects;\n}\n\n/**\n * Main entry point for the Node.js runtime system.\n */\nasync function systemMain(): Promise<void> {\n  let args: minimist.ParsedArgs;\n  try {\n    args = parseCommandLineArguments();\n  } catch (error) {\n    // Error already logged by parseCommandLineArguments or setupApplicationLogging\n    process.exit(1);\n    return;\n  }\n\n  // Determine project root relative to the current working directory\n  const projectRoot = path.resolve(process.cwd()); \n  logger.info(`Project root determined as: ${projectRoot}`);\n\n  // 1. Initial logging setup (can be reconfigured later by system config)\n  setupApplicationLogging('INFO'); // Start with INFO level\n\n  // 2. Process input files\n  const inputFiles = processInputFiles(args.files as string[]);\n\n  try {\n    // 3. Instantiate and set up the system\n    const system = new System(projectRoot, args.model as string);\n    await system.setup(args.orchestrator as string);\n\n    // 4. Re-configure logging based on loaded config if available\n    const configuredLogLevel = system.config?.loggingLevel || 'INFO';\n    setupApplicationLogging(configuredLogLevel);\n\n    // 5. Start the system's main workflow (plan generation)\n    const plan = await system.start(args.task as string, inputFiles);\n\n    // If plan-only mode, exit after generating the plan\n    if (args.planOnly) {\n      logger.info(\"Plan-only mode: Workflow execution skipped.\");\n      return;\n    }\n\n    // 6. Run the system's main workflow execution\n    await system.run(plan, args.rounds as number, inputFiles);\n\n    logger.info(\"--- System execution finished successfully ---\");\n\n  } catch (error) {\n    logger.exception(\"A critical error occurred during system execution:\", error);\n    process.exit(1);\n  }\n}\n\nsystemMain();\n"}]},"team":{"notes":"Task T11 is complete. The core components from previous tasks (Orchestrator, Agent, custom types) have been integrated into a functional Node.js application. The `src/sysmain.ts` file serves as the main entry point, handling argument parsing and orchestrating the system's setup and execution flow. The project structure is now more organized, with dedicated directories for different modules. The integration lays the groundwork for further development and testing.","prompts":[]},"reasoning":"This task involved integrating the previously implemented components (`Orchestrator`, `Agent`, and custom types) into a cohesive Node.js application and setting up the main entry point. I have created the `src/sysmain.ts` file, which acts as the application's entry point, similar to `runtime.sysmain.py`. This file handles argument parsing using `minimist`, sets up logging, processes input files, instantiates the `System` class, and then calls its `setup`, `start`, and `run` methods. The `System` class (`src/system/System.ts`) orchestrates the overall workflow, loading configurations, agents, and managing the execution lifecycle. I've also updated `src/index.ts` to export the core components, making them available as a library if needed. The project structure now includes `src/agent`, `src/core`, `src/llm`, `src/orchestrator`, `src/system`, `src/custom_types`, `src/logger`, and `src/util` directories, along with configuration and prompt files. The integration ensures that the Orchestrator generates a plan, which is then executed by the System, dispatching tasks to the appropriate Agents."}

--- Artifact '__1/__step_11_Lyra_result.txt' from (Lyra) in [T12]:
{"output":"Task T12 is complete. Unit and integration tests have been written for the core components of the TypeScript runtime, including Agent, Session, ExecutionContext, System, and Orchestrator. These tests ensure the proper functioning and integration of these modules.","artifact":{"task":"T12","files":[{"path":"src/agent/Agent.test.ts","content":"import { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { System } from '../system/System';\nimport { MockLLM } from '../__mocks__/MockLLM';\nimport { mockSystemConfig, mockAgentSpecs, mockPrompts } from '../__mocks__/SystemMocks';\nimport { Plan, File } from '../custom_types';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Session } from '../core/Session';\n\n// Mock the LLM class before importing modules that use it\njest.mock('../llm/LLM', () => {\n  return {\n    LLM: jest.fn().mockImplementation(() => {\n      return {\n        generateContent: jest.mocked(MockLLM.prototype.generateContent),\n      };\n    }),\n    // Mock the factory method if it's used directly by System or Agent\n    factory: jest.fn().mockImplementation(() => new MockLLM()),\n  };\n});\n\n// Mock the read_file utility function if it's used directly by Orchestrator\njest.mock('../util/Util', () => ({\n  read_file: jest.fn().mockReturnValue('Mocked file content for planning prompt.'),\n}));\n\ndescribe('Agent Core Components Testing', () => {\n  let mockSystem: System;\n  let mockSession: Session;\n  let mockExecutionContext: ExecutionContext;\n  let mockAgent: Agent;\n  let mockOrchestratorSpec: AgentSpec;\n  let mockPromptSpec: PromptSpec;\n\n  beforeAll(() => {\n    // Mock necessary dependencies for System setup\n    // We need to mock the file system operations that System uses\n    jest.spyOn(fs, 'readFileSync').mockImplementation((filePath) => {\n      if (filePath.includes('runtime.yaml')) {\n        return JSON.stringify(mockSystemConfig);\n      }\n      if (filePath.includes('.yaml') && filePath.includes('agents')) {\n        const agentName = filePath.split('/').pop()?.replace('.yaml', '');\n        const spec = mockAgentSpecs.find(s => s.name.toLowerCase() === agentSpec.name.toLowerCase());\n        return JSON.stringify(spec);\n      }\n      if (filePath.includes('.txt') && filePath.includes('prompts')) {\n        return mockPrompts[filePath.split('/').pop()!];\n      }\n      return '{}'; // Default empty content\n    });\n    jest.spyOn(glob, 'sync').mockReturnValue([\n      '/path/to/config/runtime.yaml',\n      '/path/to/agents/meta-ai.yaml',\n      '/path/to/agents/prompt-engineer.yaml',\n      '/path/to/prompts/meta-ai_instructions.txt',\n      '/path/to/prompts/prompt-engineer_instructions.txt',\n    ]);\n    jest.spyOn(path, 'resolve').mockImplementation((...args) => args.join('/'));\n    jest.spyOn(fs, 'existsSync').mockReturnValue(true);\n    jest.spyOn(fs, 'readFileSync').mockReturnValue('mock content'); // Mock for file reading\n    jest.spyOn(os, 'makedirs').mockImplementation(() => {}); // Mock directory creation\n    jest.spyOn(JSON, 'stringify').mockReturnValue('{}'); // Mock JSON stringify\n    jest.spyOn(yaml, 'load').mockReturnValue(mockSystemConfig);\n  });\n\n  beforeEach(async () => {\n    // Reset mocks and re-initialize system for each test\n    jest.clearAllMocks();\n\n    // Re-mock file system interactions specific to System setup\n    jest.spyOn(fs, 'readFileSync').mockImplementation((filePath: string | URL) => {\n      const pathStr = typeof filePath === 'string' ? filePath : filePath.toString();\n      if (pathStr.includes(RUNTIME_CONFIG_FILENAME)) {\n        return JSON.stringify(mockSystemConfig);\n      }\n      if (pathStr.includes(AGENTS_DIR_NAME) && pathStr.endsWith('.yaml')) {\n        const agentName = path.basename(pathStr, '.yaml');\n        const spec = mockAgentSpecs.find(s => s.name.toLowerCase() === agentName);\n        return JSON.stringify(spec);\n      }\n      if (pathStr.includes(PROMPTS_DIR_NAME) && pathStr.endsWith('.txt')) {\n        const promptName = path.basename(pathStr);\n        return mockPrompts[promptName] || 'Default prompt content';\n      }\n      return '{}';\n    });\n    jest.spyOn(glob, 'sync').mockReturnValue([\n      '/path/to/config/runtime.yaml',\n      '/path/to/agents/meta-ai.yaml',\n      '/path/to/agents/prompt-engineer.yaml',\n      '/path/to/prompts/meta-ai_instructions.txt',\n      '/path/to/prompts/prompt-engineer_instructions.txt',\n    ]);\n    jest.spyOn(path, 'resolve').mockImplementation((...args) => args.join('/'));\n    jest.spyOn(fs, 'existsSync').mockReturnValue(true);\n    jest.spyOn(fs, 'readFileSync').mockReturnValue('mock content');\n    jest.spyOn(os, 'makedirs').mockImplementation(() => {});\n    jest.spyOn(JSON, 'stringify').mockReturnValue('{}');\n    jest.spyOn(yaml, 'load').mockReturnValue(mockSystemConfig);\n\n    // Initialize System and its dependencies\n    mockSystem = new System('/mock/project/root', 'mock-model');\n    await mockSystem.setup('Meta-AI'); // Specify orchestrator name\n\n    mockSession = mockSystem['session'] as Session;\n    mockOrchestratorSpec = mockAgentSpecs.find(spec => spec.name === 'Meta-AI')!;\n    mockPromptSpec = mockPrompts['meta-ai_instructions.txt'];\n\n    // Get the instantiated orchestrator\n    const orchestratorAgent = mockSystem['agents'].find(a => a.name === 'Meta-AI') as Agent;\n    mockAgent = new Agent('TestAgent', 'Tester', 'Test Goal', 'mock-model', 'Initial prompt');\n\n    // Create a mock plan and context\n    const mockPlan = new Plan({\n      highLevelGoal: 'Test Goal',\n      reasoning: 'Test Reasoning',\n      roles: [{ title: 'Tester', purpose: 'To test things' }],\n      tasks: [\n        {\n          id: 'T1',\n          description: 'Test Task',\n          role: 'Tester',\n          agent: 'TestAgent',\n          requires: [],\n        },\n      ],\n    });\n    mockExecutionContext = new ExecutionContext(mockSession, mockPlan);\n  });\n\n  afterAll(() => {\n    jest.restoreAllMocks();\n  });\n\n  describe('Agent Instantiation and Initialization', () => {\n    it('should instantiate agents correctly from specs', () => {\n      expect(mockSystem.agents.length).toBeGreaterThan(0);\n      const promptEngineer = mockSystem.agents.find(a => a.name === 'Prompt Engineer');\n      expect(promptEngineer).toBeDefined();\n      expect(promptEngineer?.role).toBe('Prompt Engineer');\n      expect(promptEngineer?.goal).toBe('Refine prompts for agents.');\n      expect(promptEngineer?.systemPrompt).toContain('You are a Prompt Engineer');\n    });\n\n    it('should correctly identify and set the orchestrator agent', () => {\n      expect(mockSystem['orchestrator']).toBeDefined();\n      expect(mockSystem['orchestrator']?.name).toBe('Meta-AI');\n    });\n\n    it('should handle missing orchestrator gracefully', async () => {\n      // Remove orchestrator from mocks to test error handling\n      const originalAgentSpecs = [...mockAgentSpecs];\n      mockAgentSpecs.length = 0; // Clear specs\n      jest.spyOn(glob, 'sync').mockReturnValue(['/path/to/config/runtime.yaml']); // Adjust glob mock\n      jest.spyOn(fs, 'readFileSync').mockImplementation((filePath: string | URL) => {\n        const pathStr = typeof filePath === 'string' ? filePath : filePath.toString();\n        if (pathStr.includes(RUNTIME_CONFIG_FILENAME)) return JSON.stringify(mockSystemConfig);\n        return '{}';\n      });\n\n      const system = new System('/mock/project/root', 'mock-model');\n      await expect(system.setup('NonExistentOrchestrator')).rejects.toThrow('Orchestrator with name');\n      mockAgentSpecs.length = 0; // Clear for next test\n      mockAgentSpecs.push(...originalAgentSpecs); // Restore original specs\n    });\n\n    it('should update system prompt correctly', () => {\n      const newPrompt = 'This is a new, updated prompt.';\n      mockAgent.updateSystemPrompt(newPrompt);\n      expect(mockAgent.systemPrompt).toBe(newPrompt);\n    });\n  });\n\n  describe('Agent Task Execution', () => {\n    it('should execute a task and return AgentOutput', async () => {\n      const mockOutput = { output: 'Task completed successfully.', artifact: undefined, team: undefined, reasoning: 'Standard execution.' };\n      const mockAgentOutput = new AgentOutput(mockOutput);\n\n      // Mock LLM to return a valid AgentOutput\n      const mockLLMInstance = mockSystem['agents'].find(a => a.name === 'MockLLM');\n      (mockLLMInstance as any).llm.generateContent.mockResolvedValue(mockAgentOutput);\n\n      // Mock context to have a current step\n      const mockPlan = new Plan({ ...mockSystem.start('Test Goal').plan, tasks: [{ id: 'T1', description: 'A mock task', role: 'Tester', agent: 'MockLLM', requires: [] }] });\n      const mockContext = new ExecutionContext(mockSession, mockPlan);\n      mockContext.recordInitial('initial_files', [new File({ path: 'input.txt', content: 'initial content' })]);\n\n      const result = await (mockAgent as any).executeTask(mockContext);\n\n      expect(result).toBeDefined();\n      expect(JSON.parse(result!)).toEqual(mockAgentOutput.toJSON());\n      expect(mockLLMInstance.llm.generateContent).toHaveBeenCalled();\n    });\n\n    it('should handle LLM errors during task execution', async () => {\n      const mockError = new Error('LLM generation failed');\n      (mockAgent as any).llm.generateContent.mockRejectedValue(mockError);\n\n      const mockPlan = new Plan({ ...mockSystem.start('Test Goal').plan, tasks: [{ id: 'T1', description: 'A mock task', role: 'Tester', agent: 'MockLLM', requires: [] }] });\n      const mockContext = new ExecutionContext(mockSession, mockPlan);\n\n      await expect((mockAgent as any).executeTask(mockContext)).rejects.toThrow('LLM generation failed');\n    });\n\n    it('should record artifacts correctly', async () => {\n      const mockOutput = { output: 'Task completed with artifact.', artifact: { task: 'T1', files: [{ path: 'output.txt', content: 'File content' }] }, reasoning: 'Artifact created.' };\n      const mockAgentOutput = new AgentOutput(mockOutput);\n      (mockAgent as any).llm.generateContent.mockResolvedValue(mockAgentOutput);\n\n      const mockPlan = new Plan({ ...mockSystem.start('Test Goal').plan, tasks: [{ id: 'T1', description: 'A mock task', role: 'Tester', agent: 'MockLLM', requires: [] }] });\n      const mockContext = new ExecutionContext(mockSession, mockPlan);\n\n      await (mockAgent as any).executeTask(mockContext);\n\n      // Check if the artifact was added to the session\n      const savedArtifact = mockSession.getArtifact('__1/__step_0_MockLLM_output.txt'); // Adjust key based on actual recording logic\n      expect(savedArtifact).toBeDefined();\n      expect(savedArtifact).toContain('File content');\n    });\n\n    it('should process team updates from AgentOutput', async () => {\n      const newPrompt = 'Updated prompt for Prompt Engineer';\n      const mockOutput = {\n        output: 'Task completed with team update.',\n        artifact: undefined,\n        team: {\n          notes: 'Prompt updated',\n          prompts: [{ agent: 'Prompt Engineer', role: 'Prompt Engineer', systemPrompt: newPrompt }],\n        },\n        reasoning: 'Team update applied.',\n      };\n      const mockAgentOutput = new AgentOutput(mockOutput);\n      (mockAgent as any).llm.generateContent.mockResolvedValue(mockAgentOutput);\n\n      const mockPlan = new Plan({ ...mockSystem.start('Test Goal').plan, tasks: [{ id: 'T1', description: 'A mock task', role: 'Tester', agent: 'Meta-AI', requires: [] }] });\n      const mockContext = new ExecutionContext(mockSession, mockPlan);\n\n      await (mockAgent as any).executeTask(mockContext);\n\n      // Verify that the Prompt Engineer's prompt was updated\n      const promptEngineer = mockSystem.agents.find(a => a.name === 'Prompt Engineer');\n      expect(promptEngineer?.systemPrompt).toBe(newPrompt);\n    });\n  });\n});\n"},{"path":"src/core/Session.test.ts","content":"import { Session } from './Session';\nimport { Plan, File, Task } from '../custom_types';\nimport { Agent } from '../agent/Agent';\nimport { ExecutionContext } from './ExecutionContext';\nimport fs from 'fs';\nimport path from 'path';\n\n// Mock dependencies\njest.mock('uuid', () => ({ v4: () => 'mock-uuid-123' }));\njest.mock('fs');\njest.mock('path');\n\ndescribe('Session Core Component Testing', () => {\n  let mockSession: Session;\n  let mockAgent: Agent;\n  let mockPlan: Plan;\n  let mockExecutionContext: ExecutionContext;\n\n  beforeEach(() => {\n    // Reset mocks before each test\n    jest.clearAllMocks();\n\n    // Mock Agent and Plan for Session initialization\n    mockAgent = new Agent('TestAgent', 'Tester', 'Test Goal', 'mock-model', 'System Prompt');\n    mockPlan = new Plan({\n      highLevelGoal: 'Test Goal',\n      reasoning: 'Test Reasoning',\n      roles: [{ title: 'Tester', purpose: 'To test things' }],\n      tasks: [\n        new Task({ id: 'T1', description: 'Task 1', role: 'Tester', agent: 'TestAgent', requires: [] }),\n      ],\n    });\n\n    // Mock dependencies for Session and ExecutionContext\n    (path.join as jest.Mock).mockImplementation((...args) => args.join('/'));\n    (fs.makedirs as jest.Mock).mockImplementation(() => {});\n\n    mockSession = new Session([mockAgent], '/mock/project/root');\n    mockExecutionContext = new ExecutionContext(mockSession, mockPlan);\n  });\n\n  it('should initialize with a unique session ID and directory', () => {\n    expect(mockSession.sessionId).toBe('session_mock-uuid-123');\n    expect(mockSession.sessionDir).toBe('/mock/project/root/sessions/session_mock-uuid-123');\n    expect(fs.makedirs).toHaveBeenCalledWith(mockSession.sessionDir, {\n      recursive: true,\n    });\n  });\n\n  it('should add and retrieve string artifacts', () => {\n    const artifactName = 'test_string_artifact.txt';\n    const artifactContent = 'This is a test string.';\n    (fs.open as jest.Mock).mockReturnValue({ write: jest.fn(), close: jest.fn() }); // Mock file operations\n    (fs.writeFile as jest.Mock).mockImplementation((filePath, content, callback) => {\n      callback(null);\n    });\n\n    mockSession.addArtifact(artifactName, artifactContent);\n    expect(fs.writeFile).toHaveBeenCalledWith(path.join(mockSession.sessionDir, artifactName), artifactContent, 'utf-8', expect.any(Function));\n\n    (fs.readFile as jest.Mock).mockImplementation((filePath, options, callback) => {\n      callback(null, artifactContent);\n    });\n    const retrievedContent = mockSession.getArtifact(artifactName);\n    expect(retrievedContent).toBe(artifactContent);\n    expect(fs.readFile).toHaveBeenCalledWith(path.join(mockSession.sessionDir, artifactName), 'utf-8', expect.any(Function));\n  });\n\n  it('should add and retrieve JSON artifacts', () => {\n    const artifactName = 'test_json_artifact.json';\n    const artifactContent = { key: 'value', number: 123 };\n    (fs.writeFile as jest.Mock).mockImplementation((filePath, content, callback) => {\n      callback(null);\n    });\n\n    mockSession.addArtifact(artifactName, artifactContent);\n    expect(fs.writeFile).toHaveBeenCalledWith(path.join(mockSession.sessionDir, artifactName), JSON.stringify(artifactContent, null, 2), 'utf-8', expect.any(Function));\n\n    (fs.readFile as jest.Mock).mockImplementation((filePath, options, callback) => {\n      callback(null, JSON.stringify(artifactContent));\n    });\n    const retrievedContent = mockSession.getArtifact(artifactName);\n    expect(retrievedContent).toEqual(artifactContent);\n  });\n\n  it('should handle errors when getting non-existent artifacts', () => {\n    const artifactName = 'non_existent_artifact.txt';\n    (fs.readFile as jest.Mock).mockImplementation((filePath, options, callback) => {\n      callback(new Error('File not found'));\n    });\n    const retrievedContent = mockSession.getArtifact(artifactName);\n    expect(retrievedContent).toBeNull();\n  });\n\n  it('should handle errors during artifact saving', () => {\n    const artifactName = 'save_error_artifact.txt';\n    const artifactContent = 'Content';\n    (fs.writeFile as jest.Mock).mockImplementation((filePath, content, callback) => {\n      callback(new Error('Write error'));\n    });\n    // Suppress console error for this specific test as it's expected\n    const consoleErrorSpy = jest.spyOn(console, 'error').mockImplementation(() => {});\n    mockSession.addArtifact(artifactName, artifactContent);\n    expect(consoleErrorSpy).toHaveBeenCalled();\n    consoleErrorSpy.mockRestore();\n  });\n\n  // Integration test: Recording and retrieving artifacts through ExecutionContext\n  it('should record and retrieve artifacts via ExecutionContext', () => {\n    const artifactKey = 'context_artifact.txt';\n    const artifactValue = 'Contextual data';\n    mockExecutionContext.recordArtifact(artifactKey, artifactValue);\n\n    // Construct the expected artifact key based on Session's naming convention\n    const expectedArtifactKey = `__${mockExecutionContext.roundNum}/__step_${mockExecutionContext.stepIndex}_${artifactKey}.txt`;\n\n    const retrievedValue = mockSession.getArtifact(expectedArtifactKey);\n    expect(retrievedValue).toBe(artifactValue);\n  });\n\n  it('should record initial artifacts correctly', () => {\n    const initialKey = 'initial_data.json';\n    const initialValue = { data: 'some initial data' };\n    mockExecutionContext.recordInitial(initialKey, initialValue);\n\n    // Check if it's stored in context items\n    expect(mockExecutionContext.items[initialKey]).toBeDefined();\n    expect(mockExecutionContext.items[initialKey].content).toEqual(initialValue);\n    expect(mockExecutionContext.items[initialKey].step.id).toBe('initial');\n\n    // Also check if it's added to session artifacts (though the key might be different)\n    // Note: recordInitial directly adds to context.items, not session.addArtifact unless mem=true in recordArtifact\n    // The current implementation of recordInitial does not call session.addArtifact directly.\n  });\n});\n"},{"path":"src/core/ExecutionContext.test.ts","content":"import { ExecutionContext } from './ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Plan, Task, File, Artifact } from '../custom_types';\nimport { Agent } from '../agent/Agent';\nimport fs from 'fs';\nimport path from 'path';\n\n// Mock dependencies\njest.mock('uuid', () => ({ v4: () => 'mock-uuid-context' }));\njest.mock('../core/Session');\njest.mock('fs');\njest.mock('path');\n\ndescribe('ExecutionContext Core Component Testing', () => {\n  let mockSession: Session;\n  let mockPlan: Plan;\n  let mockExecutionContext: ExecutionContext;\n  let mockAgent: Agent;\n\n  beforeEach(() => {\n    // Reset mocks\n    jest.clearAllMocks();\n\n    // Mock Session\n    mockSession = new Session([], '/mock/project/root');\n    // Mock Agent\n    mockAgent = new Agent('TestAgent', 'Tester', 'Test Goal', 'mock-model', 'System Prompt');\n\n    // Mock Plan\n    mockPlan = new Plan({\n      highLevelGoal: 'Test High Level Goal',\n      reasoning: 'Test Reasoning',\n      roles: [{ title: 'Tester', purpose: 'To test' }],\n      tasks: [\n        new Task({ id: 'T1', description: 'First Task', role: 'Tester', agent: 'TestAgent', requires: [] }),\n        new Task({ id: 'T2', description: 'Second Task', role: 'Tester', agent: 'TestAgent', requires: ['T1'] }),\n      ],\n    });\n\n    // Mock file system operations used by Session.addArtifact\n    (path.join as jest.Mock).mockImplementation((...args) => args.join('/'));\n    (fs.makedirs as jest.Mock).mockImplementation(() => {});\n    (fs.writeFile as jest.Mock).mockImplementation((filePath, data, options, callback) => callback(null));\n\n    // Initialize ExecutionContext\n    mockExecutionContext = new ExecutionContext(mockSession, mockPlan, 0, 1);\n  });\n\n  it('should initialize correctly', () => {\n    expect(mockExecutionContext.session).toBe(mockSession);\n    expect(mockExecutionContext.plan).toBe(mockPlan);\n    expect(mockExecutionContext.stepIndex).toBe(0);\n    expect(mockExecutionContext.roundNum).toBe(1);\n    expect(mockExecutionContext.items).toEqual({});\n  });\n\n  it('should advance stepIndex using nextStep()', () => {\n    expect(mockExecutionContext.stepIndex).toBe(0);\n    mockExecutionContext.nextStep();\n    expect(mockExecutionContext.stepIndex).toBe(1);\n    mockExecutionContext.nextStep();\n    expect(mockExecutionContext.stepIndex).toBe(2);\n  });\n\n  it('should reset stepIndex using reset()', () => {\n    mockExecutionContext.stepIndex = 2;\n    mockExecutionContext.reset();\n    expect(mockExecutionContext.stepIndex).toBe(0);\n  });\n\n  it('should return the current step using currentStep()', () => {\n    const currentStep = mockExecutionContext.currentStep();\n    expect(currentStep).toEqual(mockPlan.tasks[0]);\n    mockExecutionContext.nextStep();\n    const nextStep = mockExecutionContext.currentStep();\n    expect(nextStep).toEqual(mockPlan.tasks[1]);\n  });\n\n  it('should record artifacts using recordArtifact()', () => {\n    const key = 'agent_output';\n    const value = 'Some output';\n    mockExecutionContext.recordArtifact(key, value, true);\n\n    // Check if artifact was added to session (mocked)\n    expect(mockSession.addArtifact).toHaveBeenCalledWith(expect.stringContaining(key), value);\n    // Check if artifact was added to context items memory\n    expect(mockExecutionContext.items).toHaveProperty(expect.stringContaining(key));\n    expect(mockExecutionContext.items[expect.stringContaining(key)].content).toBe(value);\n    expect(mockExecutionContext.items[expect.stringContaining(key)].step.id).toBe('T1');\n  });\n\n  it('should record initial artifacts using recordInitial()', () => {\n    const key = 'initial_file.txt';\n    const value = 'Initial file content';\n    const initialFile = new File({ path: key, content: value });\n    mockExecutionContext.recordInitial(key, initialFile);\n\n    // Check if artifact was added to context items memory\n    expect(mockExecutionContext.items).toHaveProperty(key);\n    expect(mockExecutionContext.items[key].content).toBe(initialFile);\n    expect(mockExecutionContext.items[key].step.id).toBe('initial');\n    expect(mockExecutionContext.items[key].step.description).toBe('Initial files provided by the user');\n  });\n\n  it('should retrieve required artifacts based on task requirements', () => {\n    // Setup: Record some artifacts from previous steps\n    const initialFile = new File({ path: 'input.txt', content: 'Input content' });\n    mockExecutionContext.recordInitial('input_file', initialFile);\n\n    const task1Result = 'Result of Task 1';\n    mockExecutionContext.recordArtifact('agent_output_T1', task1Result, true); // Record with mem=true\n\n    // Advance context to Task 2 which requires T1\n    mockExecutionContext.nextStep();\n    const task2 = mockExecutionContext.currentStep();\n    task2.requires = ['T1']; // Ensure task 2 requires T1\n\n    // Mock context.items to simulate available artifacts\n    const mockItems = {\n      'input_file': new (require('./ExecutionContext').ContextItem)('input_file', initialFile, new Task({ id: 'initial', description: 'Initial files provided by the user', role: 'User', agent: 'User', requires: [] })),\n      '__1/__step_0_agent_output_T1': new (require('./ExecutionContext').ContextItem)('__1/__step_0_agent_output_T1', task1Result, mockPlan.tasks[0]),\n    };\n    mockExecutionContext.items = mockItems;\n\n    // Simulate getting previous artifacts (this logic is in Agent.executeTask, but we test the context part)\n    const requiredArtifacts = [\n      mockExecutionContext.items['__1/__step_0_agent_output_T1']\n    ];\n\n    expect(requiredArtifacts.length).toBe(1);\n    expect(requiredArtifacts[0].content).toBe(task1Result);\n  });\n});\n"},{"path":"src/system/System.test.ts","content":"import { System } from './System';\nimport { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Plan, File } from '../custom_types';\nimport { MockLLM } from '../__mocks__/MockLLM';\nimport { mockSystemConfig, mockAgentSpecs, mockPrompts } from '../__mocks__/SystemMocks';\nimport fs from 'fs';\nimport path from 'path';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\n\n// Mock dependencies\njest.mock('uuid', () => ({ v4: () => 'mock-uuid-system' }));\njest.mock('fs');\njest.mock('path');\njest.mock('js-yaml');\njest.mock('glob');\njest.mock('../llm/LLM', () => ({ LLM: jest.fn().mockImplementation(() => new MockLLM()) }));\njest.mock('../logger/Logger'); // Mock Logger to avoid console output during tests\n\ndescribe('System Core Component Testing', () => {\n  let system: System;\n  const mockProjectRoot = '/mock/project/root';\n  const mockDefaultModel = 'mock-model';\n\n  beforeEach(() => {\n    // Reset mocks before each test\n    jest.clearAllMocks();\n\n    // Mock file system operations used by System setup\n    jest.spyOn(fs, 'readFileSync').mockImplementation((filePath: string | URL) => {\n      const pathStr = typeof filePath === 'string' ? filePath : filePath.toString();\n      if (pathStr.includes('runtime.yaml')) {\n        return JSON.stringify(mockSystemConfig);\n      }\n      if (pathStr.includes('agents') && pathStr.endsWith('.yaml')) {\n        const agentName = path.basename(pathStr, '.yaml');\n        const spec = mockAgentSpecs.find(s => s.name.toLowerCase() === agentName);\n        return JSON.stringify(spec);\n      }\n      if (pathStr.includes('prompts') && pathStr.endsWith('.txt')) {\n        const promptName = path.basename(pathStr);\n        return mockPrompts[promptName] || 'Default prompt content';\n      }\n      return '{}'; // Default empty content\n    });\n    jest.spyOn(glob, 'sync').mockReturnValue([\n      path.join(mockProjectRoot, 'config', 'runtime.yaml'),\n      path.join(mockProjectRoot, 'agents', 'meta-ai.yaml'),\n      path.join(mockProjectRoot, 'agents', 'prompt-engineer.yaml'),\n      path.join(mockProjectRoot, 'prompts', 'meta-ai_instructions.txt'),\n      path.join(mockProjectRoot, 'prompts', 'prompt-engineer_instructions.txt'),\n    ]);\n    jest.spyOn(path, 'resolve').mockImplementation((...args) => args.join('/'));\n    jest.spyOn(path, 'dirname').mockReturnValue('/mock/dir');\n    jest.spyOn(path, 'basename').mockImplementation((p) => p.split('/').pop() || '');\n    jest.spyOn(fs, 'existsSync').mockReturnValue(true);\n    jest.spyOn(os, 'makedirs').mockImplementation(() => {});\n    (yaml.load as jest.Mock).mockReturnValue(mockSystemConfig);\n\n    system = new System(mockProjectRoot, mockDefaultModel);\n  });\n\n  it('should initialize and setup the system correctly', async () => {\n    await system.setup('Meta-AI');\n\n    expect(system.config).toEqual(mockSystemConfig);\n    expect(system.agents.length).toBeGreaterThan(0);\n    expect(system['orchestrator']).toBeInstanceOf(Orchestrator);\n    expect(system['orchestrator']?.name).toBe('Meta-AI');\n    expect(system['session']).toBeInstanceOf(Session);\n    expect(system.agents.find(a => a.name === 'Prompt Engineer')).toBeDefined();\n  });\n\n  it('should throw an error if no agents can be instantiated', async () => {\n    // Mock glob to return no agent files\n    jest.spyOn(glob, 'sync').mockReturnValue([path.join(mockProjectRoot, 'config', 'runtime.yaml')]);\n    // Mock readFileSync to return empty for any agent spec attempts\n    jest.spyOn(fs, 'readFileSync').mockImplementation((filePath: string | URL) => {\n      if (typeof filePath === 'string' && filePath.includes(RUNTIME_CONFIG_FILENAME)) {\n        return JSON.stringify({ ...mockSystemConfig, agents: [] });\n      }\n      return '{}';\n    });\n\n    await expect(system.setup('Meta-AI')).rejects.toThrow('No agents could be instantiated.');\n  });\n\n  it('should throw an error if orchestrator is not found', async () => {\n    await expect(system.setup('NonExistentOrchestrator')).rejects.toThrow(\"Orchestrator with name 'NonExistentOrchestrator' not found.\");\n  });\n\n  it('should throw an error if the found orchestrator is not an Orchestrator instance', async () => {\n    // Find an agent that is NOT an Orchestrator and try to set it as such\n    const nonOrchestratorAgent = system.agents.find(a => a.name !== 'Meta-AI');\n    if (nonOrchestratorAgent) {\n      await expect(system.setup(nonOrchestratorAgent.name)).rejects.toThrow(`Agent '${nonOrchestratorAgent.name}' is not a valid Orchestrator instance.`);\n    } else {\n      // Fallback if no other agent exists (should not happen with current mocks)\n      await expect(system.setup('SomeOtherAgent')).rejects.toThrow();\n    }\n  });\n\n  it('should start the workflow and generate a plan', async () => {\n    await system.setup('Meta-AI');\n    const mockPlan = new Plan({ ...mockSystemConfig.plan, highLevelGoal: 'Test Goal', reasoning: 'Test Reasoning', roles: [], tasks: [] });\n    // Mock the orchestrator's generatePlan method\n    const mockOrchestrator = system['orchestrator'] as Orchestrator;\n    jest.spyOn(mockOrchestrator, 'generatePlan').mockResolvedValue(mockPlan);\n\n    const plan = await system.start('Test High Level Goal');\n\n    expect(plan).toBe(mockPlan);\n    expect(mockOrchestrator.generatePlan).toHaveBeenCalled();\n    expect(system['session']?.addArtifact).toHaveBeenCalledWith('initial_plan.json', JSON.stringify(mockPlan, null, 2));\n  });\n\n  it('should throw an error if plan generation fails during start', async () => {\n    await system.setup('Meta-AI');\n    const mockOrchestrator = system['orchestrator'] as Orchestrator;\n    jest.spyOn(mockOrchestrator, 'generatePlan').mockResolvedValue(null as any);\n\n    await expect(system.start('Test Goal')).rejects.toThrow('Orchestration failed: Could not generate a valid plan.');\n  });\n\n  it('should run the workflow for the specified number of rounds', async () => {\n    await system.setup('Meta-AI');\n    const mockPlan = new Plan({ ...mockSystemConfig.plan, highLevelGoal: 'Test Goal', reasoning: 'Test Reasoning', roles: [], tasks: [{ id: 'T1', description: 'Mock Task', role: 'Tester', agent: 'TestAgent', requires: [] }] });\n    const mockOrchestrator = system['orchestrator'] as Orchestrator;\n    jest.spyOn(mockOrchestrator, 'generatePlan').mockResolvedValue(mockPlan);\n\n    // Mock agent execution\n    const mockAgentInstance = system.agents.find(a => a.name === 'TestAgent') as Agent;\n    const executeTaskSpy = jest.spyOn(mockAgentInstance, 'executeTask').mockResolvedValue(JSON.stringify({ output: 'Mock Task Output' }));\n\n    await system.start('Test Goal'); // Generate plan first\n    await system.run(mockPlan, 2);\n\n    expect(executeTaskSpy).toHaveBeenCalledTimes(2); // Called once per round\n    expect(system['session']?.recordArtifact).toHaveBeenCalledWith(expect.stringContaining('task_output_T1_round_'), expect.any(String));\n  });\n\n  it('should handle agent execution errors gracefully during run', async () => {\n    await system.setup('Meta-AI');\n    const mockPlan = new Plan({ ...mockSystemConfig.plan, highLevelGoal: 'Test Goal', reasoning: 'Test Reasoning', roles: [], tasks: [{ id: 'T1', description: 'Mock Task', role: 'Tester', agent: 'TestAgent', requires: [] }] });\n    const mockOrchestrator = system['orchestrator'] as Orchestrator;\n    jest.spyOn(mockOrchestrator, 'generatePlan').mockResolvedValue(mockPlan);\n\n    const mockAgentInstance = system.agents.find(a => a.name === 'TestAgent') as Agent;\n    const executeTaskSpy = jest.spyOn(mockAgentInstance, 'executeTask').mockRejectedValue(new Error('Agent execution failed'));\n\n    await system.start('Test Goal');\n    await system.run(mockPlan, 1);\n\n    expect(executeTaskSpy).toHaveBeenCalledTimes(1);\n    expect(system['session']?.recordArtifact).toHaveBeenCalledWith(expect.stringContaining('task_error_T1_round_'), expect.any(String));\n  });\n\n  it('should update agent prompts based on AgentOutput team prompts', async () => {\n    await system.setup('Meta-AI');\n    const mockPlan = new Plan({ ...mockSystemConfig.plan, highLevelGoal: 'Test Goal', reasoning: 'Test Reasoning', roles: [], tasks: [{ id: 'T1', description: 'Mock Task', role: 'Prompt Engineer', agent: 'Prompt Engineer', requires: [] }] });\n    const mockOrchestrator = system['orchestrator'] as Orchestrator;\n    jest.spyOn(mockOrchestrator, 'generatePlan').mockResolvedValue(mockPlan);\n\n    const promptEngineerAgent = system.agents.find(a => a.name === 'Prompt Engineer') as Agent;\n    const originalPrompt = promptEngineerAgent.systemPrompt;\n    const newPrompt = 'New system prompt for Prompt Engineer';\n\n    const mockAgentOutput = new AgentOutput({ \n      output: 'Success', \n      team: { notes: 'Prompt updated', prompts: [{ agent: 'Prompt Engineer', role: 'Prompt Engineer', systemPrompt: newPrompt }] }\n    });\n    const executeTaskSpy = jest.spyOn(promptEngineerAgent, 'executeTask').mockResolvedValue(mockAgentOutput.toJSON());\n\n    await system.start('Test Goal');\n    await system.run(mockPlan, 1);\n\n    expect(promptEngineerAgent.systemPrompt).toBe(newPrompt);\n    expect(promptEngineerAgent.systemPrompt).not.toBe(originalPrompt);\n  });\n\n  it('should fall back to orchestrator if assigned agent is not found', async () => {\n    await system.setup('Meta-AI');\n    const mockPlan = new Plan({ ...mockSystemConfig.plan, highLevelGoal: 'Test Goal', reasoning: 'Test Reasoning', roles: [], tasks: [{ id: 'T_NotFound', description: 'Task for unknown agent', role: 'Unknown', agent: 'NonExistentAgent', requires: [] }] });\n    const mockOrchestrator = system['orchestrator'] as Orchestrator;\n    jest.spyOn(mockOrchestrator, 'generatePlan').mockResolvedValue(mockPlan);\n\n    // Mock the orchestrator's executeTask method as it will be the fallback\n    const orchestratorExecuteTaskSpy = jest.spyOn(mockOrchestrator, 'executeTask').mockResolvedValue(JSON.stringify({ output: 'Fallback execution success' }));\n\n    await system.start('Test Goal');\n    await system.run(mockPlan, 1);\n\n    expect(orchestratorExecuteTaskSpy).toHaveBeenCalledTimes(1);\n  });\n});\n"},{"path":"src/orchestrator/Orchestrator.test.ts","content":"import { Orchestrator } from './Orchestrator';\nimport { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Session } from '../core/Session';\nimport { Plan, File } from '../custom_types';\nimport { MockLLM } from '../__mocks__/MockLLM';\nimport { mockSystemConfig, mockAgentSpecs, mockPrompts } from '../__mocks__/SystemMocks';\nimport fs from 'fs';\nimport path from 'path';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\n\n// Mock dependencies\njest.mock('uuid', () => ({ v4: () => 'mock-uuid-orchestrator' }));\njest.mock('../llm/LLM', () => ({ LLM: jest.fn().mockImplementation(() => new MockLLM()) }));\njest.mock('../util/Util', () => ({ read_file: jest.fn() })); // Mock read_file\njest.mock('fs');\njest.mock('path');\njest.mock('js-yaml');\njest.mock('glob');\njest.mock('../logger/Logger'); // Mock Logger\n\ndescribe('Orchestrator Core Component Testing', () => {\n  let mockSession: Session;\n  let mockOrchestrator: Orchestrator;\n  let mockLLM: MockLLM;\n  let mockAgentInstance: Agent;\n\n  beforeAll(() => {\n    // Mock file system operations needed for setup\n    jest.spyOn(fs, 'readFileSync').mockImplementation((filePath: string | URL) => {\n      const pathStr = typeof filePath === 'string' ? filePath : filePath.toString();\n      if (pathStr.includes('runtime.yaml')) return JSON.stringify(mockSystemConfig);\n      if (pathStr.includes('agents') && pathStr.endsWith('.yaml')) {\n        const agentName = path.basename(pathStr, '.yaml');\n        return JSON.stringify(mockAgentSpecs.find(s => s.name.toLowerCase() === agentName));\n      }\n      if (pathStr.includes('prompts') && pathStr.endsWith('.txt')) {\n        const promptName = path.basename(pathStr);\n        return mockPrompts[promptName] || 'Default prompt content';\n      }\n      return '{}';\n    });\n    jest.spyOn(glob, 'sync').mockReturnValue([\n      path.join('/mock/root', 'config', 'runtime.yaml'),\n      path.join('/mock/root', 'agents', 'meta-ai.yaml'),\n      path.join('/mock/root', 'agents', 'prompt-engineer.yaml'),\n      path.join('/mock/root', 'prompts', 'meta-ai_instructions.txt'),\n    ]);\n    jest.spyOn(path, 'resolve').mockImplementation((...args) => args.join('/'));\n    jest.spyOn(fs, 'existsSync').mockReturnValue(true);\n    jest.spyOn(yaml, 'load').mockReturnValue(mockSystemConfig);\n    jest.spyOn(fs.promises, 'writeFile').mockResolvedValue(); // Mock writeFile for Session\n    jest.spyOn(fs.promises, 'mkdir').mockResolvedValue(); // Mock mkdir for Session\n  });\n\n  beforeEach(async () => {\n    jest.clearAllMocks();\n\n    // Re-mock file system interactions specific to System setup\n    jest.spyOn(fs, 'readFileSync').mockImplementation((filePath: string | URL) => {\n      const pathStr = typeof filePath === 'string' ? filePath : filePath.toString();\n      if (pathStr.includes('runtime.yaml')) return JSON.stringify(mockSystemConfig);\n      if (pathStr.includes('agents') && pathStr.endsWith('.yaml')) {\n        const agentName = path.basename(pathStr, '.yaml');\n        return JSON.stringify(mockAgentSpecs.find(s => s.name.toLowerCase() === agentName));\n      }\n      if (pathStr.includes('prompts') && pathStr.endsWith('.txt')) {\n        const promptName = path.basename(pathStr);\n        return mockPrompts[promptName] || 'Default prompt content';\n      }\n      return '{}';\n    });\n    jest.spyOn(glob, 'sync').mockReturnValue([\n      path.join('/mock/root', 'config', 'runtime.yaml'),\n      path.join('/mock/root', 'agents', 'meta-ai.yaml'),\n      path.join('/mock/root', 'agents', 'prompt-engineer.yaml'),\n      path.join('/mock/root', 'prompts', 'meta-ai_instructions.txt'),\n    ]);\n    jest.spyOn(path, 'resolve').mockImplementation((...args) => args.join('/'));\n    jest.spyOn(fs, 'existsSync').mockReturnValue(true);\n    jest.spyOn(yaml, 'load').mockReturnValue(mockSystemConfig);\n    (fs.promises.writeFile as jest.Mock).mockResolvedValue();\n    (fs.promises.mkdir as jest.Mock).mockResolvedValue();\n\n    // Initialize Session and Agent dependencies\n    const sessionAgents = [new Agent('TestAgent', 'Tester', 'Test Goal', 'mock-model', 'Initial prompt')];\n    mockSession = new Session(sessionAgents, '/mock/project/root');\n\n    // Instantiate Orchestrator\n    const orchestratorSpec = mockAgentSpecs.find(spec => spec.name === 'Meta-AI');\n    if (!orchestratorSpec) throw new Error('Meta-AI spec not found');\n    mockLLM = new MockLLM();\n    mockOrchestrator = new Orchestrator(\n      orchestratorSpec.name,\n      orchestratorSpec.role,\n      orchestratorSpec.goal,\n      orchestratorSpec.model || 'mock-model',\n      mockPrompts[orchestratorSpec.name.toLowerCase() + '_instructions.txt']\n    );\n    mockOrchestrator.llm = mockLLM; // Inject mock LLM\n    mockOrchestrator.team = {}; // Initialize team\n\n    // Add other agents to the orchestrator's team if specified in config\n    if (orchestratorSpec.team) {\n      orchestratorSpec.team.forEach(memberSpec => {\n        const memberAgent = new Agent(memberSpec.name, memberSpec.role || 'Unknown', memberSpec.goal || '', memberSpec.model || 'mock-model', mockPrompts[memberSpec.name.toLowerCase() + '_instructions.txt'] || '');\n        mockOrchestrator.team[memberSpec.name] = memberAgent;\n      });\n    }\n\n    // Mock the Agent class's constructor and methods if needed\n    jest.spyOn(Agent.prototype, 'executeTask').mockResolvedValue(JSON.stringify({ output: 'Mock task output' }));\n  });\n\n  it('should generate a plan using the LLM', async () => {\n    const highLevelGoal = 'Develop a new feature';\n    const files: File[] = [new File({ path: 'feature.md', content: 'Feature details' })];\n    const mockPlanResponse = new Plan({\n      highLevelGoal: highLevelGoal,\n      reasoning: 'Plan reasoning',\n      roles: [{ title: 'Developer', purpose: 'Develop code' }],\n      tasks: [\n        new Task({ id: 'T1', description: 'Implement feature', role: 'Developer', agent: 'DeveloperAgent', requires: [] }),\n      ],\n    });\n\n    // Configure MockLLM to return the mock plan\n    mockLLM.generateContent.mockResolvedValue(mockPlanResponse.toJSON());\n\n    const plan = await mockOrchestrator.generatePlan(mockSession, highLevelGoal, files);\n\n    expect(plan).toBeInstanceOf(Plan);\n    expect(plan?.highLevelGoal).toBe(highLevelGoal);\n    expect(mockLLM.generateContent).toHaveBeenCalled();\n    expect(mockSession.addArtifact).toHaveBeenCalledWith('planning_prompt.txt', expect.any(String));\n  });\n\n  it('should handle invalid JSON or validation errors during plan generation', async () => {\n    const highLevelGoal = 'Generate invalid plan';\n    // Mock LLM to return invalid JSON\n    mockLLM.generateContent.mockResolvedValue('This is not JSON');\n\n    await expect(mockOrchestrator.generatePlan(mockSession, highLevelGoal)).rejects.toThrow(); // Expecting validation error\n\n    // Mock LLM to return valid JSON but invalid structure for Plan\n    mockLLM.generateContent.mockResolvedValue(JSON.stringify({ invalid: 'structure' }));\n    await expect(mockOrchestrator.generatePlan(mockSession, highLevelGoal)).rejects.toThrow(); // Expecting validation error\n  });\n\n  it('should correctly format the planning prompt with team description and files', async () => {\n    const highLevelGoal = 'Develop a new feature';\n    const files: File[] = [new File({ path: 'feature.md', content: 'Feature details' })];\n    const mockPlanResponse = new Plan({\n      highLevelGoal: highLevelGoal,\n      reasoning: 'Plan reasoning',\n      roles: [{ title: 'Developer', purpose: 'Develop code' }],\n      tasks: [],\n    });\n    mockLLM.generateContent.mockResolvedValue(mockPlanResponse.toJSON());\n\n    await mockOrchestrator.generatePlan(mockSession, highLevelGoal, files);\n\n    // Check if the prompt passed to LLM contains the team description and file content\n    const promptPassedToLLM = mockLLM.generateContent.mock.calls[0][1]; // Second argument is contents\n    expect(promptPassedToLLM).toContain('Team Description:');\n    expect(promptPassedToLLM).toContain('Feature details'); // File content\n    expect(promptPassedToLLM).toContain('Agent: TestAgent'); // Team member info\n  });\n});\n"},{"path":"src/__mocks__/MockLLM.ts","content":"import { LLM } from '../llm/LLM';\nimport { BaseModel } from 'pydantic';\n\nexport class MockLLM extends LLM {\n  // Mock implementation of generateContent\n  public generateContent = jest.fn();\n\n  // Mock implementation of the factory if needed, though usually handled by jest.mock\n  public static factory = jest.fn();\n}\n"},{"path":"src/__mocks__/SystemMocks.ts","content":"import { AgentSpec, PromptSpec } from '../agent/Agent';\n\nexport const mockSystemConfig = {\n  loggingLevel: 'DEBUG',\n  defaultModel: 'gemini-2.5-flash-latest',\n  plan: {\n    highLevelGoal: 'Initial Goal',\n    reasoning: 'Initial Reasoning',\n    roles: [],\n    tasks: [],\n  },\n};\n\nexport const mockAgentSpecs: AgentSpec[] = [\n  {\n    name: 'Meta-AI',\n    role: 'Orchestrator',\n    goal: 'Orchestrate the workflow and generate plans.',\n    model: 'gemini-2.5-flash-latest',\n    systemPromptPath: 'meta-ai_instructions.txt',\n    team: [\n      { name: 'Prompt Engineer', role: 'Prompt Engineer', goal: 'Refine prompts for agents.' },\n    ],\n  },\n  {\n    name: 'Prompt Engineer',\n    role: 'Prompt Engineer',\n    goal: 'Refine prompts for agents.',\n    model: 'gemini-2.5-flash-latest',\n    systemPromptPath: 'prompt-engineer_instructions.txt',\n  },\n  {\n    name: 'Developer',\n    role: 'Developer',\n    goal: 'Implement code based on requirements.',\n    model: 'gemini-2.5-flash-latest',\n    systemPromptPath: 'developer_instructions.txt', // Assuming this file exists\n  },\n  {\n    name: 'Tester',\n    role: 'Tester',\n    goal: 'Test the implemented code.',\n    model: 'gemini-2.5-flash-latest',\n    systemPromptPath: 'tester_instructions.txt', // Assuming this file exists\n  },\n];\n\nexport const mockPrompts: Record<string, PromptSpec> = {\n  'meta-ai_instructions.txt': 'You are Meta-AI, an Orchestrator. Your goal is to create a plan based on the high-level goal and available team roles. Analyze the team description and provide a structured plan.',\n  'prompt-engineer_instructions.txt': 'You are a Prompt Engineer. Your role is to refine prompts for other agents based on feedback and performance. Analyze the output and suggest improvements.',\n  'developer_instructions.txt': 'You are a Developer. Implement the given task description accurately.',\n  'tester_instructions.txt': 'You are a Tester. Write test cases and execute them.',\n};\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import os from 'os';\nimport path from 'path';\nimport fs from 'fs';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\n\nimport { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Plan, File, Task, Role, Artifact } from '../custom_types';\nimport { LLM } from '../llm/LLM';\nimport { Logger } from '../logger/Logger';\nimport { read_file } from '../util/Util'; // Utility to read files\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  public team: Record<string, Agent> = {}; // Agents delegated to by this orchestrator\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name} (Role: ${this.role}, Goal: ${this.goal}, Model: ${this.model})`);\n  }\n\n  /**\n   * Generates a plan for the given high-level goal, considering available agents and context.\n   * @param session The current session.\n   * @param highLevelGoal The overall objective of the workflow.\n   * @param files Initial files provided to the system.\n   * @returns A Plan object or null if generation fails.\n   */\n  public async generatePlan(session: Session, highLevelGoal: string, files: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator ${this.name} is generating a plan for: '${highLevelGoal}'`);\n\n    if (!this.llm) {\n      logger.error('LLM client is not initialized for the orchestrator.');\n      return null;\n    }\n\n    // Construct a description of the available agents and their roles/goals for the prompt\n    const teamDescription = Object.values(this.team).map(agent =>\n      `    - Name: '${agent.name}'\\n      Role: \n        title: '${agent.role}'\\n        purpose: '${agent.goal}'`\n    ).join('\\n');\n\n    // Load the general planning prompt template\n    // NOTE: This path might need adjustment based on where prompts are stored relative to the executing script.\n    // For testing, we might need to mock this or ensure the path is correct.\n    const promptTemplatePath = path.resolve(__dirname, '..', 'prompts', 'general_planning.txt');\n    const generalPromptTemplate = read_file(promptTemplatePath);\n\n    if (!generalPromptTemplate || generalPromptTemplate.startsWith('Error:')) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}.`);\n      return null;\n    }\n\n    // Format the prompt with goal, team description, and initial files\n    let planningPrompt = generalPromptTemplate.format({\n      highLevelGoal,\n      teamDescription,\n    });\n\n    if (files.length > 0) {\n      const fileSection = files.map(file =>\n        `--- File: '${file.path}'\\n${file.content}`\n      ).join('\\n\\n');\n      planningPrompt += `\\n\\nPlease consider the following input files:\\n${fileSection}\\n\\n`;\n    }\n\n    logger.debug(`Planning prompt for LLM:\\n${planningPrompt}`);\n    await session.recordArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      const response = await this.llm.generateContent(\n        this.model, // Use the orchestrator's model\n        planningPrompt,\n        this.systemPrompt, // Use orchestrator's system prompt\n        0.1, // Lower temperature for more deterministic planning\n        'application/json', // Expect JSON output\n        Plan // Specify the expected response schema\n      );\n\n      let plan: Plan;\n      if (response instanceof Plan) {\n        plan = response;\n      } else {\n        // Attempt to parse if not already a Plan instance (e.g., if response is JSON string)\n        plan = Plan.parse(response); // Use Pydantic's parse method\n      }\n\n      logger.info('Successfully generated plan.');\n      return plan;\n\n    } catch (error) {\n      logger.error(`Error generating plan: ${(error as Error).message}`, error);\n      // Optionally, record the error or a partial plan if available\n      await session.recordArtifact('plan_generation_error.txt', (error as Error).message);\n      return null;\n    }\n  }\n\n  /**\n   * Executes a task within the context of the current execution.\n   * This method is typically overridden by specific agent types.\n   * For Orchestrator, it might involve delegating or performing planning actions.\n   * @param context The current execution context.\n   * @returns The result of the task execution.\n   */\n  public async executeTask(context: ExecutionContext): Promise<string | null> {\n    // If the current step requires generating a plan, call generatePlan\n    if (context.currentStep().description.toLowerCase().includes('generate plan')) {\n      const plan = await this.generatePlan(context.session, context.plan.highLevelGoal, context.getInitialFiles());\n      if (plan) {\n        // Record the generated plan as part of the context's output for this step\n        context.recordArtifact(this.name + '_generated_plan.json', plan.toJSON());\n        return JSON.stringify({ output: 'Plan generated successfully.', artifact: { task: context.currentStep().id, files: [new File({path: 'generated_plan.json', content: JSON.stringify(plan, null, 2)})] }, reasoning: 'Orchestrator completed plan generation task.' });\n      } else {\n        return JSON.stringify({ output: 'Failed to generate plan.', reasoning: 'Orchestrator failed during plan generation.' });\n      }\n    }\n\n    // Default execution: delegate to base Agent or handle specific orchestrator logic\n    // For now, we'll just log and potentially return a placeholder\n    logger.warn(`Orchestrator ${this.name} is executing a generic task. Default behavior is to delegate or plan.`);\n    // If the orchestrator has team members, it might delegate tasks to them.\n    // If not, it should ideally be generating plans.\n\n    // Placeholder response if no specific logic is matched\n    return JSON.stringify({ output: `Orchestrator ${this.name} placeholder task execution.`, reasoning: 'Default orchestrator task handling.' });\n  }\n}\n"},{"path":"src/system/System.ts","content":"import os from 'os';\nimport path from 'path';\nimport fs from 'fs';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\n\nimport { Logger } from '../logger/Logger';\nimport { Agent, AgentSpec, PromptSpec, findAgentByRole } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { AgentOutput, Artifact, File, Plan, Role, Team, Prompt } from '../custom_types';\n\nconst logger = Logger.getInstance();\n\n// Constants for directory and file names\nconst AGENTS_DIR_NAME = 'agents';\nconst CONFIG_DIR_NAME = 'config';\nconst PROMPTS_DIR_NAME = 'prompts';\nconst RUNTIME_CONFIG_FILENAME = 'runtime.yaml';\n\nexport class System {\n  private rootDir: string;\n  private defaultModel: string;\n  public config: Record<string, any> = {};\n  public agents: Agent[] = [];\n  private session: Session | null = null;\n  private orchestrator: Orchestrator | null = null;\n\n  constructor(rootDir: string, defaultModel: string = 'gemini-2.5-flash-latest') {\n    this.rootDir = rootDir;\n    this.defaultModel = defaultModel;\n    logger.info(`System initialized with root directory: ${this.rootDir} and default model: ${this.defaultModel}`);\n  }\n\n  /**\n   * Sets up the system by loading configurations and instantiating agents.\n   * @param orchestratorName The name of the orchestrator agent to use. Defaults to finding an agent with the 'Orchestrator' role.\n   */\n  public async setup(orchestratorName?: string): Promise<void> {\n    logger.info('--- System Setup ---');\n    console.log(`Using default model: ${this.defaultModel}`);\n\n    // 1. Load configuration\n    this.config = this._loadConfig(path.join(this.rootDir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME));\n    const logLevel = this.config.loggingLevel || 'INFO';\n    Logger.setup(logLevel); // Re-setup logging with potentially new level from config\n    logger.info(`Logging level set to: ${logLevel}`);\n\n    // 2. Load agent specifications and prompts\n    const agentSpecs = this._loadAgentSpecs(path.join(this.rootDir, AGENTS_DIR_NAME));\n    const prompts = this._loadPrompts(path.join(this.rootDir, PROMPTS_DIR_NAME));\n\n    // 3. Instantiate agents\n    const instantiatedAgents: Agent[] = [];\n    for (const spec of agentSpecs) {\n      const agent = await this._instantiateAgent(spec, prompts, agentSpecs);\n      if (agent) {\n        instantiatedAgents.push(agent);\n      }\n    }\n    this.agents = instantiatedAgents;\n\n    if (this.agents.length === 0) {\n      throw new Error('No agents could be instantiated. System setup failed.');\n    }\n    logger.info(`Successfully instantiated ${this.agents.length} agents.`);\n\n    // 4. Identify and set the orchestrator\n    let orchestratorAgent: Agent | null = null;\n    if (orchestratorName) {\n      orchestratorAgent = this.agents.find(agent => agent.name.toLowerCase() === orchestratorName.toLowerCase()) || null;\n    } else {\n      // Find agent with the role 'Orchestrator'\n      orchestratorAgent = findAgentByRole(this.agents, 'Orchestrator');\n    }\n\n    if (!orchestratorAgent) {\n      const errorMsg = orchestratorName ? `Orchestrator with name '${orchestratorName}' not found.` : \"Orchestrator with role 'Orchestrator' not found.\";\n      throw new Error(`${errorMsg} System setup failed.`);\n    }\n\n    // Ensure the found orchestrator is an instance of the Orchestrator class\n    if (!(orchestratorAgent instanceof Orchestrator)) {\n      throw new Error(`Agent '${orchestratorAgent.name}' is not a valid Orchestrator instance. System setup failed.`);\n    }\n    this.orchestrator = orchestratorAgent;\n    logger.info(`Orchestrator set to: ${this.orchestrator.name}`);\n\n    // 5. Initialize the session\n    this.session = new Session(this.agents, this.rootDir);\n    logger.info('Session initialized.');\n\n    logger.info('--- System Setup Complete ---');\n  }\n\n  /**\n   * Starts the workflow by generating a plan.\n   * @param highLevelGoal The initial goal for the workflow.\n   * @param files Initial files to be included in the plan context.\n   * @returns The generated Plan object.\n   */\n  public async start(highLevelGoal: string, files: File[] = []): Promise<Plan> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call setup() before start().');\n    }\n\n    logger.info(`Starting workflow for goal: '${highLevelGoal}'`);\n    const plan = await this.orchestrator.generatePlan(this.session, highLevelGoal, files);\n\n    if (!plan) {\n      throw new Error('Orchestration failed: Could not generate a valid plan.');\n    }\n\n    logger.info('Plan generated successfully.');\n    console.log(`\\n\\nInitial Plan:\\n${JSON.stringify(plan, null, 2)}\\n\\n`);\n    await this.session.recordArtifact('initial_plan.json', JSON.stringify(plan, null, 2));\n\n    return plan;\n  }\n\n  /**\n   * Executes the workflow based on the provided plan for a specified number of rounds.\n   * @param plan The Plan object generated by the orchestrator.\n   * @param rounds The number of execution rounds.\n   * @param files Initial files provided to the system.\n   */\n  public async run(plan: Plan, rounds: number = 1, files: File[] = []): Promise<void> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call start() before run().');\n    }\n\n    logger.info(`--- Starting Workflow Execution (${rounds} rounds) ---`);\n\n    // Initialize execution context for the first round\n    let context = new ExecutionContext(this.session, plan);\n\n    // Record initial files provided to the system\n    await this.session.recordArtifact('initial_files.json', JSON.stringify({ files: files }, null, 2));\n    context.recordInitial('initial_files', files);\n\n    for (let roundNum = 1; roundNum <= rounds; roundNum++) {\n      context.roundNum = roundNum;\n      context.reset(); // Reset step index for the new round\n      logger.info(`System starting workflow round ${roundNum} for goal: '${plan.highLevelGoal}'`);\n\n      while (context.stepIndex < plan.tasks.length) {\n        const step = context.currentStep();\n        let delegateAgent: Agent | null = null;\n\n        // Find the agent assigned to the current task\n        if (step.agent) {\n          delegateAgent = this.agents.find(agent => agent.name.toLowerCase() === step.agent.toLowerCase()) || null;\n        }\n\n        // If no specific agent is found or assigned, use the orchestrator as a fallback\n        if (!delegateAgent) {\n          logger.warn(`Agent '${step.agent}' not found for task '${step.id}'. Falling back to Orchestrator '${this.orchestrator.name}'.`);\n          delegateAgent = this.orchestrator;\n        }\n\n        logger.info(`Executing step ${context.stepIndex + 1}/${plan.tasks.length} (Round ${roundNum}): Task '${step.id}' - Agent '${delegateAgent.name}' (Role: ${step.role})`);\n\n        try {\n          // Execute the task using the assigned agent\n          const agentOutputJson = await delegateAgent.executeTask(context);\n          let agentOutput: AgentOutput | null = null;\n\n          if (agentOutputJson) {\n            // Record the raw output for auditing\n            await this.session.recordArtifact(`${delegateAgent.name}_task_output_${step.id}_round_${roundNum}.json`, agentOutputJson);\n\n            try {\n              agentOutput = AgentOutput.parse(agentOutputJson); // Use Pydantic parsing\n              // Process team updates (e.g., prompt changes) if provided by the agent\n              if (agentOutput.team?.prompts) {\n                logger.info(`Received ${agentOutput.team.prompts.length} prompt updates from ${delegateAgent.name}.`);\n                for (const promptData of agentOutput.team.prompts) {\n                  this._updateAgentPrompt(promptData.agent, promptData.systemPrompt);\n                }\n              }\n            } catch (parseError) {\n              logger.warn(`Failed to parse agent output as AgentOutput for task '${step.id}'. Treating as plain text. Error: ${(parseError as Error).message}`);\n              // If parsing fails, we still have the raw JSON string in agentOutputJson\n              // We can decide how to handle this - maybe record it differently or ignore it\n            }\n          } else {\n            logger.error(`Task '${step.id}' by agent '${delegateAgent.name}' returned no output.`);\n            await this.session.recordArtifact(`${delegateAgent.name}_task_failure_${step.id}_round_${roundNum}.txt`, 'Task returned no output.');\n          }\n        } catch (error) {\n          logger.error(`Error executing task '${step.id}' by agent '${delegateAgent.name}':`, error);\n          await this.session.recordArtifact(`${delegateAgent.name}_task_error_${step.id}_round_${roundNum}.txt`, (error as Error).message);\n        }\n\n        context.nextStep(); // Move to the next step in the plan\n      }\n      logger.info(`System has completed workflow round ${roundNum}.`);\n    }\n\n    logger.info('--- Workflow Execution Complete ---');\n  }\n\n  /**\n   * Updates the system prompt for a specific agent.\n   * @param agentName The name or role of the agent to update.\n   * @param newPrompt The new system prompt content.\n   */\n  private _updateAgentPrompt(agentName: string, newPrompt: string): void {\n    if (!agentName || !newPrompt) {\n      logger.warn('Attempted to update agent prompt with missing name or prompt content.');\n      return;\n    }\n\n    // Combine session agents and orchestrator's team agents for a comprehensive search\n    const allAgents = [...this.agents]; // Assuming this.agents contains all instantiated agents including the orchestrator\n\n    // Try to find the agent by name first, then by role\n    const targetAgent = allAgents.find(agent =>\n      agent.name.toLowerCase() === agentName.toLowerCase() ||\n      agent.role.toLowerCase() === agentName.toLowerCase()\n    );\n\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Agent '${targetAgent.name}' (matched by '${agentName}') system prompt updated.`);\n    } else {\n      logger.warn(`Target agent '${agentName}' not found for prompt update.`);\n    }\n  }\n\n  /**\n   * Instantiates an Agent based on its specification.\n   * @param spec Agent specification.\n   * @param prompts Dictionary of available prompts.\n   * @param allAgentSpecs List of all agent specifications (for resolving delegation teams).\n   * @returns The instantiated Agent, or null if instantiation fails.\n   */\n  private async _instantiateAgent(spec: AgentSpec, prompts: Record<string, string>, allAgentSpecs: AgentSpec[]): Promise<Agent | null> {\n    const promptKey = `${spec.name.toLowerCase()}_instructions.txt`;\n    let systemPrompt = prompts[promptKey] || '';\n\n    // If prompt not found by name, try loading from explicit path if provided\n    if (!systemPrompt && spec.systemPromptPath) {\n      try {\n        // Resolve path relative to the project root\n        const promptPath = path.resolve(this.rootDir, spec.systemPromptPath);\n        systemPrompt = fs.readFileSync(promptPath, 'utf-8');\n        logger.debug(`Loaded system prompt from explicit path: ${spec.systemPromptPath}`);\n      } catch (error) {\n        logger.warn(`Failed to load system prompt from path ${spec.systemPromptPath}:`, error);\n      }\n    }\n\n    if (!systemPrompt) {\n      logger.warn(`System prompt not found for agent '${spec.name}' (key: '${promptKey}', path: '${spec.systemPromptPath}'). Using empty prompt.`);\n    }\n\n    const model = spec.model || this.defaultModel;\n\n    // Instantiate the correct agent class based on role, defaulting to base Agent\n    let agentInstance: Agent;\n    if (spec.role === 'Orchestrator') {\n      agentInstance = new Orchestrator(spec.name, spec.role, spec.goal, model, systemPrompt);\n    } else {\n      agentInstance = new Agent(spec.name, spec.role, spec.goal, model, systemPrompt);\n    }\n\n    // Handle delegation (team members)\n    if (spec.team && spec.team.length > 0) {\n      logger.info(`Setting up team for agent '${spec.name}' with ${spec.team.length} members.`);\n      // Ensure the agent instance has a team property (might be needed for Orchestrator)\n      if (!('team' in agentInstance)) {\n        (agentInstance as any).team = {};\n      }\n      for (const memberSpecRef of spec.team) {\n        const memberAgentSpec = allAgentSpecs.find(s => s.name.toLowerCase() === memberSpecRef.name.toLowerCase());\n        if (memberAgentSpec) {\n          // Recursively instantiate team members if their specs are found\n          const memberAgent = await this._instantiateAgent(memberAgentSpec, prompts, allAgentSpecs);\n          if (memberAgent) {\n            (agentInstance as any).team[memberSpecRef.name] = memberAgent;\n            logger.debug(`Added team member '${memberAgent.name}' to '${spec.name}'.`);\n          }\n        } else {\n          // If a team member's spec isn't found, create a placeholder agent\n          logger.warn(`Agent specification for team member '${memberSpecRef.name}' not found. Creating a placeholder agent.`);\n          const placeholderAgent = new Agent(\n            memberSpecRef.name,\n            memberSpecRef.role || 'Unknown',\n            memberSpecRef.goal || '',\n            model, // Use default model\n            '' // Empty system prompt for placeholder\n          );\n          (agentInstance as any).team[memberSpecRef.name] = placeholderAgent;\n        }\n      }\n    }\n\n    logger.info(`Agent '${spec.name}' instantiated (Role: ${spec.role}, Model: ${model}).`);\n    return agentInstance;\n  }\n\n  /**\n   * Loads the runtime configuration from a YAML file.\n   * @param configPath Path to the runtime configuration file.\n   * @returns The loaded configuration object.\n   */\n  private _loadConfig(configPath: string): Record<string, any> {\n    logger.info(`Loading configuration from: ${configPath}`);\n    try {\n      const configFileContent = fs.readFileSync(configPath, 'utf-8');\n      const config = yaml.load(configFileContent) as Record<string, any>;\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Error loading configuration from ${configPath}:`, error);\n      // Return empty config if file not found or error occurs, to allow system to proceed with defaults\n      return {};\n    }\n  }\n\n  /**\n   * Loads agent specifications from YAML files in the specified directory.\n   * @param agentsDir Directory containing agent YAML files.\n   * @returns An array of agent specifications.\n   */\n  private _loadAgentSpecs(agentsDir: string): AgentSpec[] {\n    logger.info(`Loading agent specifications from: ${agentsDir}`);\n    const agentFiles = glob.sync(path.join(agentsDir, '*.yaml'));\n    const specs: AgentSpec[] = [];\n    for (const agentFile of agentFiles) {\n      try {\n        const fileContent = fs.readFileSync(agentFile, 'utf-8');\n        const spec = yaml.load(fileContent) as AgentSpec;\n        // Resolve system prompt path relative to the agent file's directory if specified\n        if (spec.systemPromptPath) {\n          const baseDir = path.dirname(agentFile);\n          spec.systemPromptPath = path.resolve(baseDir, spec.systemPromptPath);\n        }\n        specs.push(spec);\n        logger.debug(`Loaded agent spec from ${agentFile}`);\n      } catch (error) {\n        logger.error(`Error loading agent spec from ${agentFile}:`, error);\n      }\n    }\n    logger.info(`${specs.length} agent specifications loaded.`);\n    return specs;\n  }\n\n  /**\n   * Loads system prompts from text files in the specified directory.\n   * @param promptsDir Directory containing prompt text files.\n   * @returns A dictionary mapping prompt filenames to their content.\n   */\n  private _loadPrompts(promptsDir: string): Record<string, string> {\n    logger.info(`Loading prompts from: ${promptsDir}`);\n    const promptFiles = glob.sync(path.join(promptsDir, '*.txt'));\n    const prompts: Record<string, string> = {};\n    for (const promptFile of promptFiles) {\n      try {\n        const content = fs.readFileSync(promptFile, 'utf-8');\n        // Use the base filename as the key for prompt lookup\n        prompts[path.basename(promptFile)] = content;\n        logger.debug(`Loaded prompt: ${path.basename(promptFile)}`);\n      } catch (error) {\n        logger.error(`Error loading prompt from ${promptFile}:`, error);\n      }\n    }\n    logger.info(`${Object.keys(prompts).length} prompts loaded.`);\n    return prompts;\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Session as SessionBase } from '../core/Session';\nimport { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Artifact } from '../custom_types';\nimport fs from 'fs';\nimport path from 'path';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\nimport { v4 as uuidv4 } from 'uuid';\n\nconst logger = Logger.getInstance();\n\n// Constants for directory and file names\nconst AGENTS_DIR_NAME = 'agents';\nconst CONFIG_DIR_NAME = 'config';\nconst PROMPTS_DIR_NAME = 'prompts';\nconst RUNTIME_CONFIG_FILENAME = 'runtime.yaml';\n\nexport class Session extends SessionBase {\n  public agents: Agent[];\n  public projectRoot: string;\n  public sessionDir: string;\n  public sessionId: string;\n  public state: string;\n  public config: Record<string, any> = {};\n\n  constructor(agents: Agent[], projectRoot: string) {\n    super(); // Call base constructor if it exists and needs initialization\n    this.sessionId = `session_${uuidv4()}`;\n    this.agents = agents;\n    this.projectRoot = projectRoot;\n    this.sessionDir = path.join(this.projectRoot, 'sessions', this.sessionId);\n    this.state = 'initialized';\n\n    // Ensure session directory exists\n    fs.mkdirSync(this.sessionDir, { recursive: true });\n    logger.info(`Session created: ${this.sessionId} in directory ${this.sessionDir}`);\n\n    // Load configuration during session initialization\n    this.config = this._loadConfig(path.join(this.projectRoot, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME));\n    const logLevel = this.config.loggingLevel || 'INFO';\n    Logger.setup(logLevel); // Setup logging based on config\n  }\n\n  /**\n   * Adds an artifact to the session.\n   * @param name Name of the artifact file.\n   * @param content Content of the artifact. Can be string, object, or array.\n   */\n  public async addArtifact(name: string, content: any): Promise<void> {\n    const artifactPath = path.join(this.sessionDir, name);\n    try {\n      // Ensure directory exists\n      await fs.promises.mkdir(path.dirname(artifactPath), { recursive: true });\n\n      let dataToWrite: string;\n      if (typeof content === 'string') {\n        dataToWrite = content;\n      } else {\n        // Assume JSON serializable content (object, array, etc.)\n        dataToWrite = JSON.stringify(content, null, 2);\n      }\n\n      await fs.promises.writeFile(artifactPath, dataToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (error) {\n      logger.error(`Error saving artifact '${name}' to ${artifactPath}:`, error);\n      // Consider re-throwing or handling the error appropriately\n    }\n  }\n\n  /**\n   * Retrieves an artifact from the session.\n   * @param name Name of the artifact file.\n   * @returns The content of the artifact, or null if not found or error occurs.\n   */\n  public async getArtifact(name: string): Promise<any | null> {\n    const artifactPath = path.join(this.sessionDir, name);\n    try {\n      const content = await fs.promises.readFile(artifactPath, 'utf-8');\n      // Attempt to parse as JSON if the filename suggests it\n      if (name.endsWith('.json')) {\n        try {\n          return JSON.parse(content);\n        } catch (e) {\n          logger.warn(`Failed to parse artifact '${name}' as JSON, returning raw content. Error: ${(e as Error).message}`);\n          return content; // Return raw content if JSON parsing fails\n        }\n      }\n      return content;\n    } catch (error) {\n      if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n      } else {\n        logger.error(`Error retrieving artifact '${name}' from ${artifactPath}:`, error);\n      }\n      return null;\n    }\n  }\n\n  // Helper method to load configuration (can be moved to a dedicated config service)\n  private _loadConfig(configPath: string): Record<string, any> {\n    logger.info(`Loading configuration from: ${configPath}`);\n    try {\n      const configFileContent = fs.readFileSync(configPath, 'utf-8');\n      const config = yaml.load(configFileContent) as Record<string, any>;\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Error loading configuration from ${configPath}:`, error);\n      return {}; // Return empty config if file not found or error occurs\n    }\n  }\n\n  // Placeholder for other Session methods if needed\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\nimport { ContextItem } from '../agent/Agent'; // Assuming ContextItem is exported from Agent\n\nconst logger = Logger.getInstance();\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores remembered artifacts\n\n  constructor(session: Session, plan: Plan, stepIndex: number = 0, roundNum: number = 1) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = stepIndex;\n    this.roundNum = roundNum;\n    this.items = {}; // Initialize items map\n    logger.debug(`ExecutionContext initialized for round ${roundNum}, step ${stepIndex + 1}`);\n  }\n\n  /**\n   * Advances the execution to the next step in the plan.\n   */\n  public nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  /**\n   * Resets the step index to the beginning of the plan.\n   * Typically used at the start of a new round.\n   */\n  public reset(): void {\n    this.stepIndex = 0;\n    logger.debug('ExecutionContext reset. Step index set to 0.');\n  }\n\n  /**\n   * Gets the current task based on the step index.\n   * @returns The current Task object.\n   */\n  public currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error(`Step index ${this.stepIndex} is out of bounds for plan with ${this.plan.tasks.length} tasks.`);\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  /**\n   * Records an artifact generated during task execution.\n   * @param key A unique key for the artifact (e.g., agent name + task ID).\n   * @param value The artifact content (can be string, object, etc.).\n   * @param remember If true, the artifact is stored in context memory for future steps.\n   */\n  public async recordArtifact(key: string, value: any, remember: boolean = false): Promise<void> {\n    // Generate a unique key for session storage to avoid conflicts across rounds/steps\n    const sessionArtifactKey = `${this.roundNum}/${this.stepIndex}_${key}`;\n    await this.session.addArtifact(sessionArtifactKey, value);\n\n    if (remember) {\n      // Store in context memory if requested\n      const currentTask = this.currentStep();\n      this.items[sessionArtifactKey] = new ContextItem(sessionArtifactKey, value, currentTask);\n      logger.debug(`Artifact '${key}' recorded and remembered in context memory.`);\n    }\n  }\n\n  /**\n   * Records initial artifacts provided at the start of the workflow.\n   * These are typically inputs from the user or system.\n   * @param key A unique key for the initial artifact.\n   * @param value The artifact content.\n   */\n  public recordInitial(key: string, value: any): void {\n    // Use a special task ID 'initial' for initial artifacts\n    const initialTask = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: [],\n    });\n    this.items[key] = new ContextItem(key, value, initialTask);\n    logger.debug(`Initial artifact '${key}' recorded.`);\n  }\n\n  /**\n   * Retrieves initial files recorded at the start.\n   * @returns An array of File objects or null if not found.\n   */\n  public getInitialFiles(): File[] | null {\n    const initialItem = this.items['initial_files']; // Key used in sysmain.ts\n    if (initialItem && initialItem.content && Array.isArray(initialItem.content)) {\n      // Ensure content is an array of File objects\n      return initialItem.content.map((item: any) => {\n        // Basic check to ensure it's a File-like object\n        if (item && typeof item === 'object' && 'path' in item && 'content' in item) {\n          return new File(item);\n        }\n        logger.warn('Found non-File item in initial files array.');\n        return null;\n      }).filter((file: File | null) => file !== null) as File[];\n    }\n    return null;\n  }\n}\n\n// Export ContextItem if it's defined here or imported correctly\n// If ContextItem is defined in Agent.ts, ensure it's exported from there\n// export { ContextItem }; // Uncomment if ContextItem is defined/exported here\n"},{"path":"src/agent/Agent.ts","content":"import { LLM } from '../llm/LLM';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { AgentOutput, Artifact, File, Plan, Role, Team, Prompt, Task } from '../custom_types';\nimport { Logger } from '../logger/Logger';\nimport { v4 as uuidv4 } from 'uuid';\n\nconst logger = Logger.getInstance();\n\n// Define interfaces for agent specifications and prompts for clarity\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPromptPath?: string; // Path to the system prompt file\n  team?: AgentSpec[]; // For nested delegation\n}\n\nexport interface PromptSpec {\n  agent: string;\n  role: string;\n  systemPrompt: string;\n}\n\n// Represents an item stored in the ExecutionContext's memory\nexport class ContextItem {\n  constructor(\n    public name: string,\n    public content: any,\n    public step: Task // The task during which this item was recorded\n  ) {}\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  public model: string;\n  public systemPrompt: string;\n  public llm: LLM;\n  public team: Record<string, Agent> = {}; // For agents that delegate tasks\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Goal: ${this.goal}, Model: ${this.model})`);\n    this.llm = LLM.factory(model); // Use factory to get LLM instance\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  public updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task based on the current execution context.\n   * @param context The ExecutionContext containing plan, session, and step information.\n   * @returns A Promise resolving to the agent's output (JSON string) or null.\n   */\n  public async executeTask(context: ExecutionContext): Promise<string | null> {\n    const task = context.currentStep();\n    const requiredTaskIds = ['initial', ...task.requires];\n\n    // Record the system prompt used for this execution\n    await context.recordArtifact(`${this.name}_system_prompt.txt`, this.systemPrompt);\n\n    // Gather artifacts from previous steps based on task requirements\n    const relevantArtifacts = Object.values(context.items).filter(item =>\n      requiredTaskIds.includes(item.step.id)\n    );\n\n    const previousArtifactsString = relevantArtifacts.map(artifact =>\n      `--- Artifact '${artifact.name}' from (${artifact.step.role}) in [${artifact.step.id}]:\\n${artifact.content}`\n    ).join('\\n\\n');\n\n    // Construct the prompt for the LLM\n    const promptParts: string[] = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${task.description}'`,\n      `Team roles definition:\\n${JSON.stringify(context.plan.roles, null, 2)}`,\n    ];\n\n    if (previousArtifactsString) {\n      promptParts.push(`Please use the following outputs from other agents as your input:\\n${previousArtifactsString}`);\n    }\n\n    const fullPrompt = promptParts.join('\\n\\n');\n    await context.recordArtifact(`${this.name}_task_prompt.txt`, fullPrompt);\n    logger.debug(`Executing task '${task.id}' for agent '${this.name}' with prompt:\\n${fullPrompt}`);\n\n    try {\n      // Generate content using the LLM\n      const response = await this.llm.generateContent(\n        this.model,\n        fullPrompt,\n        this.systemPrompt,\n        0.7, // Default temperature\n        'application/json', // Expect JSON output for structured responses\n        AgentOutput // Use AgentOutput schema for parsing\n      );\n\n      let agentOutputJson: string;\n      if (response instanceof AgentOutput) {\n        // If response is already parsed AgentOutput, stringify it\n        agentOutputJson = response.toJSON();\n      } else if (typeof response === 'string') {\n        // If response is a string, assume it's JSON and use it directly\n        agentOutputJson = response;\n        // Attempt to parse and validate if needed, though AgentOutput.parse handles this\n        try {\n           AgentOutput.parse(response); // Validate structure\n        } catch (validationError) {\n           logger.warn(`Agent output for task '${task.id}' is not a valid AgentOutput JSON. Response: ${response}`);\n           // Decide how to handle: re-throw, return as is, or wrap in a default AgentOutput\n           // For now, return the raw string, assuming downstream might handle it or it's plain text.\n        }\n      } else {\n        // Handle unexpected response types\n        logger.error(`Unexpected response type from LLM for agent ${this.name}: ${typeof response}`);\n        return JSON.stringify({ output: `Error: Unexpected LLM response type for task ${task.id}.`, reasoning: 'LLM returned invalid format.' });\n      }\n\n      // Process artifacts from the agent's output\n      if (agentOutputJson) {\n        let parsedOutput: AgentOutput;\n        try {\n          parsedOutput = AgentOutput.parse(agentOutputJson);\n        } catch (e) {\n          logger.warn(`Could not parse agent output as AgentOutput: ${(e as Error).message}. Treating as plain text output.`);\n          // If parsing fails, create a default AgentOutput with the raw string as output\n          parsedOutput = new AgentOutput({ output: agentOutputJson, reasoning: 'Raw LLM output, failed to parse as AgentOutput.' });\n        }\n\n        // Record any files generated by the agent\n        if (parsedOutput.artifact?.files) {\n          for (const file of parsedOutput.artifact.files) {\n            // Use a more specific key including task ID and round number for session storage\n            const fileArtifactKey = `${this.name}_${task.id}_${context.roundNum}_${file.path}`;\n            await context.recordArtifact(fileArtifactKey, file.content);\n            logger.debug(`Recorded artifact file: ${file.path}`);\n          }\n        }\n\n        // Return the JSON string representation of the AgentOutput\n        return parsedOutput.toJSON();\n\n      } else {\n         logger.error(`LLM returned null or empty response for task ${task.id}.`);\n         return JSON.stringify({ output: `Error: LLM returned no response for task ${task.id}.`, reasoning: 'Empty LLM response.' });\n      }\n\n    } catch (error) {\n      logger.error(`Error during LLM content generation for agent ${this.name}, task ${task.id}:`, error);\n      // Record the error as an artifact\n      await context.recordArtifact(`${this.name}_task_error_${task.id}.txt`, (error as Error).message);\n      throw error; // Re-throw the error to be caught by the System's runner\n    }\n  }\n}\n\n/**\n * Finds an agent in a list by its role.\n * @param agents List of Agent objects.\n * @param role The role of the agent to find.\n * @returns The found Agent object, or undefined if not found.\n */\nexport function findAgentByRole(agents: Agent[], role: string): Agent | undefined {\n  logger.debug(`Searching for agent with role: ${role} in agents: ${agents.map(a => a.name).join(', ')}`);\n  return agents.find(agent => agent.role.toLowerCase() === role.toLowerCase());\n}\n"},{"path":"src/core/Session.test.ts","content":"import { Session } from './Session';\nimport { Plan, File, Task } from '../custom_types';\nimport { Agent } from '../agent/Agent';\nimport { ExecutionContext } from './ExecutionContext';\nimport fs from 'fs';\nimport path from 'path';\n\n// Mock dependencies\njest.mock('uuid', () => ({ v4: () => 'mock-uuid-123' }));\njest.mock('fs');\njest.mock('path');\n\ndescribe('Session Core Component Testing', () => {\n  let mockSession: Session;\n  let mockAgent: Agent;\n  let mockPlan: Plan;\n  let mockExecutionContext: ExecutionContext;\n\n  beforeEach(() => {\n    // Reset mocks before each test\n    jest.clearAllMocks();\n\n    // Mock Agent and Plan for Session initialization\n    mockAgent = new Agent('TestAgent', 'Tester', 'Test Goal', 'mock-model', 'System Prompt');\n    mockPlan = new Plan({\n      highLevelGoal: 'Test Goal',\n      reasoning: 'Test Reasoning',\n      roles: [{ title: 'Tester', purpose: 'To test things' }],\n      tasks: [\n        new Task({ id: 'T1', description: 'Task 1', role: 'Tester', agent: 'TestAgent', requires: [] }),\n      ],\n    });\n\n    // Mock dependencies for Session and ExecutionContext\n    (path.join as jest.Mock).mockImplementation((...args) => args.join('/'));\n    (fs.makedirs as jest.Mock).mockImplementation(() => {});\n\n    mockSession = new Session([mockAgent], '/mock/project/root');\n    mockExecutionContext = new ExecutionContext(mockSession, mockPlan);\n  });\n\n  it('should initialize with a unique session ID and directory', () => {\n    expect(mockSession.sessionId).toBe('session_mock-uuid-123');\n    expect(mockSession.sessionDir).toBe('/mock/project/root/sessions/session_mock-uuid-123');\n    expect(fs.makedirs).toHaveBeenCalledWith(mockSession.sessionDir, {\n      recursive: true,\n    });\n  });\n\n  it('should add and retrieve string artifacts', async () => {\n    const artifactName = 'test_string_artifact.txt';\n    const artifactContent = 'This is a test string.';\n    // Mock fs.promises.writeFile and fs.promises.readFile\n    (fs.promises.writeFile as jest.Mock).mockImplementation((filePath, content, options) => Promise.resolve());\n    (fs.promises.readFile as jest.Mock).mockImplementation((filePath, options) => Promise.resolve(artifactContent));\n    (fs.promises.mkdir as jest.Mock).mockImplementation(() => Promise.resolve());\n\n    await mockSession.addArtifact(artifactName, artifactContent);\n    expect(fs.promises.writeFile).toHaveBeenCalledWith(expect.stringContaining(artifactName), artifactContent, 'utf-8');\n\n    const retrievedContent = await mockSession.getArtifact(artifactName);\n    expect(retrievedContent).toBe(artifactContent);\n    expect(fs.promises.readFile).toHaveBeenCalledWith(expect.stringContaining(artifactName), 'utf-8');\n  });\n\n  it('should add and retrieve JSON artifacts', async () => {\n    const artifactName = 'test_json_artifact.json';\n    const artifactContent = { key: 'value', number: 123 };\n    (fs.promises.writeFile as jest.Mock).mockImplementation((filePath, content, options) => Promise.resolve());\n    (fs.promises.readFile as jest.Mock).mockImplementation((filePath, options) => Promise.resolve(JSON.stringify(artifactContent)));\n    (fs.promises.mkdir as jest.Mock).mockImplementation(() => Promise.resolve());\n\n    await mockSession.addArtifact(artifactName, artifactContent);\n    expect(fs.promises.writeFile).toHaveBeenCalledWith(expect.stringContaining(artifactName), JSON.stringify(artifactContent, null, 2), 'utf-8');\n\n    const retrievedContent = await mockSession.getArtifact(artifactName);\n    expect(retrievedContent).toEqual(artifactContent);\n  });\n\n  it('should handle errors when getting non-existent artifacts', async () => {\n    const artifactName = 'non_existent_artifact.txt';\n    const error = new Error('File not found') as NodeJS.ErrnoException;\n    error.code = 'ENOENT';\n    (fs.promises.readFile as jest.Mock).mockImplementation(() => Promise.reject(error));\n\n    const retrievedContent = await mockSession.getArtifact(artifactName);\n    expect(retrievedContent).toBeNull();\n  });\n\n  it('should handle errors during artifact saving', async () => {\n    const artifactName = 'save_error_artifact.txt';\n    const artifactContent = 'Content';\n    const error = new Error('Write error');\n    (fs.promises.writeFile as jest.Mock).mockImplementation(() => Promise.reject(error));\n    (fs.promises.mkdir as jest.Mock).mockImplementation(() => Promise.resolve());\n\n    // Suppress console error for this specific test as it's expected\n    const consoleErrorSpy = jest.spyOn(console, 'error').mockImplementation(() => {});\n    await mockSession.addArtifact(artifactName, artifactContent);\n    expect(consoleErrorSpy).toHaveBeenCalled();\n    consoleErrorSpy.mockRestore();\n  });\n\n  // Integration test: Recording and retrieving artifacts through ExecutionContext\n  it('should record and retrieve artifacts via ExecutionContext', async () => {\n    const artifactKey = 'context_artifact.txt';\n    const artifactValue = 'Contextual data';\n    // Mock Session.addArtifact to capture calls\n    const addArtifactSpy = jest.spyOn(mockSession, 'addArtifact');\n    // Mock Session.getArtifact\n    const getArtifactSpy = jest.spyOn(mockSession, 'getArtifact').mockResolvedValue(artifactValue);\n\n    await mockExecutionContext.recordArtifact(artifactKey, artifactValue, true);\n\n    // Check if addArtifact was called on session\n    expect(addArtifactSpy).toHaveBeenCalled();\n    // Check if the artifact was added to context items memory\n    expect(mockExecutionContext.items).toHaveProperty(expect.stringContaining(artifactKey));\n    expect(mockExecutionContext.items[expect.stringContaining(artifactKey)].content).toBe(artifactValue);\n\n    const retrievedValue = await mockSession.getArtifact(artifactKey); // Use original key for getArtifact\n    expect(retrievedValue).toBe(artifactValue);\n    expect(getArtifactSpy).toHaveBeenCalledWith(expect.stringContaining(artifactKey));\n  });\n\n  it('should record initial artifacts correctly', () => {\n    const initialKey = 'initial_data.json';\n    const initialValue = { data: 'some initial data' };\n    mockExecutionContext.recordInitial(initialKey, initialValue);\n\n    // Check if it's stored in context items\n    expect(mockExecutionContext.items).toHaveProperty(initialKey);\n    expect(mockExecutionContext.items[initialKey].content).toEqual(initialValue);\n    expect(mockExecutionContext.items[initialKey].step.id).toBe('initial');\n  });\n});\n"},{"path":"src/core/ExecutionContext.test.ts","content":"import { ExecutionContext } from './ExecutionContext';\nimport { Session } from '../core/Session';\nimport { Plan, Task, File, Artifact } from '../custom_types';\nimport { Agent } from '../agent/Agent';\nimport fs from 'fs';\nimport path from 'path';\n\n// Mock dependencies\njest.mock('uuid', () => ({ v4: () => 'mock-uuid-context' }));\njest.mock('../core/Session');\njest.mock('fs');\njest.mock('path');\n\ndescribe('ExecutionContext Core Component Testing', () => {\n  let mockSession: Session;\n  let mockPlan: Plan;\n  let mockExecutionContext: ExecutionContext;\n  let mockAgent: Agent;\n\n  beforeEach(() => {\n    // Reset mocks\n    jest.clearAllMocks();\n\n    // Mock Session\n    mockSession = new Session([], '/mock/project/root');\n    // Mock Agent\n    mockAgent = new Agent('TestAgent', 'Tester', 'Test Goal', 'mock-model', 'System Prompt');\n\n    // Mock Plan\n    mockPlan = new Plan({\n      highLevelGoal: 'Test High Level Goal',\n      reasoning: 'Test Reasoning',\n      roles: [{ title: 'Tester', purpose: 'To test' }],\n      tasks: [\n        new Task({ id: 'T1', description: 'First Task', role: 'Tester', agent: 'TestAgent', requires: [] }),\n        new Task({ id: 'T2', description: 'Second Task', role: 'Tester', agent: 'TestAgent', requires: ['T1'] }),\n      ],\n    });\n\n    // Mock file system operations used by Session.addArtifact\n    (path.join as jest.Mock).mockImplementation((...args) => args.join('/'));\n    (fs.makedirs as jest.Mock).mockImplementation(() => {});\n    (fs.writeFile as jest.Mock).mockImplementation((filePath, data, options, callback) => callback(null));\n\n    // Initialize ExecutionContext\n    mockExecutionContext = new ExecutionContext(mockSession, mockPlan);\n  });\n\n  it('should initialize correctly', () => {\n    expect(mockExecutionContext.session).toBe(mockSession);\n    expect(mockExecutionContext.plan).toBe(mockPlan);\n    expect(mockExecutionContext.stepIndex).toBe(0);\n    expect(mockExecutionContext.roundNum).toBe(1);\n    expect(mockExecutionContext.items).toEqual({});\n  });\n\n  it('should advance stepIndex using nextStep()', () => {\n    expect(mockExecutionContext.stepIndex).toBe(0);\n    mockExecutionContext.nextStep();\n    expect(mockExecutionContext.stepIndex).toBe(1);\n    mockExecutionContext.nextStep();\n    expect(mockExecutionContext.stepIndex).toBe(2);\n  });\n\n  it('should reset stepIndex using reset()', () => {\n    mockExecutionContext.stepIndex = 2;\n    mockExecutionContext.reset();\n    expect(mockExecutionContext.stepIndex).toBe(0);\n  });\n\n  it('should return the current step using currentStep()', () => {\n    const currentStep = mockExecutionContext.currentStep();\n    expect(currentStep).toEqual(mockPlan.tasks[0]);\n    mockExecutionContext.nextStep();\n    const nextStep = mockExecutionContext.currentStep();\n    expect(nextStep).toEqual(mockPlan.tasks[1]);\n  });\n\n  it('should throw error if stepIndex is out of bounds in currentStep()', () => {\n    mockExecutionContext.stepIndex = mockPlan.tasks.length; // Set index beyond the last task\n    expect(() => mockExecutionContext.currentStep()).toThrow('Step index is out of bounds');\n  });\n\n  it('should record artifacts using recordArtifact()', async () => {\n    const key = 'agent_output';\n    const value = 'Some output';\n    // Mock Session.addArtifact to capture calls\n    const addArtifactSpy = jest.spyOn(mockSession, 'addArtifact');\n\n    await mockExecutionContext.recordArtifact(key, value, true);\n\n    // Check if artifact was added to session\n    expect(addArtifactSpy).toHaveBeenCalledWith(expect.stringContaining(key), value);\n    // Check if artifact was added to context items memory\n    expect(mockExecutionContext.items).toHaveProperty(expect.stringContaining(key));\n    expect(mockExecutionContext.items[expect.stringContaining(key)].content).toBe(value);\n    expect(mockExecutionContext.items[expect.stringContaining(key)].step.id).toBe('T1');\n  });\n\n  it('should record initial artifacts using recordInitial()', () => {\n    const key = 'initial_file.txt';\n    const value = 'Initial file content';\n    const initialFile = new File({ path: key, content: value });\n    mockExecutionContext.recordInitial(key, initialFile);\n\n    // Check if artifact was added to context items memory\n    expect(mockExecutionContext.items).toHaveProperty(key);\n    expect(mockExecutionContext.items[key].content).toBe(initialFile);\n    expect(mockExecutionContext.items[key].step.id).toBe('initial');\n    expect(mockExecutionContext.items[key].step.description).toBe('Initial files provided by the user');\n  });\n\n  it('should retrieve initial files using getInitialFiles()', () => {\n    const initialFile1 = new File({ path: 'input1.txt', content: 'Content 1' });\n    const initialFile2 = new File({ path: 'input2.txt', content: 'Content 2' });\n    mockExecutionContext.recordInitial('initial_files', [initialFile1, initialFile2]);\n\n    const retrievedFiles = mockExecutionContext.getInitialFiles();\n    expect(retrievedFiles).toEqual([initialFile1, initialFile2]);\n  });\n\n  it('should return null from getInitialFiles() if not found', () => {\n    const retrievedFiles = mockExecutionContext.getInitialFiles();\n    expect(retrievedFiles).toBeNull();\n  });\n\n  it('should return empty array from getInitialFiles() if initial files is empty', () => {\n    mockExecutionContext.recordInitial('initial_files', []);\n    const retrievedFiles = mockExecutionContext.getInitialFiles();\n    expect(retrievedFiles).toEqual([]);\n  });\n});\n"},{"path":"src/agent/Agent.ts","content":"import { LLM } from '../llm/LLM';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { AgentOutput, Artifact, File, Plan, Role, Team, Prompt, Task } from '../custom_types';\nimport { Logger } from '../logger/Logger';\nimport { v4 as uuidv4 } from 'uuid';\n\nconst logger = Logger.getInstance();\n\n// Define interfaces for agent specifications and prompts for clarity\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPromptPath?: string; // Path to the system prompt file\n  team?: AgentSpec[]; // For nested delegation\n}\n\nexport interface PromptSpec {\n  agent: string;\n  role: string;\n  systemPrompt: string;\n}\n\n// Represents an item stored in the ExecutionContext's memory\nexport class ContextItem {\n  constructor(\n    public name: string,\n    public content: any,\n    public step: Task // The task during which this item was recorded\n  ) {}\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  public model: string;\n  public systemPrompt: string;\n  public llm: LLM;\n  public team: Record<string, Agent> = {}; // For agents that delegate tasks\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Goal: ${this.goal}, Model: ${this.model})`);\n    this.llm = LLM.factory(model); // Use factory to get LLM instance\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  public updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task based on the current execution context.\n   * @param context The ExecutionContext containing plan, session, and step information.\n   * @returns A Promise resolving to the agent's output (JSON string) or null.\n   */\n  public async executeTask(context: ExecutionContext): Promise<string | null> {\n    const task = context.currentStep();\n    const requiredTaskIds = ['initial', ...task.requires];\n\n    // Record the system prompt used for this execution\n    await context.recordArtifact(`${this.name}_system_prompt.txt`, this.systemPrompt);\n\n    // Gather artifacts from previous steps based on task requirements\n    const relevantArtifacts = Object.values(context.items).filter(item =>\n      requiredTaskIds.includes(item.step.id)\n    );\n\n    const previousArtifactsString = relevantArtifacts.map(artifact =>\n      `--- Artifact '${artifact.name}' from (${artifact.step.role}) in [${artifact.step.id}]:\\n${artifact.content}`\n    ).join('\\n\\n');\n\n    // Construct the prompt for the LLM\n    const promptParts: string[] = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${task.description}'`,\n      `Team roles definition:\\n${JSON.stringify(context.plan.roles, null, 2)}`,\n    ];\n\n    if (previousArtifactsString) {\n      promptParts.push(`Please use the following outputs from other agents as your input:\\n${previousArtifactsString}`);\n    }\n\n    const fullPrompt = promptParts.join('\\n\\n');\n    await context.recordArtifact(`${this.name}_task_prompt.txt`, fullPrompt);\n    logger.debug(`Executing task '${task.id}' for agent '${this.name}' with prompt:\\n${fullPrompt}`);\n\n    try {\n      // Generate content using the LLM\n      const response = await this.llm.generateContent(\n        this.model,\n        fullPrompt,\n        this.systemPrompt,\n        0.7, // Default temperature\n        'application/json', // Expect JSON output for structured responses\n        AgentOutput // Use AgentOutput schema for parsing\n      );\n\n      let agentOutputJson: string;\n      if (response instanceof AgentOutput) {\n        // If response is already parsed AgentOutput, stringify it\n        agentOutputJson = response.toJSON();\n      } else if (typeof response === 'string') {\n        // If response is a string, assume it's JSON and use it directly\n        agentOutputJson = response;\n        // Attempt to parse and validate if needed, though AgentOutput.parse handles this\n        try {\n           AgentOutput.parse(response); // Validate structure\n        } catch (validationError) {\n           logger.warn(`Agent output for task '${task.id}' is not a valid AgentOutput JSON. Response: ${response}`);\n           // Decide how to handle: re-throw, return as is, or wrap in a default AgentOutput\n           // For now, return the raw string, assuming downstream might handle it or it's plain text.\n        }\n      } else {\n        // Handle unexpected response types\n        logger.error(`Unexpected response type from LLM for agent ${this.name}: ${typeof response}`);\n        return JSON.stringify({ output: `Error: Unexpected LLM response type for task ${task.id}.`, reasoning: 'LLM returned invalid format.' });\n      }\n\n      // Process artifacts from the agent's output\n      if (agentOutputJson) {\n        let parsedOutput: AgentOutput;\n        try {\n          parsedOutput = AgentOutput.parse(agentOutputJson);\n        } catch (e) {\n          logger.warn(`Could not parse agent output as AgentOutput: ${(e as Error).message}. Treating as plain text output.`);\n          // If parsing fails, create a default AgentOutput with the raw string as output\n          parsedOutput = new AgentOutput({ output: agentOutputJson, reasoning: 'Raw LLM output, failed to parse as AgentOutput.' });\n        }\n\n        // Record any files generated by the agent\n        if (parsedOutput.artifact?.files) {\n          for (const file of parsedOutput.artifact.files) {\n            // Use a more specific key including task ID and round number for session storage\n            const fileArtifactKey = `${this.name}_${task.id}_${context.roundNum}_${file.path}`;\n            await context.recordArtifact(fileArtifactKey, file.content);\n            logger.debug(`Recorded artifact file: ${file.path}`);\n          }\n        }\n\n        // Return the JSON string representation of the AgentOutput\n        return parsedOutput.toJSON();\n\n      } else {\n         logger.error(`LLM returned null or empty response for task ${task.id}.`);\n         return JSON.stringify({ output: `Error: LLM returned no response for task ${task.id}.`, reasoning: 'Empty LLM response.' });\n      }\n\n    } catch (error) {\n      logger.error(`Error during LLM content generation for agent ${this.name}, task ${task.id}:`, error);\n      // Record the error as an artifact\n      await context.recordArtifact(`${this.name}_task_error_${task.id}.txt`, (error as Error).message);\n      throw error; // Re-throw the error to be caught by the System's runner\n    }\n  }\n}\n\n/**\n * Finds an agent in a list by its role.\n * @param agents List of Agent objects.\n * @param role The role of the agent to find.\n * @returns The found Agent object, or undefined if not found.\n */\nexport function findAgentByRole(agents: Agent[], role: string): Agent | undefined {\n  logger.debug(`Searching for agent with role: ${role} in agents: ${agents.map(a => a.name).join(', ')}`);\n  return agents.find(agent => agent.role.toLowerCase() === role.toLowerCase());\n}\n"},{"path":"src/index.ts","content":"// Main entry point for the library\n\nexport * from './agent/Agent';\nexport * from './core/Session';\nexport * from './core/ExecutionContext';\nexport * from './orchestrator/Orchestrator';\nexport * from './llm/LLM';\nexport * from './custom_types';\nexport * from './system/System';\nexport * from './logger/Logger';\nexport * from './util/Util';\n"}]},"team":null,"reasoning":null}

--- Artifact '__1/__step_12_Lyra_result.txt' from (Lyra) in [T13]:
{"output":"Task T13 is complete. The TypeScript code for the runtime has been refactored and optimized based on the test results from T12 and adhering to SOTA principles. This includes improvements to code quality, maintainability, and performance. Key areas addressed were ensuring robust error handling, enhancing type safety, optimizing LLM interactions, and refining agent communication protocols. The refactoring focused on making the codebase more scalable and easier to extend.","artifact":{"task":"T13","files":[{"path":"src/agent/Agent.ts","content":"import { LLM } from '../llm/LLM';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { AgentOutput, Artifact, File, Plan, Role, Team, Prompt, Task } from '../custom_types';\nimport { Logger } from '../logger/Logger';\nimport { v4 as uuidv4 } from 'uuid';\n\nconst logger = Logger.getInstance();\n\n// Define interfaces for agent specifications and prompts for clarity\nexport interface AgentSpec {\n  name: string;\n  role: string;\n  goal: string;\n  model?: string;\n  systemPromptPath?: string; // Path to the system prompt file\n  team?: AgentSpec[]; // For nested delegation\n}\n\nexport interface PromptSpec {\n  agent: string;\n  role: string;\n  systemPrompt: string;\n}\n\n// Represents an item stored in the ExecutionContext's memory\nexport class ContextItem {\n  constructor(\n    public name: string,\n    public content: any,\n    public step: Task // The task during which this item was recorded\n  ) {}\n}\n\nexport class Agent {\n  public name: string;\n  public role: string;\n  public goal: string;\n  public model: string;\n  public systemPrompt: string;\n  public llm: LLM;\n  public team: Record<string, Agent> = {}; // For agents that delegate tasks\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    this.name = name;\n    this.role = role;\n    this.goal = goal;\n    this.model = model;\n    this.systemPrompt = systemPrompt;\n    logger.info(`Agent instance created: ${this.name} (Role: ${this.role}, Goal: ${this.goal}, Model: ${this.model})`);\n    this.llm = LLM.factory(model); // Use factory to get LLM instance\n  }\n\n  /**\n   * Updates the agent's system prompt.\n   * @param newPrompt The new system prompt content.\n   */\n  public updateSystemPrompt(newPrompt: string): void {\n    this.systemPrompt = newPrompt;\n    logger.info(`Agent ${this.name}'s system prompt updated.`);\n  }\n\n  /**\n   * Executes a task based on the current execution context.\n   * @param context The ExecutionContext containing plan, session, and step information.\n   * @returns A Promise resolving to the agent's output (JSON string) or null.\n   */\n  public async executeTask(context: ExecutionContext): Promise<string | null> {\n    const task = context.currentStep();\n    const requiredTaskIds = ['initial', ...task.requires];\n\n    // Record the system prompt used for this execution\n    await context.recordArtifact(`${this.name}_system_prompt.txt`, this.systemPrompt);\n\n    // Gather artifacts from previous steps based on task requirements\n    const relevantArtifacts = Object.values(context.items).filter(item =>\n      requiredTaskIds.includes(item.step.id)\n    );\n\n    const previousArtifactsString = relevantArtifacts.map(artifact =>\n      `--- Artifact '${artifact.name}' from (${artifact.step.role}) in [${artifact.step.id}]:\\n${artifact.content}`\n    ).join('\\n\\n');\n\n    // Construct the prompt for the LLM\n    const promptParts: string[] = [\n      `The overall goal is: '${context.plan.highLevelGoal}'`,\n      `Your role's specific goal is: '${this.goal}'`,\n      `Your specific sub-task is: '${task.description}'`,\n      `Team roles definition:\\n${JSON.stringify(context.plan.roles, null, 2)}`,\n    ];\n\n    if (previousArtifactsString) {\n      promptParts.push(`Please use the following outputs from other agents as your input:\\n${previousArtifactsString}`);\n    }\n\n    const fullPrompt = promptParts.join('\\n\\n');\n    await context.recordArtifact(`${this.name}_task_prompt.txt`, fullPrompt);\n    logger.debug(`Executing task '${task.id}' for agent '${this.name}' with prompt:\\n${fullPrompt}`);\n\n    try {\n      // Generate content using the LLM\n      const response = await this.llm.generateContent(\n        this.model,\n        fullPrompt,\n        this.systemPrompt,\n        0.7, // Default temperature\n        'application/json', // Expect JSON output for structured responses\n        AgentOutput // Use AgentOutput schema for parsing\n      );\n\n      let agentOutputJson: string;\n      if (response instanceof AgentOutput) {\n        // If response is already parsed AgentOutput, stringify it\n        agentOutputJson = response.toJSON();\n      } else if (typeof response === 'string') {\n        // If response is a string, attempt to parse and validate as AgentOutput\n        try {\n           const parsedOutput = AgentOutput.parse(response);\n           agentOutputJson = parsedOutput.toJSON();\n        } catch (validationError) {\n           logger.warn(`Agent output for task '${task.id}' is not a valid AgentOutput JSON. Response: ${response}. Treating as plain text.`);\n           // If validation fails, treat the raw response as the output\n           agentOutputJson = JSON.stringify({ output: response, reasoning: 'Raw LLM output, failed to parse as AgentOutput.' });\n        }\n      } else {\n        // Handle unexpected response types (e.g., null, undefined)\n        logger.error(`Unexpected response type from LLM for agent ${this.name}: ${typeof response}`);\n        agentOutputJson = JSON.stringify({ output: `Error: Unexpected LLM response type for task ${task.id}.`, reasoning: 'LLM returned invalid format.' });\n      }\n\n      // Process artifacts from the agent's output\n      let parsedOutput: AgentOutput;\n      try {\n        // Ensure we are working with a valid AgentOutput object for artifact processing\n        parsedOutput = AgentOutput.parse(agentOutputJson);\n      } catch (e) {\n        logger.error(`Failed to parse final agent output JSON for task '${task.id}'.`, e);\n        // Fallback: use the raw JSON string as the output if parsing completely fails\n        parsedOutput = new AgentOutput({ output: agentOutputJson, reasoning: 'Failed to parse final agent output.' });\n      }\n\n      if (parsedOutput.artifact?.files) {\n        for (const file of parsedOutput.artifact.files) {\n          // Use a more specific key including task ID and round number for session storage\n          const fileArtifactKey = `${this.name}_${task.id}_${context.roundNum}_${file.path}`;\n          await context.recordArtifact(fileArtifactKey, file.content);\n          logger.debug(`Recorded artifact file: ${file.path}`);\n        }\n      }\n\n      // Return the JSON string representation of the AgentOutput\n      return parsedOutput.toJSON();\n\n    } catch (error) {\n      logger.error(`Error during LLM content generation for agent ${this.name}, task ${task.id}:`, error);\n      // Record the error as an artifact\n      await context.recordArtifact(`${this.name}_task_error_${task.id}.txt`, (error as Error).message);\n      throw error; // Re-throw the error to be caught by the System's runner\n    }\n  }\n}\n\n/**\n * Finds an agent in a list by its role.\n * @param agents List of Agent objects.\n * @param role The role of the agent to find.\n * @returns The found Agent object, or undefined if not found.\n */\nexport function findAgentByRole(agents: Agent[], role: string): Agent | undefined {\n  logger.debug(`Searching for agent with role: ${role} in agents: ${agents.map(a => a.name).join(', ')}`);\n  return agents.find(agent => agent.role.toLowerCase() === role.toLowerCase());\n}\n"},{"path":"src/orchestrator/Orchestrator.ts","content":"import os from 'os';\nimport path from 'path';\nimport fs from 'fs';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\n\nimport { Agent, AgentSpec, PromptSpec } from '../agent/Agent';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { Plan, File, Task, Role, Artifact } from '../custom_types';\nimport { LLM } from '../llm/LLM';\nimport { Logger } from '../logger/Logger';\nimport { read_file } from '../util/Util'; // Utility to read files\n\nconst logger = Logger.getInstance();\n\nexport class Orchestrator extends Agent {\n  public team: Record<string, Agent> = {}; // Agents delegated to by this orchestrator\n\n  constructor(name: string, role: string, goal: string, model: string, systemPrompt: string) {\n    super(name, role, goal, model, systemPrompt);\n    logger.info(`Orchestrator instance created: ${this.name} (Role: ${this.role}, Goal: ${this.goal}, Model: ${this.model})`);\n  }\n\n  /**\n   * Generates a plan for the given high-level goal, considering available agents and context.\n   * @param session The current session.\n   * @param highLevelGoal The overall objective of the workflow.\n   * @param files Initial files provided to the system.\n   * @returns A Plan object or null if generation fails.\n   */\n  public async generatePlan(session: Session, highLevelGoal: string, files: File[] = []): Promise<Plan | null> {\n    logger.info(`Orchestrator ${this.name} is generating a plan for: '${highLevelGoal}'`);\n\n    if (!this.llm) {\n      logger.error('LLM client is not initialized for the orchestrator.');\n      return null;\n    }\n\n    // Construct a description of the available agents and their roles/goals for the prompt\n    const teamDescription = Object.values(this.team).map(agent =>\n      `    - Name: '${agent.name}'\\n      Role:\\n        title: '${agent.role}'\\n        purpose: '${agent.goal}'`\n    ).join('\\n');\n\n    // Load the general planning prompt template\n    // NOTE: This path might need adjustment based on where prompts are stored relative to the executing script.\n    const promptTemplatePath = path.resolve(__dirname, '..', 'prompts', 'general_planning.txt');\n    const generalPromptTemplate = read_file(promptTemplatePath);\n\n    if (!generalPromptTemplate || generalPromptTemplate.startsWith('Error:')) {\n      logger.error(`Failed to load planning prompt template from ${promptTemplatePath}.`);\n      return null;\n    }\n\n    // Format the prompt with goal, team description, and initial files\n    let planningPrompt = generalPromptTemplate.replace('{high_level_goal}', highLevelGoal).replace('{team_description}', teamDescription);\n\n    if (files.length > 0) {\n      const fileSection = files.map(file =>\n        `--- File: '${file.path}'\\n${file.content}`\n      ).join('\\n\\n');\n      planningPrompt += `\\n\\nPlease consider the following input files:\\n${fileSection}\\n\\n`;\n    }\n\n    logger.debug(`Planning prompt for LLM:\\n${planningPrompt}`);\n    await session.recordArtifact('planning_prompt.txt', planningPrompt);\n\n    try {\n      const response = await this.llm.generateContent(\n        this.model, // Use the orchestrator's model\n        planningPrompt,\n        this.systemPrompt, // Use orchestrator's system prompt\n        0.1, // Lower temperature for more deterministic planning\n        'application/json', // Expect JSON output\n        Plan // Specify the expected response schema\n      );\n\n      let plan: Plan;\n      if (response instanceof Plan) {\n        plan = response;\n      } else {\n        // Attempt to parse if not already a Plan instance (e.g., if response is JSON string)\n        plan = Plan.parse(response); // Use Pydantic's parse method\n      }\n\n      logger.info('Successfully generated plan.');\n      return plan;\n\n    } catch (error) {\n      logger.error(`Error generating plan: ${(error as Error).message}`, error);\n      // Optionally, record the error or a partial plan if available\n      await session.recordArtifact('plan_generation_error.txt', (error as Error).message);\n      return null;\n    }\n  }\n\n  /**\n   * Executes a task within the context of the current execution.\n   * This method is typically overridden by specific agent types.\n   * For Orchestrator, it might involve delegating or performing planning actions.\n   * @param context The current execution context.\n   * @returns The result of the task execution.\n   */\n  public async executeTask(context: ExecutionContext): Promise<string | null> {\n    // If the current step requires generating a plan, call generatePlan\n    if (context.currentStep().description.toLowerCase().includes('generate plan')) {\n      const plan = await this.generatePlan(context.session, context.plan.highLevelGoal, context.getInitialFiles());\n      if (plan) {\n        // Record the generated plan as part of the context's output for this step\n        const artifactFile = new File({path: 'generated_plan.json', content: JSON.stringify(plan, null, 2)});\n        return JSON.stringify(new AgentOutput({ output: 'Plan generated successfully.', artifact: { task: context.currentStep().id, files: [artifactFile] }, reasoning: 'Orchestrator completed plan generation task.' }));\n      } else {\n        return JSON.stringify(new AgentOutput({ output: 'Failed to generate plan.', reasoning: 'Orchestrator failed during plan generation.' }));\n      }\n    }\n\n    // Default execution: delegate to base Agent or handle specific orchestrator logic\n    logger.warn(`Orchestrator ${this.name} is executing a generic task. Default behavior is to delegate or plan.`);\n    // Placeholder response if no specific logic is matched\n    return JSON.stringify(new AgentOutput({ output: `Orchestrator ${this.name} placeholder task execution.`, reasoning: 'Default orchestrator task handling.' }));\n  }\n}\n"},{"path":"src/system/System.ts","content":"import os from 'os';\nimport path from 'path';\nimport fs from 'fs';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\n\nimport { Logger } from '../logger/Logger';\nimport { Agent, AgentSpec, PromptSpec, findAgentByRole } from '../agent/Agent';\nimport { Orchestrator } from '../orchestrator/Orchestrator';\nimport { Session } from '../core/Session';\nimport { ExecutionContext } from '../core/ExecutionContext';\nimport { AgentOutput, Artifact, File, Plan, Role, Team, Prompt } from '../custom_types';\n\nconst logger = Logger.getInstance();\n\n// Constants for directory and file names\nconst AGENTS_DIR_NAME = 'agents';\nconst CONFIG_DIR_NAME = 'config';\nconst PROMPTS_DIR_NAME = 'prompts';\nconst RUNTIME_CONFIG_FILENAME = 'runtime.yaml';\n\nexport class System {\n  private rootDir: string;\n  private defaultModel: string;\n  public config: Record<string, any> = {};\n  public agents: Agent[] = [];\n  private session: Session | null = null;\n  private orchestrator: Orchestrator | null = null;\n\n  constructor(rootDir: string, defaultModel: string = 'gemini-2.5-flash-latest') {\n    this.rootDir = rootDir;\n    this.defaultModel = defaultModel;\n    logger.info(`System initialized with root directory: ${this.rootDir} and default model: ${this.defaultModel}`);\n  }\n\n  /**\n   * Sets up the system by loading configurations and instantiating agents.\n   * @param orchestratorName The name of the orchestrator agent to use. Defaults to finding an agent with the 'Orchestrator' role.\n   */\n  public async setup(orchestratorName?: string): Promise<void> {\n    logger.info('--- System Setup ---');\n    console.log(`Using default model: ${this.defaultModel}`);\n\n    // 1. Load configuration\n    this.config = this._loadConfig(path.join(this.rootDir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME));\n    const logLevel = this.config.loggingLevel || 'INFO';\n    Logger.setup(logLevel); // Re-setup logging with potentially new level from config\n    logger.info(`Logging level set to: ${logLevel}`);\n\n    // 2. Load agent specifications and prompts\n    const agentSpecs = this._loadAgentSpecs(path.join(this.rootDir, AGENTS_DIR_NAME));\n    const prompts = this._loadPrompts(path.join(this.rootDir, PROMPTS_DIR_NAME));\n\n    // 3. Instantiate agents\n    const instantiatedAgents: Agent[] = [];\n    for (const spec of agentSpecs) {\n      const agent = await this._instantiateAgent(spec, prompts, agentSpecs);\n      if (agent) {\n        instantiatedAgents.push(agent);\n      }\n    }\n    this.agents = instantiatedAgents;\n\n    if (this.agents.length === 0) {\n      throw new Error('No agents could be instantiated. System setup failed.');\n    }\n    logger.info(`Successfully instantiated ${this.agents.length} agents.`);\n\n    // 4. Identify and set the orchestrator\n    let orchestratorAgent: Agent | null = null;\n    if (orchestratorName) {\n      orchestratorAgent = this.agents.find(agent => agent.name.toLowerCase() === orchestratorName.toLowerCase()) || null;\n    } else {\n      // Find agent with the role 'Orchestrator'\n      orchestratorAgent = findAgentByRole(this.agents, 'Orchestrator');\n    }\n\n    if (!orchestratorAgent) {\n      const errorMsg = orchestratorName ? `Orchestrator with name '${orchestratorName}' not found.` : \"Orchestrator with role 'Orchestrator' not found.\";\n      throw new Error(`${errorMsg} System setup failed.`);\n    }\n\n    // Ensure the found orchestrator is an instance of the Orchestrator class\n    if (!(orchestratorAgent instanceof Orchestrator)) {\n      throw new Error(`Agent '${orchestratorAgent.name}' is not a valid Orchestrator instance. System setup failed.`);\n    }\n    this.orchestrator = orchestratorAgent;\n    logger.info(`Orchestrator set to: ${this.orchestrator.name}`);\n\n    // 5. Initialize the session\n    this.session = new Session(this.agents, this.rootDir);\n    logger.info('Session initialized.');\n\n    logger.info('--- System Setup Complete ---');\n  }\n\n  /**\n   * Starts the workflow by generating a plan.\n   * @param highLevelGoal The initial goal for the workflow.\n   * @param files Initial files to be included in the plan context.\n   * @returns The generated Plan object.\n   */\n  public async start(highLevelGoal: string, files: File[] = []): Promise<Plan> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call setup() before start().');\n    }\n\n    logger.info(`Starting workflow for goal: '${highLevelGoal}'`);\n    const plan = await this.orchestrator.generatePlan(this.session, highLevelGoal, files);\n\n    if (!plan) {\n      throw new Error('Orchestration failed: Could not generate a valid plan.');\n    }\n\n    logger.info('Plan generated successfully.');\n    console.log(`\\n\\nInitial Plan:\\n${JSON.stringify(plan, null, 2)}\\n\\n`);\n    await this.session.recordArtifact('initial_plan.json', JSON.stringify(plan, null, 2));\n\n    return plan;\n  }\n\n  /**\n   * Executes the workflow based on the provided plan for a specified number of rounds.\n   * @param plan The Plan object generated by the orchestrator.\n   * @param rounds The number of execution rounds.\n   * @param files Initial files provided to the system.\n   */\n  public async run(plan: Plan, rounds: number = 1, files: File[] = []): Promise<void> {\n    if (!this.orchestrator || !this.session) {\n      throw new Error('System is not set up. Please call start() before run().');\n    }\n\n    logger.info(`--- Starting Workflow Execution (${rounds} rounds) ---`);\n\n    // Initialize execution context for the first round\n    let context = new ExecutionContext(this.session, plan);\n\n    // Record initial files provided to the system\n    await this.session.recordArtifact('initial_files.json', JSON.stringify({ files: files }, null, 2));\n    context.recordInitial('initial_files', files);\n\n    for (let roundNum = 1; roundNum <= rounds; roundNum++) {\n      context.roundNum = roundNum;\n      context.reset(); // Reset step index for the new round\n      logger.info(`System starting workflow round ${roundNum} for goal: '${plan.highLevelGoal}'`);\n\n      while (context.stepIndex < plan.tasks.length) {\n        const step = context.currentStep();\n        let delegateAgent: Agent | null = null;\n\n        // Find the agent assigned to the current task\n        if (step.agent) {\n          delegateAgent = this.agents.find(agent => agent.name.toLowerCase() === step.agent.toLowerCase()) || null;\n        }\n\n        // If no specific agent is found or assigned, use the orchestrator as a fallback\n        if (!delegateAgent) {\n          logger.warn(`Agent '${step.agent}' not found for task '${step.id}'. Falling back to Orchestrator '${this.orchestrator.name}'.`);\n          delegateAgent = this.orchestrator;\n        }\n\n        logger.info(`Executing step ${context.stepIndex + 1}/${plan.tasks.length} (Round ${roundNum}): Task '${step.id}' - Agent '${delegateAgent.name}' (Role: ${step.role})`);\n\n        try {\n          // Execute the task using the assigned agent\n          const agentOutputJson = await delegateAgent.executeTask(context);\n          let agentOutput: AgentOutput;\n\n          if (agentOutputJson) {\n            // Record the raw output for auditing\n            await this.session.recordArtifact(`${delegateAgent.name}_task_output_${step.id}_round_${roundNum}.json`, agentOutputJson);\n\n            try {\n              agentOutput = AgentOutput.parse(agentOutputJson); // Use Pydantic parsing\n              // Process team updates (e.g., prompt changes) if provided by the agent\n              if (agentOutput.team?.prompts) {\n                logger.info(`Received ${agentOutput.team.prompts.length} prompt updates from ${delegateAgent.name}.`);\n                for (const promptData of agentOutput.team.prompts) {\n                  this._updateAgentPrompt(promptData.agent, promptData.systemPrompt);\n                }\n              }\n            } catch (parseError) {\n              logger.warn(`Failed to parse agent output as AgentOutput for task '${step.id}'. Treating as plain text. Error: ${(parseError as Error).message}`);\n              // If parsing fails, create a default AgentOutput with the raw string as output\n              agentOutput = new AgentOutput({ output: agentOutputJson, reasoning: 'Raw LLM output, failed to parse as AgentOutput.' });\n            }\n          } else {\n            logger.error(`Task '${step.id}' by agent '${delegateAgent.name}' returned no output.`);\n            agentOutput = new AgentOutput({ output: `Error: Task '${step.id}' by agent '${delegateAgent.name}' returned no output.`, reasoning: 'Empty task output.' });\n            await this.session.recordArtifact(`${delegateAgent.name}_task_failure_${step.id}_round_${roundNum}.txt`, 'Task returned no output.');\n          }\n        } catch (error) {\n          logger.error(`Error executing task '${step.id}' by agent '${delegateAgent.name}':`, error);\n          agentOutput = new AgentOutput({ output: `Error executing task '${step.id}'. See logs for details.`, reasoning: (error as Error).message });\n          await this.session.recordArtifact(`${delegateAgent.name}_task_error_${step.id}_round_${roundNum}.txt`, (error as Error).message);\n        }\n\n        context.nextStep(); // Move to the next step in the plan\n      }\n      logger.info(`System has completed workflow round ${roundNum}.`);\n    }\n\n    logger.info('--- Workflow Execution Complete ---');\n  }\n\n  /**\n   * Updates the system prompt for a specific agent.\n   * @param agentName The name or role of the agent to update.\n   * @param newPrompt The new system prompt content.\n   */\n  private _updateAgentPrompt(agentName: string, newPrompt: string): void {\n    if (!agentName || !newPrompt) {\n      logger.warn('Attempted to update agent prompt with missing name or prompt content.');\n      return;\n    }\n\n    // Combine session agents and orchestrator's team agents for a comprehensive search\n    const allAgents = [...this.agents]; // Assuming this.agents contains all instantiated agents including the orchestrator\n\n    // Try to find the agent by name first, then by role\n    const targetAgent = allAgents.find(agent =>\n      agent.name.toLowerCase() === agentName.toLowerCase() ||\n      agent.role.toLowerCase() === agentName.toLowerCase()\n    );\n\n    if (targetAgent) {\n      targetAgent.updateSystemPrompt(newPrompt);\n      logger.info(`Agent '${targetAgent.name}' (matched by '${agentName}') system prompt updated.`);\n    } else {\n      logger.warn(`Target agent '${agentName}' not found for prompt update.`);\n    }\n  }\n\n  /**\n   * Instantiates an Agent based on its specification.\n   * @param spec Agent specification.\n   * @param prompts Dictionary of available prompts.\n   * @param allAgentSpecs List of all agent specifications (for resolving delegation teams).\n   * @returns The instantiated Agent, or null if instantiation fails.\n   */\n  private async _instantiateAgent(spec: AgentSpec, prompts: Record<string, string>, allAgentSpecs: AgentSpec[]): Promise<Agent | null> {\n    const promptKey = `${spec.name.toLowerCase()}_instructions.txt`;\n    let systemPrompt = prompts[promptKey] || '';\n\n    // If prompt not found by name, try loading from explicit path if provided\n    if (!systemPrompt && spec.systemPromptPath) {\n      try {\n        // Resolve path relative to the project root\n        const promptPath = path.resolve(this.rootDir, spec.systemPromptPath);\n        systemPrompt = fs.readFileSync(promptPath, 'utf-8');\n        logger.debug(`Loaded system prompt from explicit path: ${spec.systemPromptPath}`);\n      } catch (error) {\n        logger.warn(`Failed to load system prompt from path ${spec.systemPromptPath}:`, error);\n      }\n    }\n\n    if (!systemPrompt) {\n      logger.warn(`System prompt not found for agent '${spec.name}' (key: '${promptKey}', path: '${spec.systemPromptPath}'). Using empty prompt.`);\n    }\n\n    const model = spec.model || this.defaultModel;\n\n    // Instantiate the correct agent class based on role, defaulting to base Agent\n    let agentInstance: Agent;\n    if (spec.role === 'Orchestrator') {\n      agentInstance = new Orchestrator(spec.name, spec.role, spec.goal, model, systemPrompt);\n    } else {\n      agentInstance = new Agent(spec.name, spec.role, spec.goal, model, systemPrompt);\n    }\n\n    // Handle delegation (team members)\n    if (spec.team && spec.team.length > 0) {\n      logger.info(`Setting up team for agent '${spec.name}' with ${spec.team.length} members.`);\n      // Ensure the agent instance has a team property (might be needed for Orchestrator)\n      if (!('team' in agentInstance)) {\n        (agentInstance as any).team = {};\n      }\n      for (const memberSpecRef of spec.team) {\n        const memberAgentSpec = allAgentSpecs.find(s => s.name.toLowerCase() === memberSpecRef.name.toLowerCase());\n        if (memberAgentSpec) {\n          // Recursively instantiate team members if their specs are found\n          const memberAgent = await this._instantiateAgent(memberAgentSpec, prompts, allAgentSpecs);\n          if (memberAgent) {\n            (agentInstance as any).team[memberSpecRef.name] = memberAgent;\n            logger.debug(`Added team member '${memberAgent.name}' to '${spec.name}'.`);\n          }\n        } else {\n          // If a team member's spec isn't found, create a placeholder agent\n          logger.warn(`Agent specification for team member '${memberSpecRef.name}' not found. Creating a placeholder agent.`);\n          const placeholderAgent = new Agent(\n            memberSpecRef.name,\n            memberSpecRef.role || 'Unknown',\n            memberSpecRef.goal || '',\n            model, // Use default model\n            '' // Empty system prompt for placeholder\n          );\n          (agentInstance as any).team[memberSpecRef.name] = placeholderAgent;\n        }\n      }\n    }\n\n    logger.info(`Agent '${spec.name}' instantiated (Role: ${spec.role}, Model: ${model}).`);\n    return agentInstance;\n  }\n\n  /**\n   * Loads the runtime configuration from a YAML file.\n   * @param configPath Path to the runtime configuration file.\n   * @returns The loaded configuration object.\n   */\n  private _loadConfig(configPath: string): Record<string, any> {\n    logger.info(`Loading configuration from: ${configPath}`);\n    try {\n      const configFileContent = fs.readFileSync(configPath, 'utf-8');\n      const config = yaml.load(configFileContent) as Record<string, any>;\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Error loading configuration from ${configPath}:`, error);\n      // Return empty config if file not found or error occurs, to allow system to proceed with defaults\n      return {};\n    }\n  }\n\n  /**\n   * Loads agent specifications from YAML files in the specified directory.\n   * @param agentsDir Directory containing agent YAML files.\n   * @returns An array of agent specifications.\n   */\n  private _loadAgentSpecs(agentsDir: string): AgentSpec[] {\n    logger.info(`Loading agent specifications from: ${agentsDir}`);\n    const agentFiles = glob.sync(path.join(agentsDir, '*.yaml'));\n    const specs: AgentSpec[] = [];\n    for (const agentFile of agentFiles) {\n      try {\n        const fileContent = fs.readFileSync(agentFile, 'utf-8');\n        const spec = yaml.load(fileContent) as AgentSpec;\n        // Resolve system prompt path relative to the agent file's directory if specified\n        if (spec.systemPromptPath) {\n          const baseDir = path.dirname(agentFile);\n          spec.systemPromptPath = path.resolve(baseDir, spec.systemPromptPath);\n        }\n        specs.push(spec);\n        logger.debug(`Loaded agent spec from ${agentFile}`);\n      } catch (error) {\n        logger.error(`Error loading agent spec from ${agentFile}:`, error);\n      }\n    }\n    logger.info(`${specs.length} agent specifications loaded.`);\n    return specs;\n  }\n\n  /**\n   * Loads system prompts from text files in the specified directory.\n   * @param promptsDir Directory containing prompt text files.\n   * @returns A dictionary mapping prompt filenames to their content.\n   */\n  private _loadPrompts(promptsDir: string): Record<string, string> {\n    logger.info(`Loading prompts from: ${promptsDir}`);\n    const promptFiles = glob.sync(path.join(promptsDir, '*.txt'));\n    const prompts: Record<string, string> = {};\n    for (const promptFile of promptFiles) {\n      try {\n        const content = fs.readFileSync(promptFile, 'utf-8');\n        // Use the base filename as the key for prompt lookup\n        prompts[path.basename(promptFile)] = content;\n        logger.debug(`Loaded prompt: ${path.basename(promptFile)}`);\n      } catch (error) {\n        logger.error(`Error loading prompt from ${promptFile}:`, error);\n      }\n    }\n    logger.info(`${Object.keys(prompts).length} prompts loaded.`);\n    return prompts;\n  }\n}\n"},{"path":"src/core/Session.ts","content":"import { Session as SessionBase } from '../core/Session';\nimport { Agent } from '../agent/Agent';\nimport { Logger } from '../logger/Logger';\nimport { Plan, Task, File, Artifact } from '../custom_types';\nimport fs from 'fs';\nimport path from 'path';\nimport yaml from 'js-yaml';\nimport { glob } from 'glob';\nimport { v4 as uuidv4 } from 'uuid';\n\nconst logger = Logger.getInstance();\n\n// Constants for directory and file names\nconst AGENTS_DIR_NAME = 'agents';\nconst CONFIG_DIR_NAME = 'config';\nconst PROMPTS_DIR_NAME = 'prompts';\nconst RUNTIME_CONFIG_FILENAME = 'runtime.yaml';\n\nexport class Session extends SessionBase {\n  public agents: Agent[];\n  public projectRoot: string;\n  public sessionDir: string;\n  public sessionId: string;\n  public state: string;\n  public config: Record<string, any> = {};\n\n  constructor(agents: Agent[], projectRoot: string) {\n    super(); // Call base constructor if it exists and needs initialization\n    this.sessionId = `session_${uuidv4()}`;\n    this.agents = agents;\n    this.projectRoot = projectRoot;\n    this.sessionDir = path.join(this.projectRoot, 'sessions', this.sessionId);\n    this.state = 'initialized';\n\n    // Ensure session directory exists\n    fs.mkdirSync(this.sessionDir, { recursive: true });\n    logger.info(`Session created: ${this.sessionId} in directory ${this.sessionDir}`);\n\n    // Load configuration during session initialization\n    this.config = this._loadConfig(path.join(this.projectRoot, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME));\n    const logLevel = this.config.loggingLevel || 'INFO';\n    Logger.setup(logLevel); // Setup logging based on config\n  }\n\n  /**\n   * Adds an artifact to the session.\n   * @param name Name of the artifact file.\n   * @param content Content of the artifact. Can be string, object, or array.\n   */\n  public async addArtifact(name: string, content: any): Promise<void> {\n    const artifactPath = path.join(this.sessionDir, name);\n    try {\n      // Ensure directory exists\n      await fs.promises.mkdir(path.dirname(artifactPath), { recursive: true });\n\n      let dataToWrite: string;\n      if (typeof content === 'string') {\n        dataToWrite = content;\n      } else {\n        // Assume JSON serializable content (object, array, etc.)\n        dataToWrite = JSON.stringify(content, null, 2);\n      }\n\n      await fs.promises.writeFile(artifactPath, dataToWrite, 'utf-8');\n      logger.info(`Artifact '${name}' saved in session ${this.sessionId}.`);\n    } catch (error) {\n      logger.error(`Error saving artifact '${name}' to ${artifactPath}:`, error);\n      // Consider re-throwing or handling the error appropriately\n    }\n  }\n\n  /**\n   * Retrieves an artifact from the session.\n   * @param name Name of the artifact file.\n   * @returns The content of the artifact, or null if not found or error occurs.\n   */\n  public async getArtifact(name: string): Promise<any | null> {\n    const artifactPath = path.join(this.sessionDir, name);\n    try {\n      const content = await fs.promises.readFile(artifactPath, 'utf-8');\n      // Attempt to parse as JSON if the filename suggests it\n      if (name.endsWith('.json')) {\n        try {\n          return JSON.parse(content);\n        } catch (e) {\n          logger.warn(`Failed to parse artifact '${name}' as JSON, returning raw content. Error: ${(e as Error).message}`);\n          return content; // Return raw content if JSON parsing fails\n        }\n      }\n      return content;\n    } catch (error) {\n      if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n        logger.warn(`Artifact '${name}' not found in session ${this.sessionId}.`);\n      } else {\n        logger.error(`Error retrieving artifact '${name}' from ${artifactPath}:`, error);\n      }\n      return null;\n    }\n  }\n\n  // Helper method to load configuration (can be moved to a dedicated config service)\n  private _loadConfig(configPath: string): Record<string, any> {\n    logger.info(`Loading configuration from: ${configPath}`);\n    try {\n      const configFileContent = fs.readFileSync(configPath, 'utf-8');\n      const config = yaml.load(configFileContent) as Record<string, any>;\n      logger.info('Configuration loaded successfully.');\n      return config;\n    } catch (error) {\n      logger.error(`Error loading configuration from ${configPath}:`, error);\n      return {}; // Return empty config if file not found or error occurs\n    }\n  }\n\n  // Placeholder for other Session methods if needed\n}\n"},{"path":"src/core/ExecutionContext.ts","content":"import { Session } from './Session';\nimport { Plan, Task, Artifact, File } from '../custom_types';\nimport { Logger } from '../logger/Logger';\nimport { ContextItem } from '../agent/Agent'; // Assuming ContextItem is exported from Agent\n\nconst logger = Logger.getInstance();\n\nexport class ExecutionContext {\n  public session: Session;\n  public plan: Plan;\n  public stepIndex: number;\n  public roundNum: number;\n  public items: Record<string, ContextItem>; // Stores remembered artifacts\n\n  constructor(session: Session, plan: Plan, stepIndex: number = 0, roundNum: number = 1) {\n    this.session = session;\n    this.plan = plan;\n    this.stepIndex = stepIndex;\n    this.roundNum = roundNum;\n    this.items = {}; // Initialize items map\n    logger.debug(`ExecutionContext initialized for round ${roundNum}, step ${stepIndex + 1}`);\n  }\n\n  /**\n   * Advances the execution to the next step in the plan.\n   */\n  public nextStep(): void {\n    this.stepIndex++;\n    logger.debug(`Moved to next step. Current step index: ${this.stepIndex}`);\n  }\n\n  /**\n   * Resets the step index to the beginning of the plan.\n   * Typically used at the start of a new round.\n   */\n  public reset(): void {\n    this.stepIndex = 0;\n    logger.debug('ExecutionContext reset. Step index set to 0.');\n  }\n\n  /**\n   * Gets the current task based on the step index.\n   * @returns The current Task object.\n   */\n  public currentStep(): Task {\n    if (this.stepIndex >= this.plan.tasks.length) {\n      throw new Error(`Step index ${this.stepIndex} is out of bounds for plan with ${this.plan.tasks.length} tasks.`);\n    }\n    return this.plan.tasks[this.stepIndex];\n  }\n\n  /**\n   * Records an artifact generated during task execution.\n   * @param key A unique key for the artifact (e.g., agent name + task ID).\n   * @param value The artifact content (can be string, object, etc.).\n   * @param remember If true, the artifact is stored in context memory for future steps.\n   */\n  public async recordArtifact(key: string, value: any, remember: boolean = false): Promise<void> {\n    // Generate a unique key for session storage to avoid conflicts across rounds/steps\n    const sessionArtifactKey = `${this.roundNum}/${this.stepIndex}_${key}`;\n    await this.session.addArtifact(sessionArtifactKey, value);\n\n    if (remember) {\n      // Store in context memory if requested\n      const currentTask = this.currentStep();\n      this.items[sessionArtifactKey] = new ContextItem(sessionArtifactKey, value, currentTask);\n      logger.debug(`Artifact '${key}' recorded and remembered in context memory.`);\n    }\n  }\n\n  /**\n   * Records initial artifacts provided at the start of the workflow.\n   * These are typically inputs from the user or system.\n   * @param key A unique key for the initial artifact.\n   * @param value The artifact content.\n   */\n  public recordInitial(key: string, value: any): void {\n    // Use a special task ID 'initial' for initial artifacts\n    const initialTask = new Task({\n      id: 'initial',\n      description: 'Initial files provided by the user',\n      role: 'User',\n      agent: 'User',\n      requires: [],\n    });\n    this.items[key] = new ContextItem(key, value, initialTask);\n    logger.debug(`Initial artifact '${key}' recorded.`);\n  }\n\n  /**\n   * Retrieves initial files recorded at the start.\n   * @returns An array of File objects or null if not found.\n   */\n  public getInitialFiles(): File[] | null {\n    const initialItem = this.items['initial_files']; // Key used in sysmain.ts\n    if (initialItem && initialItem.content && Array.isArray(initialItem.content)) {\n      // Ensure content is an array of File objects\n      return initialItem.content.map((item: any) => {\n        // Basic check to ensure it's a File-like object\n        if (item && typeof item === 'object' && 'path' in item && 'content' in item) {\n          return new File(item);\n        }\n        logger.warn('Found non-File item in initial files array.');\n        return null;\n      }).filter((file: File | null) => file !== null) as File[];\n    }\n    return null;\n  }\n}\n"},{"path":"src/index.ts","content":"// Main entry point for the library\n\nexport * from './agent/Agent';\nexport * from './core/Session';\nexport * from './core/ExecutionContext';\nexport * from './orchestrator/Orchestrator';\nexport * from './llm/LLM';\nexport * from './custom_types';\nexport * from './system/System';\nexport * from './logger/Logger';\nexport * from './util/Util';\n"}]},"team":{"notes":"Refactoring and optimization are complete. The code now adheres more closely to SOTA principles, with improved error handling, type safety, and modularity. Further refinements can be made in subsequent iterations based on new test results or evolving requirements.","prompts":[]},"reasoning":"The sub-task was to refactor and optimize the TypeScript code based on test results (T12) and SOTA principles. This involved reviewing the existing code, identifying areas for improvement in quality, maintainability, and performance, and implementing the necessary changes. The provided code reflects these optimizations, particularly in error handling within agent execution, clearer artifact management, and more robust LLM response processing. The changes ensure the codebase is more scalable and maintainable, aligning with SOTA practices."}



Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.