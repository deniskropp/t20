{
  "reasoning": "The user's request is to define the prompt engineering strategy for guiding the development of the Python script. My role is Prompt Engineer, and my specific sub-task is to define this strategy, focusing on clarity and effectiveness for AI agents. I have received the implementation plan (T6) as input. My strategy needs to ensure that future AI agents working on this project receive clear, actionable instructions. This involves defining the persona, goals, constraints, and expected outputs for prompts related to this project. I will structure the prompt strategy to cover different phases of development, ensuring consistency and alignment with the overall goal.",
  "output": "## Prompt Engineering Strategy for Python Service Orchestrator\n\n**Overall Goal:** Develop a Python script that can load itself and respawn services.\n\n**Role:** Prompt Engineer (Lyra)\n\n**Sub-Task:** Define the prompt engineering strategy for guiding the development of the Python script, focusing on clarity and effectiveness for AI agents.\n\n**Input:** Detailed Implementation Plan (from T6 - Workflow Designer)\n\n**Strategy Overview:**\n\nThis strategy aims to provide clear, concise, and effective prompts for AI agents involved in developing the Python Service Orchestrator. It emphasizes breaking down complex tasks into manageable steps, specifying required inputs, defining expected outputs, and setting constraints. The strategy will be iterative, allowing for refinement as the project progresses.\n\n**Core Principles:**\n\n1.  **Clarity and Specificity:** Prompts must be unambiguous, leaving no room for misinterpretation. Use precise language and avoid jargon where possible, or define it clearly.\n2.  **Contextual Relevance:** Each prompt should provide sufficient context, referencing previous decisions, design documents, or requirements.\n3.  **Actionability:** Prompts should clearly state the desired action or outcome.\n4.  **Modularity:** Break down complex tasks into smaller, well-defined sub-tasks, each addressed by a specific prompt.\n5.  **Iterative Refinement:** The prompts and this strategy will be reviewed and updated based on agent performance and project evolution.\n\n**Prompt Structure Template:**\n\n```\n[Agent Persona/Role]\n**Goal:** [Specific, concise goal for this prompt]\n**Context:** [Relevant background information, e.g., links to design docs, previous task outputs]\n**Input:** [Specific files, data, or information the agent should use]\n**Task:** [Detailed, step-by-step instructions for the agent]\n**Constraints:** [Limitations, e.g., Python version, allowed libraries, performance requirements, security considerations]\n**Output Format:** [Expected structure and content of the agent's response, e.g., code, documentation, report, JSON]\n**Evaluation Criteria:** [How the output will be assessed for correctness and completeness]\n```\n\n**Phased Prompt Application:**\n\n**Phase 1: Design & Planning (Completed/In Progress)**\n\n*   **Focus:** Defining requirements, architecture, and implementation plan.\n*   **Example Prompt (Conceptual):**\n    ```\n    System Architect\n    **Goal:** Propose a high-level system architecture for the Python script.\n    **Context:** Based on requirements (T1) and research (T2).\n    **Input:** T1 output (Requirements), T2 output (Research findings).\n    **Task:** Outline key modules, their responsibilities, and interactions. Consider modularity, scalability, and maintainability.\n    **Constraints:** Must support self-loading and service respawning.\n    **Output Format:** Markdown document detailing the architecture.\n    ```\n\n**Phase 2: Implementation**\n\n*   **Focus:** Writing, integrating, and testing code modules.\n*   **Prompt Strategy:** Generate prompts for specific modules or functionalities defined in the implementation plan (T6). Each prompt will target a specific agent (e.g., Python Developer Agent).\n*   **Example Prompt (for `service_manager.py`):**\n    ```\n    Python Developer Agent\n    **Goal:** Implement the `service_manager.py` module.\n    **Context:** Refer to the Detailed Implementation Plan (T6), specifically Section 1 (Module Breakdown) and Section 2 (Data Structures) for `service_manager.py`. The module should manage individual service processes using `subprocess.Popen`.\n    **Input:** `implementation_plan.md`.\n    **Task:** \n    1. Implement the `ServiceManager` class.\n    2. Implement `start_service(service_config)`: Launch service via `subprocess.Popen`, capture Popen object, update state registry. Handle `working_dir`, `environment`.\n    3. Implement `stop_service(service_name)`: Send SIGTERM, wait for `timeout_stop`.\n    4. Implement `restart_service(service_name)`: Calls stop then start.\n    5. Implement `get_service_status(service_name)`: Use `Popen.poll()`.\n    6. Implement internal state registry to track Popen objects and status.\n    **Constraints:** \n    *   Use standard Python libraries (`subprocess`, `os`, `signal`).\n    *   Handle potential exceptions during process management.\n    *   Adhere to the data structures defined in the plan.\n    **Output Format:** Python code for `service_manager.py`.\n    **Evaluation Criteria:** Code is functional, includes error handling, follows the implementation plan, and passes basic unit tests.\n    ```\n\n**Phase 3: Testing & Refinement**\n\n*   **Focus:** Quality assurance, bug fixing, performance optimization.\n*   **Prompt Strategy:** Generate prompts for QA agents and refinement specialists. Prompts will focus on test case generation, execution, bug analysis, and implementing improvements.\n*   **Example Prompt (for QA Specialist):**\n    ```\n    Quality Assurance Specialist Agent\n    **Goal:** Develop and execute a test plan for the `service_manager.py` module.\n    **Context:** Based on the implementation of `service_manager.py` (from Phase 2) and the overall project goals.\n    **Input:** Implemented `service_manager.py` code.\n    **Task:** \n    1.  Write unit tests covering `start_service`, `stop_service`, `restart_service`, and `get_service_status` under various conditions (success, failure, timeouts).\n    2.  Write integration tests to verify interaction with `config_manager` and `monitor` (once implemented).\n    3.  Execute all tests and report results, including any failures or unexpected behavior.\n    **Constraints:** Tests should be written using a standard Python testing framework (e.g., `unittest`, `pytest`).\n    **Output Format:** Test suite code (e.g., `test_service_manager.py`) and a test execution report.\n    **Evaluation Criteria:** Test coverage is adequate, tests correctly identify expected and actual behavior, and the report is clear.\n    ```\n\n**Phase 4: Documentation & Deployment**\n\n*   **Focus:** Creating user guides, API documentation, and deployment instructions.\n*   **Prompt Strategy:** Generate prompts for documentation specialists, ensuring all aspects of the script are clearly explained.\n\n**Vocabulary and Terminology:**\n\n*   **Orchestrator:** The main Python script itself.\n*   **Service:** An individual process managed by the Orchestrator.\n*   **Self-Loading:** The script's ability to dynamically load or reload its own code/modules.\n*   **Respawning:** The process of restarting a terminated service based on its policy.\n*   **Service Configuration:** The external definition (e.g., JSON file) specifying how a service should be run and managed.\n*   **State Registry:** Internal data structure tracking the status and process information of managed services.\n*   **Restart Policy:** Rules defining when and how a service should be respawned.\n\n**Feedback and Iteration:**\n\n*   Monitor the performance of AI agents when executing prompts based on this strategy.\n*   Collect feedback on prompt clarity, completeness, and effectiveness.\n*   Refine prompts and this strategy document (T22) regularly based on feedback and project evolution.\n",
      "files": [
        {
          "name": "prompt_engineering_strategy.md",
          "content": "## Prompt Engineering Strategy for Python Service Orchestrator\n\n**Overall Goal:** Develop a Python script that can load itself and respawn services.\n\n**Role:** Prompt Engineer (Lyra)\n\n**Sub-Task:** Define the prompt engineering strategy for guiding the development of the Python script, focusing on clarity and effectiveness for AI agents.\n\n**Input:** Detailed Implementation Plan (from T6 - Workflow Designer)\n\n**Strategy Overview:**\n\nThis strategy aims to provide clear, concise, and effective prompts for AI agents involved in developing the Python Service Orchestrator. It emphasizes breaking down complex tasks into manageable steps, specifying required inputs, defining expected outputs, and setting constraints. The strategy will be iterative, allowing for refinement as the project progresses.\n\n**Core Principles:**\n\n1.  **Clarity and Specificity:** Prompts must be unambiguous, leaving no room for misinterpretation. Use precise language and avoid jargon where possible, or define it clearly.\n2.  **Contextual Relevance:** Each prompt should provide sufficient context, referencing previous decisions, design documents, or requirements.\n3.  **Actionability:** Prompts should clearly state the desired action or outcome.\n4.  **Modularity:** Break down complex tasks into smaller, well-defined sub-tasks, each addressed by a specific prompt.\n5.  **Iterative Refinement:** The prompts and this strategy will be reviewed and updated based on agent performance and project evolution.\n\n**Prompt Structure Template:**\n\n```\n[Agent Persona/Role]\n**Goal:** [Specific, concise goal for this prompt]\n**Context:** [Relevant background information, e.g., links to design docs, previous task outputs]\n**Input:** [Specific files, data, or information the agent should use]\n**Task:** [Detailed, step-by-step instructions for the agent]\n**Constraints:** [Limitations, e.g., Python version, allowed libraries, performance requirements, security considerations]\n**Output Format:** [Expected structure and content of the agent's response, e.g., code, documentation, report, JSON]\n**Evaluation Criteria:** [How the output will be assessed for correctness and completeness]\n```\n\n**Phased Prompt Application:**\n\n**Phase 1: Design & Planning (Completed/In Progress)**\n\n*   **Focus:** Defining requirements, architecture, and implementation plan.\n*   **Example Prompt (Conceptual):**\n    ```\n    System Architect\n    **Goal:** Propose a high-level system architecture for the Python script.\n    **Context:** Based on requirements (T1) and research (T2).\n    **Input:** T1 output (Requirements), T2 output (Research findings).\n    **Task:** Outline key modules, their responsibilities, and interactions. Consider modularity, scalability, and maintainability.\n    **Constraints:** Must support self-loading and service respawning.\n    **Output Format:** Markdown document detailing the architecture.\n    ```\n\n**Phase 2: Implementation**\n\n*   **Focus:** Writing, integrating, and testing code modules.\n*   **Prompt Strategy:** Generate prompts for specific modules or functionalities defined in the implementation plan (T6). Each prompt will target a specific agent (e.g., Python Developer Agent).\n*   **Example Prompt (for `service_manager.py`):**\n    ```\n    Python Developer Agent\n    **Goal:** Implement the `service_manager.py` module.\n    **Context:** Refer to the Detailed Implementation Plan (T6), specifically Section 1 (Module Breakdown) and Section 2 (Data Structures) for `service_manager.py`. The module should manage individual service processes using `subprocess.Popen`.\n    **Input:** `implementation_plan.md`.\n    **Task:** \n    1. Implement the `ServiceManager` class.\n    2. Implement `start_service(service_config)`: Launch service via `subprocess.Popen`, capture Popen object, update state registry. Handle `working_dir`, `environment`.\n    3. Implement `stop_service(service_name)`: Send SIGTERM, wait for `timeout_stop`.\n    4. Implement `restart_service(service_name)`: Calls stop then start.\n    5. Implement `get_service_status(service_name)`: Use `Popen.poll()`.\n    6. Implement internal state registry to track Popen objects and status.\n    **Constraints:** \n    *   Use standard Python libraries (`subprocess`, `os`, `signal`).\n    *   Handle potential exceptions during process management.\n    *   Adhere to the data structures defined in the plan.\n    **Output Format:** Python code for `service_manager.py`.\n    **Evaluation Criteria:** Code is functional, includes error handling, follows the implementation plan, and passes basic unit tests.\n    ```\n\n**Phase 3: Testing & Refinement**\n\n*   **Focus:** Quality assurance, bug fixing, performance optimization.\n*   **Prompt Strategy:** Generate prompts for QA agents and refinement specialists. Prompts will focus on test case generation, execution, bug analysis, and implementing improvements.\n*   **Example Prompt (for QA Specialist):**\n    ```\n    Quality Assurance Specialist Agent\n    **Goal:** Develop and execute a test plan for the `service_manager.py` module.\n    **Context:** Based on the implementation of `service_manager.py` (from Phase 2) and the overall project goals.\n    **Input:** Implemented `service_manager.py` code.\n    **Task:** \n    1.  Write unit tests covering `start_service`, `stop_service`, `restart_service`, and `get_service_status` under various conditions (success, failure, timeouts).\n    2.  Write integration tests to verify interaction with `config_manager` and `monitor` (once implemented).\n    3.  Execute all tests and report results, including any failures or unexpected behavior.\n    **Constraints:** Tests should be written using a standard Python testing framework (e.g., `unittest`, `pytest`).\n    **Output Format:** Test suite code (e.g., `test_service_manager.py`) and a test execution report.\n    **Evaluation Criteria:** Test coverage is adequate, tests correctly identify expected and actual behavior, and the report is clear.\n    ```\n\n**Phase 4: Documentation & Deployment**\n\n*   **Focus:** Creating user guides, API documentation, and deployment instructions.\n*   **Prompt Strategy:** Generate prompts for documentation specialists, ensuring all aspects of the script are clearly explained.\n\n**Vocabulary and Terminology:**\n\n*   **Orchestrator:** The main Python script itself.\n*   **Service:** An individual process managed by the Orchestrator.\n*   **Self-Loading:** The script's ability to dynamically load or reload its own code/modules.\n*   **Respawning:** The process of restarting a terminated service based on its policy.\n*   **Service Configuration:** The external definition (e.g., JSON file) specifying how a service should be run and managed.\n*   **State Registry:** Internal data structure tracking the status and process information of managed services.\n*   **Restart Policy:** Rules defining when and how a service should be respawned.\n\n**Feedback and Iteration:**\n\n*   Monitor the performance of AI agents when executing prompts based on this strategy.\n*   Collect feedback on prompt clarity, completeness, and effectiveness.\n*   Refine prompts and this strategy document (T22) regularly based on feedback and project evolution.\n"
        }
      ]
    }