We are a meta-artificial intelligence team collaboratively developing an iterative plan to achieve the high-level goal of final synthesis.

High-Level Goal: docs for runtime

Team Members:
- Name: 'TASe'
  Role: `Task-Agnostic Step (TAS) extractor`
  Goal: "Identify and extract each 'Task Agnostic Step' (TAS) towards the high-level goal."
- Name: 'Lyra'
  Role: `Prompt Engineer`
  Goal: "Structure workflows and ensure clarity in agent instructions, system prompt engineering"
- Name: 'Aurora'
  Role: `Designer`
  Goal: "Generate aesthetic layouts, color palettes, typography, and UI flows, ensuring accessibility and visual balance."
- Name: 'Lex'
  Role: `Documentation Writer`
  Goal: "Generate clear, concise, and accurate technical documentation based on provided specifications, source code, and developer notes."

Leverage each team member, guided by their goals, to maximize collaboration.
Use prompt engineering to refine the system prompts for each agent based on their roles and tasks.


--- File: 'runtime/agent.py'
```
"""This module defines the core Agent class and related functionalities.

It includes the agent's lifecycle methods, task execution logic, and
helper functions for agent instantiation and discovery.
"""

import json as JSON
import uuid
from dataclasses import dataclass, field
import re
import logging
from typing import List, Dict, Any, Optional

from runtime.llm import LLM

from runtime.core import ExecutionContext, Session


logger = logging.getLogger(__name__)

from runtime.custom_types import AgentOutput

class Agent:
    """Represents a runtime agent instance."""
    name: str
    role: str
    goal: str
    model: str
    system_prompt: str
    llm: LLM

    def __init__(self, name: str, role: str, goal: str, model: str, system_prompt: str) -> None:
        self.name = name
        self.role = role
        self.goal = goal
        self.model = model
        self.system_prompt = system_prompt
        logger.info(f"Agent instance created: {self.name} (Role: {self.role}, Model: {self.model})")
        self.llm = LLM.factory(model)

    def update_system_prompt(self, new_prompt: str) -> None:
        """Updates the agent's system prompt."""

        self.system_prompt = new_prompt

        logger.info(f"Agent {self.name}'s system prompt updated:\n{self.system_prompt}\n")

    def execute_task(self, context: ExecutionContext) -> Optional[str]:
        """
        Executes a task using the Generative AI model based on the provided context.

        Args:
            context (ExecutionContext): The execution context containing goal, plan, and artifacts.

        Returns:
            Optional[str]: The result of the task execution as a string, or an error string.
        """

        task = context.current_step()

        context.record_artifact(f"{self.name}_prompt.txt", self.system_prompt)

        required_task_ids = ['initial']
        required_task_ids.extend(task.requires)

        required_artifacts = []

        if len(required_task_ids) != 0:
            for key, artifact in context.items.items():
                logger.debug(f"Checking artifact from {key} with task ID {artifact.step.id}")
                if artifact.step.id in required_task_ids:
                    required_artifacts.append(artifact)

        previous_artifacts = "\n\n".join(
            f"--- Artifact '{artifact.name}' from ({artifact.step.role}) in [{artifact.step.id}]:\n{artifact.content}"
            for artifact in required_artifacts
        )

        task_prompt: List[str] = [
            f"The overall goal is: '{context.plan.high_level_goal}'",
            f"Your role's specific goal is: '{self.goal}'\n"
            f"Your specific sub-task is: '{task.description}'",

            f"The team's roles are:\n    {context.plan.model_dump_json()}",
        ]

        if previous_artifacts:
            task_prompt.append(f"Please use the following outputs from the other agents as your input:\n\n{previous_artifacts}\n\n")

        task_prompt.append(
            f"Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project."
        )

        #logger.info(f"Agent '{self.name}' is executing task: {task_prompt[1]}")
        #logger.info(f"Task requires outputs from task IDs: {required_task_ids}")
        #logger.info(
        #    f"Found {len(required_artifacts)} required artifacts from previous tasks. Previous artifacts preview: {previous_artifacts[:500]}"
        #)

        ret = self._run(context, "\n\n".join(task_prompt))
        logger.info(f"Agent '{self.name}' completed task: {task.description}")
        return ret


    def _run(self, context: ExecutionContext, prompt: str) -> Optional[str]:
        context.record_artifact(f"{self.name}_task.txt", prompt)

        try:
            response = self.llm.generate_content(   # ignore type
                model_name=self.model,
                contents=prompt,
                system_instruction=self.system_prompt,
                temperature=0.7,
                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None
                response_schema=AgentOutput
            )
            if isinstance(response, AgentOutput):
                result = response.model_dump_json()
                print(f"\n--- Output:\n{response.output}\n")
                if response.artifact and response.artifact.files:
                    for file in response.artifact.files:
                        print(f"\n--- File: {file.path}\n{file.content}\n")
                        context.session.add_artifact(file.path, file.content)
            else:
                result = response or '{}'

                if '```' in result:
                    match = re.search(r'```(json)?\s*([\s\S]*?)\s*```', result, re.IGNORECASE)
                    if match:
                        # Extract the JSON content from the markdown block
                        result = match.group(2).strip()

                json_data = JSON.loads(result)
                if isinstance(json_data, dict):
                    if "output" in json_data and json_data["output"] is not None:
                        output = JSON.dumps(json_data["output"], indent=4) if isinstance(json_data["output"], dict) else json_data["output"]
                        logger.info(f"Output:\n{output[:2000]}\n")

                    if "artifact" in json_data and json_data["artifact"] is not None:
                        if "files" in json_data["artifact"]:
                            for file_data in json_data["artifact"]["files"]:
                                if "path" in file_data and "content" in file_data:
                                    print(f"\n--- File: {file_data["path"]}")
                                    print(f"{file_data["content"]}")
                                    context.session.add_artifact(file_data["path"], file_data["content"])
                else:
                    logger.warning("Agent output is not in expected AgentOutput format. Processing as plain text.")
                    logger.info(f"Output: '\n{result[:2000]}\n'")

        except Exception as e:
            logger.exception(f"Error executing task for {self.name}: {e}")
            raise

        return result

def find_agent_by_role(agents: List[Agent], role: str) -> Optional[Agent]:
    """
    Finds an agent in a list by its role.

    Args:
        agents (List[Agent]): A list of Agent objects.
        role (str): The role of the agent to find.

    Returns:
        Optional[Agent]: The found Agent object, or None if not found.
    """
    logger.debug(f"Searching for agent with role: {role} in agents: {[agent.name for agent in agents]}")
    return next((agent for agent in agents if agent.role == role), None)

```

--- File: 'runtime/core.py'
```
"""This module provides the core data structures for the multi-agent runtime.

It defines the ExecutionContext for managing workflow state and the Session
for handling artifacts and session-specific data.
"""

from dataclasses import dataclass, field
from typing import Any, Dict, List
import uuid
import os
import logging
import json

from runtime.custom_types import Plan, Task

logger = logging.getLogger(__name__)


class ContextItem:
    def __init__(self, name: str, content: Any, step: Task):
        self.name = name
        self.content = content
        self.step = step


@dataclass
class ExecutionContext:
    """Holds the state and context for a multi-agent workflow."""
    session: 'Session'
    plan: Plan
    step_index: int = 0
    round_num: int = 0
    items: Dict[str, ContextItem] = field(default_factory=dict)

    def next(self) -> None:
        self.step_index += 1

    def reset(self) -> None:
        self.step_index = 0

    def current_step(self) -> Task:
        """Returns the current step in the plan."""
        return self.plan.tasks[self.step_index]

    def _remember_artifact(self, key: str, value: Any, step: Task | None = None) -> None:
        """Remembers an artifact from a step's execution for future tasks."""
        self.items[key] = ContextItem(name=key, content=value, step=step or self.current_step())

    def record_artifact(self, key: str, value: Any, mem: bool = False) -> None:
        """
        Records an artifact from a step's execution and optionally remembers it.

        Args:
            key (str): The key under which to store the artifact.
            value (Any): The content of the artifact.
            mem (bool): If True, the artifact is also stored in the execution context's memory.
        """
        artifact_key = f"__{self.round_num}/__step_{self.step_index}_{key}"
        self.session.add_artifact(artifact_key, value)
        if mem:
            self._remember_artifact(artifact_key, value)

    def record_initial(self, key: str, value: Any) -> None:
        """
        Records an artifact from a step's execution and optionally remembers it.

        Args:
            key (str): The key under which to store the artifact.
            value (Any): The content of the artifact.
        """
        self._remember_artifact(key, value, step=Task(
            id='initial',
            description='Initial files provided by the user',
            role='User',
            agent='User'
        ))




@dataclass
class Session:
    """Manages the runtime context for a task, including session ID and artifact storage."""
    session_id: str = field(default_factory=lambda: f"session_{uuid.uuid4()}")
    agents: list = field(default_factory=list) # Type hint will be updated later to List[Agent]
    state: str = "initialized"
    session_dir: str = field(init=False)
    project_root: str = field(default_factory=str)

    def __post_init__(self) -> None:
        """Initializes the session directory."""
        if not self.project_root:
            self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
        self.session_dir = os.path.join(self.project_root, 'sessions', self.session_id)
        os.makedirs(self.session_dir, exist_ok=True)
        logger.info(f"Session created: {self.session_id} (Directory: {self.session_dir})")

    def add_artifact(self, name: str, content: Any) -> None:
        """
        Saves an artifact in the session directory.

        Args:
            name (str): The name of the artifact file.
            content (Any): The content to write to the artifact file. Can be dict, list, or string.
        """
        artifact_path = os.path.join(self.session_dir, name)
        try:
            os.makedirs(os.path.dirname(artifact_path), exist_ok=True)
            with open(artifact_path, 'w', encoding='utf-8') as f:
                if isinstance(content, (dict, list)):
                    json.dump(content, f, indent=4)
                else:
                    f.write(str(content))
            logger.info(f"Artifact '{name}' saved in session {self.session_id}.")
        except (TypeError, IOError) as e:
            logger.error(f"Error saving artifact '{name}': {e}")

    def get_artifact(self, name: str) -> Any:
        """
        Loads an artifact from the session directory.

        Args:
            name (str): The name of the artifact file to load.

        Returns:
            Any: The content of the artifact, or None if an error occurs.
        """
        artifact_path = os.path.join(self.session_dir, name)
        try:
            with open(artifact_path, 'r', encoding='utf-8') as f:
                if name.endswith('.json'):
                    return json.load(f)
                return f.read()
        except (FileNotFoundError, json.JSONDecodeError, IOError) as e:
            logger.error(f"Error retrieving artifact '{name}': {e}")
            return None

```

--- File: 'runtime/custom_types.py'
```
"""This module defines custom data types and Pydantic models for the runtime.

It ensures data consistency and validation across different modules by providing
centralized, well-defined data structures for plans, tasks, and agent outputs.
"""

from typing import List, Optional
from pydantic import BaseModel, Field

class Task(BaseModel):
    """A single actionable step in a plan."""
    id: str = Field(..., description="Unique ID for the task (e.g., 'T1', 'T-020', 'T_02_DESIGN_01').")
    description: str = Field(..., description="What the task is and how to execute it.")
    role: str = Field(..., description="Role responsible for performing this task.")
    agent: str = Field(..., description="Specific agent assigned to this task.")
    requires: List[str] = Field(default_factory=list, description="List of prerequisite task IDs.")


class File(BaseModel):
    """Represents a generated file."""
    path: str = Field(..., description="File path or name (e.g., 'src/main.py').")
    content: str = Field(..., description="Full content of the file.")


class Artifact(BaseModel):
    """Output artifact generated by a task."""
    task: str = Field(..., description="ID of the task that produced this artifact.")
    files: List[File] = Field(default_factory=list, description="Files included in the artifact.")


class Role(BaseModel):
    """Definition of a role in the team."""
    title: str = Field(..., description="Role title (e.g., 'Engineer', 'Reviewer').")
    purpose: str = Field(..., description="High-level responsibilities and objectives of this role.")


class Plan(BaseModel):
    """Overall strategy with reasoning, roles, and tasks."""
    high_level_goal: str = Field(..., description="The overarching objective this plan aims to accomplish.")
    reasoning: str = Field(..., description="Why this plan is structured this way.")
    roles: List[Role] = Field(default_factory=list, description="All roles needed for execution.")
    tasks: List[Task] = Field(default_factory=list, description="Step-by-step ordered tasks.")


class Prompt(BaseModel):
    """System prompt assigned to an agent."""
    agent: str = Field(..., description="Name of the agent this prompt is for. Must match exactly an existing agent.")
    role: str = Field(..., description="Role context of the agent.")
    system_prompt: str = Field(..., description="Full text of the system prompt for this agent.")


class Team(BaseModel):
    """Metadata and coordination for the team."""
    notes: str = Field(..., description="General notes, reflections, or feedback about the team or process.")
    prompts: List[Prompt] = Field(default_factory=list, description="Updated system prompts for agents.")


class AgentOutput(BaseModel):
    """Structured output returned by an agent after executing a task."""
    output: str = Field(..., description="Message from the agent, accompanied by artifact (or files).")
    artifact: Optional[Artifact] = Field(default=None, description="Generated artifact, if applicable.")
    team: Optional[Team] = Field(default=None, description="Updates to team configuration or system prompts.")
    reasoning: Optional[str] = Field(default=None, description="Explanation of how the agent arrived at this output.")



if __name__ == "__main__":
    import json
    print(json.dumps(AgentOutput.model_json_schema(), indent=4))

```

--- File: 'runtime/__init__.py'
```
# This file makes the 'runtime' directory a Python package.

from .custom_types import Role, Task, Plan, Artifact, File, Prompt, Team, AgentOutput

from .agent import Agent
from .core import Session
from .orchestrator import Orchestrator
from .system import System

```

--- File: 'runtime/llm.py'
```
"""This module abstracts interactions with Large Language Models (LLMs).

It provides a unified interface for various LLM providers and models,
allowing for easy integration and interchangeability of different LLMs.
"""

import json
import os
import time
from google import genai
from google.genai import types
from ollama import Client as Ollama
from typing import Optional, Any, Dict
from abc import ABC, abstractmethod
import logging
from pydantic import BaseModel

logger = logging.getLogger(__name__)


class LLM(ABC):
    """
    Abstract base class for Large Language Models.
    Provides a centralized place for LLM-related configurations and utilities.
    """
    def __init__(self) -> None:
        pass

    @abstractmethod
    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',
                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: Any = None) -> Optional[str]: # type: ignore
        """Generates content using an LLM."""
        pass

    @staticmethod
    def factory(species: str) -> 'LLM':
        print(f"LLM Factory: Creating LLM instance for species '{species}'")
        if species == 'Olli':
            return Olli()
        elif species == 'Kimi':
            return Kimi()
        if species.startswith('ollama:'):
            # Extract model name after 'ollama:'
            model_name = species.split(':', 1)[1]
            return Olli(species=model_name)
        if species.startswith('opi:'):
            # Extract model name after 'opi:'
            model_name = species.split(':', 1)[1]
            return Opi(species=model_name)
        if species.startswith('mistral:'):
            # Extract model name after 'mistral:'
            model_name = species.split(':', 1)[1]
            return Mistral(species=model_name)
        return Gemini(species)


class Gemini(LLM):
    """
    Provides a centralized place for LLM-related configurations and utilities.
    """
    _clients: Dict[str, genai.Client] = {} # Class-level cache for clients

    def __init__(self, species: str) -> None:
        self.species = species

    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',
                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore
        """
        Generates content using the specified GenAI model.

        Args:
            model_name (str): The name of the model to use (e.g., "gemini-pro").
            contents (str): The input content for the model.
            system_instruction (str, optional): System-level instructions for the model.
            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.
            response_mime_type (str, optional): The desired MIME type for the response.
            response_schema (Any, optional): The schema for the response.

        Returns:
            types.GenerateContentResponse: The response from the GenAI model.
        """
        client = self._get_client()
        if not client:
            return None

        config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            temperature=temperature,
            response_mime_type=response_mime_type,
            response_schema=response_schema,
            max_output_tokens=50000,
        )

        # try three times
        for _ in range(3):
            try:
                response = client.models.generate_content(
                    model=model_name,
                    contents=contents,
                    config=config,
                )
                if not response.candidates or response.candidates[0].content is None or response.candidates[0].content.parts is None or not response.candidates[0].content.parts or response.candidates[0].content.parts[0].text is None:
                    raise ValueError(f"Gemini: No content in response from model {model_name}. Retrying...")
                break
            except Exception as ex:
                logger.exception(f"Error generating content with model {model_name}: {ex}")
                # If it's the last retry, return None
                if _ == 2:
                    return None
                logger.warning(f"Retrying content generation for model {model_name}...")
                time.sleep(10)
                #return None

        if isinstance(response.parsed, BaseModel):
            return response.parsed

        if response_mime_type == 'application/json':
            try:
                # Attempt to parse the JSON response
                json_output = response.candidates[0].content.parts[0].text.strip()
                # Validate against schema if provided
                if response_schema:
                    response_schema.model_validate_json(json_output)
                return json_output
            except Exception as ex:
                logger.exception(f"Error parsing or validating JSON response: {ex}")

        return response.candidates[0].content.parts[0].text.strip()

    def _get_client(self) -> genai.Client:
        """
        Returns a GenAI client instance.
        """
        try:
            if self.species not in Gemini._clients:
                logger.info(f"Initializing GenAI client for species {self.species}")
                Gemini._clients[self.species] = genai.Client()
            return Gemini._clients[self.species]
        except Exception as e:
            logger.exception(f"Error initializing GenAI client: {e}")
            raise


class Olli(LLM):
    """
    Provides a centralized place for LLM-related configurations and utilities.
    """
    def __init__(self, species: str = 'Olli'):
        self.client = None
        self.species = species

    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',
                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore
        """
        Generates content using the specified GenAI model.

        Args:
            model_name (str): The name of the model to use (e.g., "gemini-pro").
            contents (str): The input content for the model.
            system_instruction (str, optional): System-level instructions for the model.
            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.
            response_mime_type (str, optional): The desired MIME type for the response.
            response_schema (Any, optional): The schema for the response.

        Returns:
            types.GenerateContentResponse: The response from the GenAI model.
        """
        print(f"Olli: Using model {model_name} with temperature {temperature}")
        client = self._get_client()
        if not client:
            return None

        try:
            out = ""
            fmt = response_schema.model_json_schema()
            print(f"Olli: Using response format {fmt}")
            response = client.generate(
                model=self.species,#model_name,
                prompt=contents,
                format=fmt,
                options={"temperature": temperature, "system_instruction": system_instruction},
                stream=True
            )
            for chunk in response:
                if hasattr(chunk, "response") and chunk.response:
                    print(chunk.response, end="")
                    out += chunk.response
            return out
        except Exception as e:
            logger.error(f"Error generating content with model {model_name}: {e}")
            return None

    def _get_client(self):
        """
        Returns a GenAI client instance.
        """
        try:
            if (self.client is None):
                if self.species not in Olli._clients:
                    Olli._clients[self.species] = Ollama()#base_url='http://localhost:11434')
            return Olli._clients[self.species]
        except Exception as e:
            logger.error(f"Error initializing GenAI client: {e}")
            return None





from huggingface_hub import InferenceClient, ChatCompletionInputResponseFormatText, ChatCompletionInputResponseFormatJSONObject, ChatCompletionInputResponseFormatJSONSchema, ChatCompletionInputJSONSchema


class Kimi(LLM):
    """
    Provides a centralized place for LLM-related configurations and utilities.
    """
    _clients = {} # Class-level cache for clients

    def __init__(self, species: str = 'Kimi'):
        self.species = species
    
    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',
                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore
        """
        Generates content using the specified GenAI model.

        Args:
            model_name (str): The name of the model to use (e.g., "gemini-pro").
            contents (str): The input content for the model.
            system_instruction (str, optional): System-level instructions for the model.
            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.
            response_mime_type (str, optional): The desired MIME type for the response.
            response_schema (Any, optional): The schema for the response.

        Returns:
            types.GenerateContentResponse: The response from the GenAI model.
        """
        client = self._get_client()
        if not client:
            return None

        try:
            out = ""

            stream = client.chat.completions.create(
                messages=[
                    {
                        "role": "system",
                        "content": system_instruction
                    },
                    {
                        "role": "user",
                        "content": contents
                    },
                ],
                temperature=temperature,
                max_tokens=50000,
                top_p=1,
                stream=True,
                response_format=ChatCompletionInputResponseFormatText() if response_mime_type == 'text/plain' else ChatCompletionInputResponseFormatJSONObject() #if response_schema is None else ChatCompletionInputResponseFormatJSONSchema(json_schema=ChatCompletionInputJSONSchema(name=response_schema.model_json_schema()))
            )

            for chunk in stream:
                if chunk.choices[0].delta.content is None:
                    continue
                print(chunk.choices[0].delta.content, end="")
                out += chunk.choices[0].delta.content

            return out
        except Exception as e:
            logger.error(f"Error generating content with model {model_name}: {e}")
            return None

    def _get_client(self):
        """
        Returns a inference client instance.
        """
        try:
            if (self.client is None):
                if self.species not in Kimi._clients:
                    Kimi._clients[self.species] = InferenceClient(
                        provider="featherless-ai",
                        api_key=os.environ.get("HF_TOKEN"),
                        model="moonshotai/Kimi-K2-Instruct"
                    )
            return Kimi._clients[self.species]
        except Exception as e:
            logger.error(f"Error initializing GenAI client: {e}")
            return None



from openai import OpenAI
from openai.types.chat.completion_create_params import ResponseFormat


class Opi(LLM):
    """
    Provides a centralized place for LLM-related configurations and utilities.
    """
    _clients = {} # Class-level cache for clients

    def __init__(self, species: str = 'Opi'):
        self.species = species
    
    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',
                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore
        """
        Generates content using the specified GenAI model.

        Args:
            model_name (str): The name of the model to use (e.g., "gemini-pro").
            contents (str): The input content for the model.
            system_instruction (str, optional): System-level instructions for the model.
            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.
            response_mime_type (str, optional): The desired MIME type for the response.
            response_schema (Any, optional): The schema for the response.

        Returns:
            types.GenerateContentResponse: The response from the GenAI model.
        """
        client = self._get_client()
        if not client:
            return None

        try:
            out = ""

            response_format: ResponseFormat = {"type": "text"}

            if response_mime_type == 'application/json':
                response_format = {"type": "json_object"}

            if response_schema:
                response_format = {"type": "json_schema", "json_schema": response_schema.model_json_schema()}   # type: ignore

            print(f"Opi: Using response format {response_format}")

            stream = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "system",
                        "content": system_instruction
                    },
                    {
                        "role": "user",
                        "content": contents
                    },
                ],
                temperature=temperature,
                max_tokens=50000,
                top_p=1,
                stream=True,
                response_format=response_format
            )

            for chunk in stream:
                if chunk.choices[0].delta.content is None:
                    continue
                print(chunk.choices[0].delta.content, end="")
                out += chunk.choices[0].delta.content

            return out
        except Exception as e:
            logger.error(f"Error generating content with model {model_name}: {e}")
            return None

    def _get_client(self):
        """
        Returns a OpenAI client instance.
        """
        try:
            if (self.client is None):
                if self.species not in Opi._clients:
                    Opi._clients[self.species] = OpenAI(api_key=os.environ.get("OPENAI_API_KEY",""), base_url=os.environ.get("OPENAI_API_BASE"))
            return Opi._clients[self.species]
        except Exception as e:
            logger.error(f"Error initializing OpenAI client: {e}")
            return None



##


from mistralai import Mistral as MistralClient
from mistralai.extra.utils import response_format_from_pydantic_model

class Mistral(LLM):
    """
    Provides a centralized place for LLM-related configurations and utilities.
    """
    _clients = {} # Class-level cache for clients

    def __init__(self, species: str = 'Mistral'):
        self.species = species
    
    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',
                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore
        """
        Generates content using the specified Mistral model.

        Args:
            model_name (str): The name of the model to use (e.g., "mistral-large-latest").
            contents (str): The input content for the model.
            system_instruction (str, optional): System-level instructions for the model.
            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.
            response_mime_type (str, optional): The desired MIME type for the response.
            response_schema (Any, optional): The schema for the response.

        Returns:
            str: The response from the Mistral model.
        """
        client = self._get_client()
        if not client:
            return None
        try:
            messages = []
            if system_instruction:
                messages.append({"role": "system", "content": system_instruction})
            messages.append({"role": "user", "content": contents})

            print(f"Mistral: Using model {model_name} with temperature {temperature}")

            response_format = {"type": "text"}

            if response_mime_type == 'application/json':
                response_format = {"type": "json_object"}

            if response_schema:
                response_format = response_format_from_pydantic_model(response_schema)  # type: ignore


            out = ""
            stream = client.chat.stream(
                model=self.species,#model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=50000,
                response_format=response_format # type: ignore
            )
            for chunk in stream:
                if chunk.data.choices[0].delta.content is None:
                    continue
                print(chunk.data.choices[0].delta.content, end="")
                out += str(chunk.data.choices[0].delta.content)

            return out
        except Exception as e:
            print(f"Error generating content with model {model_name}: {e}")
            return None

    def _get_client(self):
        """
        Returns a Mistral client instance.
        """
        try:
            if self.client is None:
                if self.species not in Mistral._clients:
                    api_key = os.environ.get("MISTRAL_API_KEY")
                    if not api_key:
                        raise ValueError("MISTRAL_API_KEY environment variable not set.")
                    Mistral._clients[self.species] = MistralClient(api_key=api_key)
            return Mistral._clients[self.species]
        except Exception as e:
            logger.error(f"Error initializing Mistral client: {e}")
            return None

```

--- File: 'runtime/log.py'
```
"""This module configures and sets up the logging for the application.

It defines the log format, level, and handlers (console, file, JSONL),
providing a centralized and consistent logging setup.
"""

import logging
import os
import re
import json
from logging.handlers import RotatingFileHandler
from colorama import Fore, Style

# --- Configuration ---
LOG_LEVEL = "INFO"
LOG_FILE_DIR = "logs"
LOG_FILE_NAME = "app.log"
LOG_FILE_PATH = os.path.join(LOG_FILE_DIR, LOG_FILE_NAME)
MAX_FILE_SIZE = 1024 * 1024  # 1 MB
BACKUP_COUNT = 5

# --- Colored Formatter ---
class ColoredFormatter(logging.Formatter):
    """
    A custom logging formatter that adds colors to the log messages.
    It can color based on log level, or on regex matching of the message.
    """
    # ANSI escape codes for colors
    COLORS = {
        'DEBUG': Fore.YELLOW,
        'INFO': Fore.CYAN,
        'WARNING': Fore.YELLOW,
        'ERROR': Fore.RED,
        'CRITICAL': Fore.MAGENTA,
        'RESET': Style.RESET_ALL
    }

    # Regex-based coloring rules (pattern, color)
    REGEX_RULES_MESSAGES = [
        (re.compile(r"Agent instance created:"), Fore.YELLOW),
        (re.compile(r"Agent '.*' system prompt updated."), Fore.GREEN),
        (re.compile(r"Agent '.*' is executing task"), Fore.GREEN),
        (re.compile(r"Agent '.*' is executing step"), Fore.LIGHTRED_EX),
        (re.compile(r"Agent '.*' completed task"), Fore.BLUE),
        (re.compile(r"Warning: Agent output is not in expected AgentOutput format."), Fore.YELLOW),
        (re.compile(r"System Runtime"), Fore.CYAN),
        (re.compile(r"Error: No agents could be instantiated."), Fore.RED),
        (re.compile(r"Session created:"), Fore.GREEN),
        (re.compile(r"Artifact '.*' saved in session .*"), Fore.LIGHTBLUE_EX),
        (re.compile(r"Error saving artifact '.*'"), Fore.RED),
        (re.compile(r"Files provided:"), Fore.MAGENTA),
        (re.compile(r"Error reading file .*"), Fore.RED),
        (re.compile(r"Generated plan:"), Fore.WHITE),
        (re.compile(r"Orchestration failed: Could not generate a valid plan."), Fore.RED),
        (re.compile(r"Orchestrator .* is generating a plan for"), Fore.BLUE),
        (re.compile(r"Orchestrator .* is starting workflow round"), Fore.CYAN),
        (re.compile(r"Orchestrator has completed workflow round"), Fore.CYAN),
        (re.compile(r"Agent .* provided new prompts."), Fore.LIGHTGREEN_EX),
        (re.compile(r"Target agent '.*' not found for prompt update."), Fore.YELLOW),
        (re.compile(r"Error generating or validating plan for .*"), Fore.RED),
    ]

    COLORS_NAMES = {
        'runtime.orchestrator': Fore.LIGHTYELLOW_EX,
        'runtime.agent': Fore.LIGHTWHITE_EX,
        'runtime.sysmain': Fore.LIGHTRED_EX,
        'runtime.core': Fore.LIGHTBLUE_EX,
        'runtime.bootstrap': Fore.LIGHTRED_EX,
        'runtime.loader': Fore.LIGHTRED_EX,
        'runtime.llm': Fore.LIGHTMAGENTA_EX,
        'runtime.util': Fore.GREEN,
        'runtime.factory': Fore.LIGHTRED_EX,
        'runtime.paths': Fore.BLUE,
        'runtime.custom_types': Fore.CYAN,
    }

    def format(self, record: logging.LogRecord) -> str:
        """
        Formats the log record with the appropriate color.
        """
        log_message = super().format(record)

        # Check for regex-based coloring first
        for pattern, color in self.REGEX_RULES_MESSAGES:
            if pattern.search(record.getMessage()):
                return f"{color}{log_message}{self.COLORS['RESET']}"

        # Check for logger name-based coloring
        for name_prefix, color in self.COLORS_NAMES.items():
            if record.name.startswith(name_prefix):
                return f"{color}{log_message}{self.COLORS['RESET']}"

        # Fallback to level-based coloring
        return f"{self.COLORS.get(record.levelname, self.COLORS['RESET'])}{log_message}{self.COLORS['RESET']}"

# --- Setup ---
def setup_logging(level: str = LOG_LEVEL) -> None:
    """
    Sets up logging for the application.
    """
    # Create log directory if it doesn't exist
    if not os.path.exists(LOG_FILE_DIR):
        os.makedirs(LOG_FILE_DIR)

    # Get the root logger
    logger = logging.getLogger()
    logger.setLevel(level)

    # --- Console Handler ---
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)

    # --- File Handler ---
    file_handler = RotatingFileHandler(
        LOG_FILE_PATH,
        maxBytes=MAX_FILE_SIZE,
        backupCount=BACKUP_COUNT
    )
    file_handler.setLevel(level)

    # --- Formatters ---
    console_formatter = ColoredFormatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    class JsonFormatter(logging.Formatter):
        def format(self, record: logging.LogRecord):
            log_record = {
                "timestamp": self.formatTime(record, self.datefmt),
                "name": record.name,
                "level": record.levelname,
                "message": record.getMessage(),
                "pathname": record.pathname,
                "lineno": record.lineno,
                "funcName": record.funcName,
                "process": record.process,
                "thread": record.thread,
                "threadName": record.threadName,
            }
            if record.exc_info:
                log_record["exc_info"] = self.formatException(record.exc_info)
            if record.stack_info:
                log_record["stack_info"] = self.formatStack(record.stack_info)
            return json.dumps(log_record)

    # --- Add Formatter to Handlers ---
    console_handler.setFormatter(console_formatter)
    file_handler.setFormatter(file_formatter)

    # --- Add Handlers to Logger ---
    # Avoid adding handlers multiple times
    if not logger.handlers:
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

        # --- JSONL File Handler ---
        jsonl_file_handler = RotatingFileHandler(
            os.path.join(LOG_FILE_DIR, "app.jsonl"),
            maxBytes=MAX_FILE_SIZE,
            backupCount=BACKUP_COUNT
        )
        jsonl_file_handler.setLevel(level)
        jsonl_file_handler.setFormatter(JsonFormatter())
        logger.addHandler(jsonl_file_handler)

# --- Main Application Logger ---
# It's good practice to use a named logger instead of the root logger directly
app_logger = logging.getLogger(__name__)

# --- Example Usage ---
if __name__ == '__main__':
    setup_logging()
    app_logger.info("Logging setup complete.")
    app_logger.debug("This is a debug message.")
    app_logger.warning("This is a warning message.")
    app_logger.error("This is an error message.")

```

--- File: 'runtime/orchestrator.py'
```
"""This module defines the Orchestrator agent and its planning capabilities.

The Orchestrator is a specialized agent responsible for generating, managing,
and executing the workflow plan based on a high-level goal. This is being
refactored to move workflow execution to the System class.
"""

import os
import json
from typing import List, Dict, Optional, Tuple
import logging

from pydantic import BaseModel, Field, ValidationError

from runtime.agent import Agent
from runtime.core import Session
from runtime.util import read_file

logger = logging.getLogger(__name__)

from runtime.custom_types import Plan, File

class Orchestrator(Agent):
    """An agent responsible for creating and managing a plan for multi-agent workflows."""
    team: Dict[str,Agent] = {}

    def generate_plan(self, session: Session, high_level_goal: str, files: List[File] = []) -> Optional[Plan]:
        """
        Invokes the language model to get a structured plan for the given high-level goal.
        The plan includes a sequence of tasks and the roles responsible for them.

        Args:
            session (Session): The current session object.
            high_level_goal (str): The high-level goal for which to generate a plan.
            file_contents (List[Tuple[str, str]]): A list of file paths and their contents to provide context to the LLM.

        Returns:
            Optional[Plan]: The generated plan as a Pydantic object, or None if an error occurs.
        """
        logger.info(f"Orchestrator {self.name} is generating a plan for: '{high_level_goal}'")
        if not self.llm or not self.team:
            logger.error("Orchestrator client or team not initialized. Cannot generate plan.")
            return None

        # Construct a description of the available agents and their roles/goals
        team_description = "\n".join(
            f"- Name: '{agent.name}'\n"
            f"  Role: `{agent.role}`\n"
            f"  Goal: \"{agent.goal}\""
            for agent in self.team.values()
        )

        # Load the general planning prompt template
        t20_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) # Not used
        general_prompt_template = read_file(os.path.join(t20_root, "prompts", "general_planning.txt")).strip()
        planning_prompt_parts = [general_prompt_template.format(
            high_level_goal=high_level_goal,
            team_description=team_description
        )]

        # Include file contents if provided
        file_section = []
        for file in files:
            file_section.append(f"\n--- File: '{file.path}'\n```\n{file.content}\n```")
        planning_prompt_parts.append("\n".join(file_section))

        logger.debug("Planning prompt parts for LLM: %s", planning_prompt_parts)

        # Combine prompt parts for the LLM call
        full_prompt_for_llm = "\n\n".join(planning_prompt_parts)
        logger.info(f"Orchestrator {self.name} Planning Prompt:\n{full_prompt_for_llm}")

        session.add_artifact("planning_prompt.txt", full_prompt_for_llm)

        try:
            response = self.llm.generate_content(
                model_name=self.model,
                contents="\n\n".join(planning_prompt_parts),
                system_instruction=self.system_prompt,
                temperature=0.1,
                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None
                response_schema=Plan
            )
            if isinstance(response, Plan):
                plan = response
            else:
                plan = Plan.model_validate_json(response or '{}')
        except (ValidationError, json.JSONDecodeError) as e:
            logger.exception(f"Error generating or validating plan for {self.name}: {e}")
            return None
        except Exception as e:
            logger.exception(f"An unexpected error occurred during plan generation for {self.name}: {e}")
            return None

        return plan

```

--- File: 'runtime/paths.py'
```
"""
Centralized path constants for the project.

This module defines constants for directory and file names, allowing them to be
reused consistently throughout the application.
"""

# Directory Names
AGENTS_DIR_NAME = "agents"
CONFIG_DIR_NAME = "config"
PROMPTS_DIR_NAME = "prompts"

# File Names
RUNTIME_CONFIG_FILENAME = "runtime.yaml"
```

--- File: 'runtime/sysmain.py'
```
"""This module serves as the main entry point for the command-line interface.

It parses command-line arguments and initiates the system bootstrap process,
acting as the primary interface for running the multi-agent system.
"""

import os
import argparse
import logging

from runtime.log import setup_logging
from runtime.system import System
from runtime.custom_types import File
from runtime.util import read_file

logger = logging.getLogger(__name__)

def parse_command_line_arguments() -> argparse.Namespace:
    """
    Parses command-line arguments for the runtime system.

    Returns:
        An argparse.Namespace object containing the parsed arguments.
    """
    parser = argparse.ArgumentParser(
        description="Run the Gemini agent runtime.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    general_group = parser.add_argument_group("General Settings")
    general_group.add_argument("-p", "--plan-only", action="store_true", help="Generate only the plan without executing tasks.")
    general_group.add_argument("-r", "--rounds", type=int, default=1, help="The number of rounds to execute the workflow.")
    general_group.add_argument("-f", "--files", nargs='*', help="List of files to be used in the task.", default=[])

    orchestrator_group = parser.add_argument_group("Orchestrator Settings")
    orchestrator_group.add_argument("-o", "--orchestrator", type=str, help="The name of the orchestrator to use.", default="Meta-AI")
    orchestrator_group.add_argument("-m", "--model", type=str, help="Default LLM model to use.", default="gemini-2.5-flash-lite")

    parser.add_argument("task", type=str, help="The initial task for the orchestrator to perform.")

    args = parser.parse_args()

    if args.rounds < 1:
        parser.error("Number of rounds must be at least 1.")
    if not args.task:
        parser.error("The task argument is required.")

    return args

def setup_application_logging(log_level: str = "INFO") -> None:
    """
    Sets up the global logging configuration for the application.

    Args:
        log_level: The desired logging level (e.g., "DEBUG", "INFO", "WARNING").
    """
    try:
        setup_logging(level=log_level)
        logger.info(f"Logging initialized with level: {log_level}")
    except ValueError as e:
        logger.error(f"Invalid log level '{log_level}': {e}")
        raise

def system_main() -> None:
    """
    Main entry point for the runtime system.
    Parses arguments, sets up logging and configuration, and runs the system.
    """
    try:
        # 1. Parse command line arguments
        args = parse_command_line_arguments()
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

        # 2. Initial logging setup
        setup_application_logging()

        # 3. Convert file paths to File objects
        file_objects = []
        for file_path in args.files:
            try:
                content = read_file(file_path)
                if content.startswith("Error:"):
                    logger.warning(f"Skipping file '{file_path}': {content}")
                    continue
                file_objects.append(File(path=file_path, content=content))
            except Exception as e:
                logger.error(f"Error processing file '{file_path}': {e}")

        # 4. Instantiate and set up the system
        system = System(root_dir=project_root, default_model=args.model)
        system.setup(orchestrator_name=args.orchestrator)

        # 5. Re-configure logging based on loaded config
        log_level = system.config.get("logging_level", "INFO")
        setup_application_logging(log_level=log_level)

        # 6. Start the system's main workflow
        plan = system.start(
            high_level_goal=args.task,
            files=file_objects
        )
        if args.plan_only:
            logger.info("Plan-only mode: Workflow execution skipped.")
            return

        # 7. Run the system's main workflow
        system.run(
            plan=plan,
            rounds=args.rounds,
            files=file_objects
        )

    except (FileNotFoundError, RuntimeError) as e:
        logger.exception(f"A critical error occurred: {e}")
        exit(1)
    except Exception as e:
        logger.exception(f"An unexpected error occurred during system main execution: {e}")
        exit(1)


if __name__ == "__main__":
    system_main()

```

--- File: 'runtime/system.py'
```
"""This module defines the core System class for the multi-agent runtime.

It encapsulates the state and behavior of the entire system, including
configuration loading, agent instantiation, and workflow execution.
"""

import os
import json
import yaml
from glob import glob

import logging
from typing import Any, List, Optional, Dict

from .core import Session, ExecutionContext
from .agent import Agent, find_agent_by_role
from .orchestrator import Orchestrator
from .log import setup_logging
from .paths import AGENTS_DIR_NAME, CONFIG_DIR_NAME, PROMPTS_DIR_NAME, RUNTIME_CONFIG_FILENAME

from .custom_types import AgentOutput, Artifact, Plan, File
logger = logging.getLogger(__name__)

class System:
    """
    Represents the entire multi-agent system, handling setup, execution, and state.
    """
    def __init__(self, root_dir: str, default_model: str = "Olli"):
        """
        Initializes the System.

        Args:
            root_dir (str): The root directory of the project.
            default_model (str): The default LLM model to use.
        """
        self.root_dir = root_dir
        self.default_model = default_model
        self.config: dict = {}
        self.agents: List[Agent] = []
        self.session: Optional[Session] = None
        self.orchestrator: Optional[Orchestrator] = None

    def setup(self, orchestrator_name: Optional[str] = None) -> None:
        """
        Sets up the system by loading configurations and instantiating agents.

        Args:
            orchestrator_name (str, optional): The name of the orchestrator to use.

        Raises:
            RuntimeError: If no agents or orchestrator can be set up.
        """
        logger.info("--- System Setup ---")
        print(f"Using default model: {self.default_model}")

        self.config = self._load_config(os.path.join(self.root_dir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME))
        agent_specs = self._load_agent_templates(os.path.join(self.root_dir, AGENTS_DIR_NAME), self.config, self.default_model)
        prompts = self._load_prompts(os.path.join(self.root_dir, PROMPTS_DIR_NAME))

        all_team_members = {member.lower() for spec in agent_specs if 'team' in spec for member in spec['team']}

        agents: List[Agent] = []
        for spec in agent_specs:
            spec["model"] = spec.get("model", self.default_model)
            if spec['name'].lower() not in all_team_members:
                agent = self._instantiate_agent(spec, prompts, agent_specs)
                if agent:
                    agents.append(agent)
        self.agents = agents

        if not self.agents:
            raise RuntimeError("No agents could be instantiated. System setup failed.")

        orchestrator: Optional[Agent] = None
        if orchestrator_name:
            orchestrator = next((agent for agent in self.agents if agent.name.lower() == orchestrator_name.lower()), None)
        else:
            orchestrator = find_agent_by_role(self.agents, 'Orchestrator')

        if not orchestrator:
            error_msg = f"Orchestrator with name '{orchestrator_name}' not found." if orchestrator_name else "Orchestrator with role 'Orchestrator' not found."
            raise RuntimeError(f"{error_msg} System setup failed.")

        if not isinstance(orchestrator, Orchestrator):
            raise RuntimeError(f"Agent '{orchestrator.name}' is not a valid Orchestrator instance. System setup failed.")

        self.orchestrator = orchestrator
        self.session = Session(agents=self.agents, project_root="./")
        logger.info("--- System Setup Complete ---")

    def start(self, high_level_goal: str, files: List[File] = []) -> Plan:
        """
        Runs the main workflow of the system.

        Args:
            initial_task (str): The initial task for the orchestrator to perform.
            plan_only (bool): If True, only generates the plan without executing tasks.
            rounds (int): The number of rounds to execute the workflow.
            files (List[str]): List of files to be used in the task.

        Raises:
            RuntimeError: If the system is not set up before running.
        """
        if not self.orchestrator or not self.session:
            raise RuntimeError("System is not set up. Please call setup() before start().")

        plan: Optional[Plan] = self.orchestrator.generate_plan(self.session, high_level_goal, files)
        if not plan:
            raise RuntimeError("Orchestration failed: Could not generate a valid plan.")

        logger.debug(f"{plan.model_dump_json()}")

        if not plan or not plan.tasks or not plan.roles:
            raise RuntimeError("Orchestration failed: Could not generate a valid plan.")

        print(f"\n\nInitial Plan: {plan.model_dump_json(indent=4)}\n\n\n")
        self.session.add_artifact("initial_plan.json", plan.model_dump_json(indent=4))

        return plan

    def run(self, plan: Plan, rounds: int = 1, files: List[File] = []) -> None:
        if not self.orchestrator or not self.session:
            raise RuntimeError("System is not set up. Please call start() before run().")

        logger.info("--- Starting Workflow ---")

        context = ExecutionContext(session=self.session, plan=plan)

        print(f"\n\nInitial Files: {plan.model_dump_json(indent=4)}\n\n\n")

        #context.record_initial("files", "\n".join([file.model_dump_json(indent=4) for file in files]))
        context.record_initial("files", Artifact(task='initial',files=files).model_dump_json())

        for context.round_num in range(1, rounds + 1):
            logger.info(f"System is starting workflow round {context.round_num} for goal: '{plan.high_level_goal}'")

            team_by_name = {agent.name: agent for agent in self.orchestrator.team.values()} if self.orchestrator.team else {}

            context.reset()

            while context.step_index < len(plan.tasks):
                step = context.current_step()
                agent_name = step.agent

                delegate_agent = team_by_name.get(agent_name)
                if not delegate_agent:
                    delegate_agent = next((agent for agent in self.agents if agent.name.lower() == agent_name.lower()), None)

                if not delegate_agent:
                    logger.warning(f"No agent found with name '{agent_name}'. Execution will continue with the Orchestrator as the fallback agent.")
                    delegate_agent = self.orchestrator

                logger.info(f"Agent '{delegate_agent.name}' is executing step {context.step_index + 1}/{len(plan.tasks)}: '{step.description}' (Role: {step.role})")

                result = delegate_agent.execute_task(context)
                if result:
                    context.record_artifact(f"{delegate_agent.name}_result.txt", result, True)
                    try:
                        agent_output = AgentOutput.model_validate_json(result)
                        if agent_output.team and agent_output.team.prompts:
                            logger.info(f"Agent {delegate_agent.name} provided new prompts.")
                            for prompt_data in agent_output.team.prompts:
                                self._update_agent_prompt(self.session, prompt_data.agent, prompt_data.system_prompt)
                    except (json.JSONDecodeError, Exception) as e:
                        logger.warning(f"Could not parse agent output as AgentOutput: {e}. Treating as plain text.")
                context.next()
            logger.info(f"System has completed workflow round {context.round_num}.")
        logger.info("--- Workflow Complete ---")

    def _update_agent_prompt(self, session: Session, agent_name: str, new_prompt: str) -> None:
        """
        Updates an agent's system prompt.

        Args:
            session (Session): The current session object.
            agent_name (str): The name or role of the agent to update.
            new_prompt (str): The new system prompt.
        """
        if not (agent_name and new_prompt):
            return

        if not (self.orchestrator and self.orchestrator.team):
            logger.warning("Orchestrator or its team is not initialized. Cannot update agent prompt.")
            return

        # Combine team agents and session agents for a comprehensive search, ensuring uniqueness
        all_agents = {agent.name: agent for agent in list(self.orchestrator.team.values()) + session.agents}.values()

        # First, try to find by name
        target_agent = next((agent for agent in all_agents if agent.name.lower() == agent_name.lower()), None)

        # If not found by name, try to find by role
        if not target_agent:
            target_agent = next((agent for agent in all_agents if agent.role.lower() == agent_name.lower()), None)

        if target_agent:
            target_agent.update_system_prompt(new_prompt)
            logger.info(f"Agent '{target_agent.name}' (matched by '{agent_name}') system prompt updated.")
        else:
            logger.warning(f"Target agent '{agent_name}' not found for prompt update.")

    def _instantiate_agent(self, agent_spec: Dict[str, Any], prompts: Dict[str, str], all_agent_specs: List[Dict[str, Any]]) -> Optional[Agent]:
        """
        Instantiates an Agent (or a subclass) based on the provided specification.

        Args:
            agent_spec (Dict[str, Any]): A dictionary containing the agent's specifications.
            prompts (Dict[str, str]): A dictionary of available system prompts.
            all_agent_specs (List[Dict[str, Any]]): A list of all agent specifications for team resolution.

        Returns:
            Optional[Agent]: An instantiated Agent object, or None if the prompt is not found.
        """
        prompt_key = f"{agent_spec['name'].lower()}_instructions.txt"
        if prompt_key not in prompts:
            logger.warning(
                f"Prompt for agent '{agent_spec['name']}' not found with key '{prompt_key}'. Empty system prompt will be used."
            )

        agent = Orchestrator(
            name=agent_spec.get("name", "Unnamed Agent"),
            role=agent_spec.get("role", "Agent"),
            goal=agent_spec.get("goal", ""),
            model=agent_spec.get("model", "default-model"),
            system_prompt="" if prompt_key not in prompts else prompts[prompt_key],
        )

        if agent_spec.get("delegation") and "team" in agent_spec:
            agent.team = {}
            for team_member_name in agent_spec["team"]:
                member_spec = next(
                    (
                        s
                        for s in all_agent_specs
                        if s["name"].lower() == team_member_name.lower()
                    ),
                    None,
                )
                if member_spec:
                    member_agent = self._instantiate_agent(
                        member_spec, prompts, all_agent_specs
                    )  # Recursive call
                else:
                    logger.warning(
                        f"Team member '{team_member_name}' not found in agent specs."
                    )
                    member_agent = self._instantiate_agent(
                        agent_spec={
                            "name": team_member_name,
                            "role": "Unknown",
                            "goal": "",
                            "model": agent_spec.get("model", "default-model"),
                            "system_prompt": "",
                        },
                        prompts=prompts,
                        all_agent_specs=all_agent_specs,
                    )  # Recursive call
                if member_agent:
                    agent.team[team_member_name] = member_agent
        return agent

    def _load_config(self, config_path: str) -> Any:
        """
        Loads the runtime configuration from a YAML file.

        Args:
            config_path (str): The absolute path to the configuration file.

        Returns:
            dict: The loaded configuration as a dictionary.
        """
        logger.info(f"Loading configuration from: {config_path}")
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _load_agent_templates(self, agents_dir: str, config: dict, default_model: str) -> List[Dict[str, Any]]:
        """
        Loads all agent specifications from YAML files within a specified directory.

        Args:
            agents_dir (str): The absolute path to the directory containing agent YAML files.

        Returns:
            list: A list of dictionaries, where each dictionary represents an agent's specification.
        """
        logger.info(f"Loading agent templates from: {agents_dir}")
        agent_files = glob(os.path.join(agents_dir, '*.yaml'))
        templates = []
        for agent_file in agent_files:
            with open(agent_file, 'r', encoding='utf-8') as f:
                template = yaml.safe_load(f)
                # Resolve prompt path relative to the agent file's directory
                if 'system_prompt' in template and template['system_prompt']:
                    base_dir = os.path.dirname(agent_file)
                    prompt_path = os.path.abspath(os.path.join(base_dir, template['system_prompt']))
                    template['system_prompt_path'] = prompt_path
                template["model"] = default_model
                templates.append(template)
        logger.info(f"{len(templates)} agent templates loaded.")
        return templates

    def _load_prompts(self, prompts_dir: str) -> Dict[str, str]:
        """
        Loads all system prompts from text files within a specified directory.

        Args:
            prompts_dir (str): The absolute path to the directory containing prompt text files.

        Returns:
            dict: A dictionary where keys are prompt filenames (e.g., 'agent_instructions.txt')
                and values are the content of the prompt files.
        """
        logger.info(f"Loading prompts from: {prompts_dir}")
        prompt_files = glob(os.path.join(prompts_dir, '*.txt'))
        prompts = {}
        for prompt_file in prompt_files:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                # Use the absolute path as the key for reliable lookup
                prompts[os.path.basename(prompt_file)] = f.read()
        logger.info(f"{len(prompts)} prompts loaded.")
        return prompts

```

--- File: 'runtime/util.py'
```
"""This module provides miscellaneous utility functions for the runtime.

It includes helper functions that are used across different modules
but do not belong to a specific component.
"""

import os

def read_file(file_path: str) -> str:
    """Reads content from a given file path."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return f"Error: File not found at {file_path}"
    except Exception as e:
        return f"Error reading file {file_path}: {e}"

```