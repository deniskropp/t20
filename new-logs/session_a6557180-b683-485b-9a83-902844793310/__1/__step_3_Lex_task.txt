The overall goal is: 'Generate documentation for the runtime.'

Your role's specific goal is: 'Generate clear, concise, and accurate technical documentation based on provided specifications, source code, and developer notes.'
Your specific sub-task is: 'Generate initial drafts for each section of the runtime documentation based on the defined structure and TAS.'

The team's roles are:
    {"high_level_goal":"Generate documentation for the runtime.","reasoning":"The goal is to create comprehensive documentation for the runtime system. This plan breaks down the process into logical steps, assigning specific roles and tasks to ensure all aspects of the documentation are covered.","roles":[{"title":"Task-Agnostic Step (TAS) Extractor","purpose":"Identify and extract each 'Task Agnostic Step' (TAS) towards the high-level goal."},{"title":"Prompt Engineer","purpose":"Structure workflows and ensure clarity in agent instructions, system prompt engineering."},{"title":"Designer","purpose":"Generate aesthetic layouts, color palettes, typography, and UI flows, ensuring accessibility and visual balance."},{"title":"Documentation Writer","purpose":"Generate clear, concise, and accurate technical documentation based on provided specifications, source code, and developer notes."}],"tasks":[{"id":"T1","description":"Extract Task Agnostic Steps (TAS) from the provided code and context related to the runtime documentation.","role":"Task-Agnostic Step (TAS) Extractor","agent":"TASe","requires":[]},{"id":"T2","description":"Based on the extracted TAS, define the overall documentation structure and identify key areas to be documented.","role":"Prompt Engineer","agent":"Lyra","requires":["T1"]},{"id":"T3","description":"Develop system prompts for each agent involved in the documentation process, ensuring clarity and alignment with their roles and the overall goal.","role":"Prompt Engineer","agent":"Lyra","requires":["T2"]},{"id":"T4","description":"Generate initial drafts for each section of the runtime documentation based on the defined structure and TAS.","role":"Documentation Writer","agent":"Lex","requires":["T2","T3"]},{"id":"T5","description":"Review the generated documentation drafts for accuracy, clarity, completeness, and adherence to project standards. Provide feedback for revisions.","role":"Documentation Writer","agent":"Lex","requires":["T4"]},{"id":"T6","description":"Refine the documentation drafts based on the review feedback. This may involve iterative writing and review cycles.","role":"Documentation Writer","agent":"Lex","requires":["T5"]},{"id":"T7","description":"Design the overall layout, visual style, and user interface elements for the documentation, ensuring accessibility and aesthetic appeal.","role":"Designer","agent":"Aurora","requires":["T2"]},{"id":"T8","description":"Integrate the designed layouts and visual elements into the documentation drafts.","role":"Designer","agent":"Aurora","requires":["T6","T7"]},{"id":"T9","description":"Perform a final review of the integrated documentation, checking for consistency, quality, and readiness for publication.","role":"Prompt Engineer","agent":"Lyra","requires":["T8"]},{"id":"T10","description":"Publish the final documentation.","role":"Documentation Writer","agent":"Lex","requires":["T9"]}]}

Please use the following outputs from the other agents as your input:

--- Artifact 'files' from (User) in [initial]:
{"task":"initial","files":[{"path":"runtime/agent.py","content":"\"\"\"This module defines the core Agent class and related functionalities.\n\nIt includes the agent's lifecycle methods, task execution logic, and\nhelper functions for agent instantiation and discovery.\n\"\"\"\n\nimport json as JSON\nimport uuid\nfrom dataclasses import dataclass, field\nimport re\nimport logging\nfrom typing import List, Dict, Any, Optional\n\nfrom runtime.llm import LLM\n\nfrom runtime.core import ExecutionContext, Session\n\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import AgentOutput\n\nclass Agent:\n    \"\"\"Represents a runtime agent instance.\"\"\"\n    name: str\n    role: str\n    goal: str\n    model: str\n    system_prompt: str\n    llm: LLM\n\n    def __init__(self, name: str, role: str, goal: str, model: str, system_prompt: str) -> None:\n        self.name = name\n        self.role = role\n        self.goal = goal\n        self.model = model\n        self.system_prompt = system_prompt\n        logger.info(f\"Agent instance created: {self.name} (Role: {self.role}, Model: {self.model})\")\n        self.llm = LLM.factory(model)\n\n    def update_system_prompt(self, new_prompt: str) -> None:\n        \"\"\"Updates the agent's system prompt.\"\"\"\n\n        self.system_prompt = new_prompt\n\n        logger.info(f\"Agent {self.name}'s system prompt updated:\\n{self.system_prompt}\\n\")\n\n    def execute_task(self, context: ExecutionContext) -> Optional[str]:\n        \"\"\"\n        Executes a task using the Generative AI model based on the provided context.\n\n        Args:\n            context (ExecutionContext): The execution context containing goal, plan, and artifacts.\n\n        Returns:\n            Optional[str]: The result of the task execution as a string, or an error string.\n        \"\"\"\n\n        task = context.current_step()\n\n        context.record_artifact(f\"{self.name}_prompt.txt\", self.system_prompt)\n\n        required_task_ids = ['initial']\n        required_task_ids.extend(task.requires)\n\n        required_artifacts = []\n\n        if len(required_task_ids) != 0:\n            for key, artifact in context.items.items():\n                logger.debug(f\"Checking artifact from {key} with task ID {artifact.step.id}\")\n                if artifact.step.id in required_task_ids:\n                    required_artifacts.append(artifact)\n\n        previous_artifacts = \"\\n\\n\".join(\n            f\"--- Artifact '{artifact.name}' from ({artifact.step.role}) in [{artifact.step.id}]:\\n{artifact.content}\"\n            for artifact in required_artifacts\n        )\n\n        task_prompt: List[str] = [\n            f\"The overall goal is: '{context.plan.high_level_goal}'\",\n            f\"Your role's specific goal is: '{self.goal}'\\n\"\n            f\"Your specific sub-task is: '{task.description}'\",\n\n            f\"The team's roles are:\\n    {context.plan.model_dump_json()}\",\n        ]\n\n        if previous_artifacts:\n            task_prompt.append(f\"Please use the following outputs from the other agents as your input:\\n\\n{previous_artifacts}\\n\\n\")\n\n        task_prompt.append(\n            f\"Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.\"\n        )\n\n        #logger.info(f\"Agent '{self.name}' is executing task: {task_prompt[1]}\")\n        #logger.info(f\"Task requires outputs from task IDs: {required_task_ids}\")\n        #logger.info(\n        #    f\"Found {len(required_artifacts)} required artifacts from previous tasks. Previous artifacts preview: {previous_artifacts[:500]}\"\n        #)\n\n        ret = self._run(context, \"\\n\\n\".join(task_prompt))\n        logger.info(f\"Agent '{self.name}' completed task: {task.description}\")\n        return ret\n\n\n    def _run(self, context: ExecutionContext, prompt: str) -> Optional[str]:\n        context.record_artifact(f\"{self.name}_task.txt\", prompt)\n\n        try:\n            response = self.llm.generate_content(   # ignore type\n                model_name=self.model,\n                contents=prompt,\n                system_instruction=self.system_prompt,\n                temperature=0.7,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=AgentOutput\n            )\n            if isinstance(response, AgentOutput):\n                result = response.model_dump_json()\n                print(f\"\\n--- Output:\\n{response.output}\\n\")\n                if response.artifact and response.artifact.files:\n                    for file in response.artifact.files:\n                        print(f\"\\n--- File: {file.path}\\n{file.content}\\n\")\n                        context.session.add_artifact(file.path, file.content)\n            else:\n                result = response or '{}'\n\n                if '```' in result:\n                    match = re.search(r'```(json)?\\s*([\\s\\S]*?)\\s*```', result, re.IGNORECASE)\n                    if match:\n                        # Extract the JSON content from the markdown block\n                        result = match.group(2).strip()\n\n                json_data = JSON.loads(result)\n                if isinstance(json_data, dict):\n                    if \"output\" in json_data and json_data[\"output\"] is not None:\n                        output = JSON.dumps(json_data[\"output\"], indent=4) if isinstance(json_data[\"output\"], dict) else json_data[\"output\"]\n                        logger.info(f\"Output:\\n{output[:2000]}\\n\")\n\n                    if \"artifact\" in json_data and json_data[\"artifact\"] is not None:\n                        if \"files\" in json_data[\"artifact\"]:\n                            for file_data in json_data[\"artifact\"][\"files\"]:\n                                if \"path\" in file_data and \"content\" in file_data:\n                                    print(f\"\\n--- File: {file_data[\"path\"]}\")\n                                    print(f\"{file_data[\"content\"]}\")\n                                    context.session.add_artifact(file_data[\"path\"], file_data[\"content\"])\n                else:\n                    logger.warning(\"Agent output is not in expected AgentOutput format. Processing as plain text.\")\n                    logger.info(f\"Output: '\\n{result[:2000]}\\n'\")\n\n        except Exception as e:\n            logger.exception(f\"Error executing task for {self.name}: {e}\")\n            raise\n\n        return result\n\ndef find_agent_by_role(agents: List[Agent], role: str) -> Optional[Agent]:\n    \"\"\"\n    Finds an agent in a list by its role.\n\n    Args:\n        agents (List[Agent]): A list of Agent objects.\n        role (str): The role of the agent to find.\n\n    Returns:\n        Optional[Agent]: The found Agent object, or None if not found.\n    \"\"\"\n    logger.debug(f\"Searching for agent with role: {role} in agents: {[agent.name for agent in agents]}\")\n    return next((agent for agent in agents if agent.role == role), None)\n"},{"path":"runtime/core.py","content":"\"\"\"This module provides the core data structures for the multi-agent runtime.\n\nIt defines the ExecutionContext for managing workflow state and the Session\nfor handling artifacts and session-specific data.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List\nimport uuid\nimport os\nimport logging\nimport json\n\nfrom runtime.custom_types import Plan, Task\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContextItem:\n    def __init__(self, name: str, content: Any, step: Task):\n        self.name = name\n        self.content = content\n        self.step = step\n\n\n@dataclass\nclass ExecutionContext:\n    \"\"\"Holds the state and context for a multi-agent workflow.\"\"\"\n    session: 'Session'\n    plan: Plan\n    step_index: int = 0\n    round_num: int = 0\n    items: Dict[str, ContextItem] = field(default_factory=dict)\n\n    def next(self) -> None:\n        self.step_index += 1\n\n    def reset(self) -> None:\n        self.step_index = 0\n\n    def current_step(self) -> Task:\n        \"\"\"Returns the current step in the plan.\"\"\"\n        return self.plan.tasks[self.step_index]\n\n    def _remember_artifact(self, key: str, value: Any, step: Task | None = None) -> None:\n        \"\"\"Remembers an artifact from a step's execution for future tasks.\"\"\"\n        self.items[key] = ContextItem(name=key, content=value, step=step or self.current_step())\n\n    def record_artifact(self, key: str, value: Any, mem: bool = False) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n            mem (bool): If True, the artifact is also stored in the execution context's memory.\n        \"\"\"\n        artifact_key = f\"__{self.round_num}/__step_{self.step_index}_{key}\"\n        self.session.add_artifact(artifact_key, value)\n        if mem:\n            self._remember_artifact(artifact_key, value)\n\n    def record_initial(self, key: str, value: Any) -> None:\n        \"\"\"\n        Records an artifact from a step's execution and optionally remembers it.\n\n        Args:\n            key (str): The key under which to store the artifact.\n            value (Any): The content of the artifact.\n        \"\"\"\n        self._remember_artifact(key, value, step=Task(\n            id='initial',\n            description='Initial files provided by the user',\n            role='User',\n            agent='User'\n        ))\n\n\n\n\n@dataclass\nclass Session:\n    \"\"\"Manages the runtime context for a task, including session ID and artifact storage.\"\"\"\n    session_id: str = field(default_factory=lambda: f\"session_{uuid.uuid4()}\")\n    agents: list = field(default_factory=list) # Type hint will be updated later to List[Agent]\n    state: str = \"initialized\"\n    session_dir: str = field(init=False)\n    project_root: str = field(default_factory=str)\n\n    def __post_init__(self) -> None:\n        \"\"\"Initializes the session directory.\"\"\"\n        if not self.project_root:\n            self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n        self.session_dir = os.path.join(self.project_root, 'sessions', self.session_id)\n        os.makedirs(self.session_dir, exist_ok=True)\n        logger.info(f\"Session created: {self.session_id} (Directory: {self.session_dir})\")\n\n    def add_artifact(self, name: str, content: Any) -> None:\n        \"\"\"\n        Saves an artifact in the session directory.\n\n        Args:\n            name (str): The name of the artifact file.\n            content (Any): The content to write to the artifact file. Can be dict, list, or string.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            os.makedirs(os.path.dirname(artifact_path), exist_ok=True)\n            with open(artifact_path, 'w', encoding='utf-8') as f:\n                if isinstance(content, (dict, list)):\n                    json.dump(content, f, indent=4)\n                else:\n                    f.write(str(content))\n            logger.info(f\"Artifact '{name}' saved in session {self.session_id}.\")\n        except (TypeError, IOError) as e:\n            logger.error(f\"Error saving artifact '{name}': {e}\")\n\n    def get_artifact(self, name: str) -> Any:\n        \"\"\"\n        Loads an artifact from the session directory.\n\n        Args:\n            name (str): The name of the artifact file to load.\n\n        Returns:\n            Any: The content of the artifact, or None if an error occurs.\n        \"\"\"\n        artifact_path = os.path.join(self.session_dir, name)\n        try:\n            with open(artifact_path, 'r', encoding='utf-8') as f:\n                if name.endswith('.json'):\n                    return json.load(f)\n                return f.read()\n        except (FileNotFoundError, json.JSONDecodeError, IOError) as e:\n            logger.error(f\"Error retrieving artifact '{name}': {e}\")\n            return None\n"},{"path":"runtime/custom_types.py","content":"\"\"\"This module defines custom data types and Pydantic models for the runtime.\n\nIt ensures data consistency and validation across different modules by providing\ncentralized, well-defined data structures for plans, tasks, and agent outputs.\n\"\"\"\n\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass Task(BaseModel):\n    \"\"\"A single actionable step in a plan.\"\"\"\n    id: str = Field(..., description=\"Unique ID for the task (e.g., 'T1', 'T-020', 'T_02_DESIGN_01').\")\n    description: str = Field(..., description=\"What the task is and how to execute it.\")\n    role: str = Field(..., description=\"Role responsible for performing this task.\")\n    agent: str = Field(..., description=\"Specific agent assigned to this task.\")\n    requires: List[str] = Field(default_factory=list, description=\"List of prerequisite task IDs.\")\n\n\nclass File(BaseModel):\n    \"\"\"Represents a generated file.\"\"\"\n    path: str = Field(..., description=\"File path or name (e.g., 'src/main.py').\")\n    content: str = Field(..., description=\"Full content of the file.\")\n\n\nclass Artifact(BaseModel):\n    \"\"\"Output artifact generated by a task.\"\"\"\n    task: str = Field(..., description=\"ID of the task that produced this artifact.\")\n    files: List[File] = Field(default_factory=list, description=\"Files included in the artifact.\")\n\n\nclass Role(BaseModel):\n    \"\"\"Definition of a role in the team.\"\"\"\n    title: str = Field(..., description=\"Role title (e.g., 'Engineer', 'Reviewer').\")\n    purpose: str = Field(..., description=\"High-level responsibilities and objectives of this role.\")\n\n\nclass Plan(BaseModel):\n    \"\"\"Overall strategy with reasoning, roles, and tasks.\"\"\"\n    high_level_goal: str = Field(..., description=\"The overarching objective this plan aims to accomplish.\")\n    reasoning: str = Field(..., description=\"Why this plan is structured this way.\")\n    roles: List[Role] = Field(default_factory=list, description=\"All roles needed for execution.\")\n    tasks: List[Task] = Field(default_factory=list, description=\"Step-by-step ordered tasks.\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"System prompt assigned to an agent.\"\"\"\n    agent: str = Field(..., description=\"Name of the agent this prompt is for. Must match exactly an existing agent.\")\n    role: str = Field(..., description=\"Role context of the agent.\")\n    system_prompt: str = Field(..., description=\"Full text of the system prompt for this agent.\")\n\n\nclass Team(BaseModel):\n    \"\"\"Metadata and coordination for the team.\"\"\"\n    notes: str = Field(..., description=\"General notes, reflections, or feedback about the team or process.\")\n    prompts: List[Prompt] = Field(default_factory=list, description=\"Updated system prompts for agents.\")\n\n\nclass AgentOutput(BaseModel):\n    \"\"\"Structured output returned by an agent after executing a task.\"\"\"\n    output: str = Field(..., description=\"Message from the agent, accompanied by artifact (or files).\")\n    artifact: Optional[Artifact] = Field(default=None, description=\"Generated artifact, if applicable.\")\n    team: Optional[Team] = Field(default=None, description=\"Updates to team configuration or system prompts.\")\n    reasoning: Optional[str] = Field(default=None, description=\"Explanation of how the agent arrived at this output.\")\n\n\n\nif __name__ == \"__main__\":\n    import json\n    print(json.dumps(AgentOutput.model_json_schema(), indent=4))\n"},{"path":"runtime/__init__.py","content":"# This file makes the 'runtime' directory a Python package.\n\nfrom .custom_types import Role, Task, Plan, Artifact, File, Prompt, Team, AgentOutput\n\nfrom .agent import Agent\nfrom .core import Session\nfrom .orchestrator import Orchestrator\nfrom .system import System\n"},{"path":"runtime/llm.py","content":"\"\"\"This module abstracts interactions with Large Language Models (LLMs).\n\nIt provides a unified interface for various LLM providers and models,\nallowing for easy integration and interchangeability of different LLMs.\n\"\"\"\n\nimport json\nimport os\nimport time\nfrom google import genai\nfrom google.genai import types\nfrom ollama import Client as Ollama\nfrom typing import Optional, Any, Dict\nfrom abc import ABC, abstractmethod\nimport logging\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLM(ABC):\n    \"\"\"\n    Abstract base class for Large Language Models.\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    def __init__(self) -> None:\n        pass\n\n    @abstractmethod\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: Any = None) -> Optional[str]: # type: ignore\n        \"\"\"Generates content using an LLM.\"\"\"\n        pass\n\n    @staticmethod\n    def factory(species: str) -> 'LLM':\n        print(f\"LLM Factory: Creating LLM instance for species '{species}'\")\n        if species == 'Olli':\n            return Olli()\n        elif species == 'Kimi':\n            return Kimi()\n        if species.startswith('ollama:'):\n            # Extract model name after 'ollama:'\n            model_name = species.split(':', 1)[1]\n            return Olli(species=model_name)\n        if species.startswith('opi:'):\n            # Extract model name after 'opi:'\n            model_name = species.split(':', 1)[1]\n            return Opi(species=model_name)\n        if species.startswith('mistral:'):\n            # Extract model name after 'mistral:'\n            model_name = species.split(':', 1)[1]\n            return Mistral(species=model_name)\n        return Gemini(species)\n\n\nclass Gemini(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients: Dict[str, genai.Client] = {} # Class-level cache for clients\n\n    def __init__(self, species: str) -> None:\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        config = types.GenerateContentConfig(\n            system_instruction=system_instruction,\n            temperature=temperature,\n            response_mime_type=response_mime_type,\n            response_schema=response_schema,\n            max_output_tokens=50000,\n        )\n\n        # try three times\n        for _ in range(3):\n            try:\n                response = client.models.generate_content(\n                    model=model_name,\n                    contents=contents,\n                    config=config,\n                )\n                if not response.candidates or response.candidates[0].content is None or response.candidates[0].content.parts is None or not response.candidates[0].content.parts or response.candidates[0].content.parts[0].text is None:\n                    raise ValueError(f\"Gemini: No content in response from model {model_name}. Retrying...\")\n                break\n            except Exception as ex:\n                logger.exception(f\"Error generating content with model {model_name}: {ex}\")\n                # If it's the last retry, return None\n                if _ == 2:\n                    return None\n                logger.warning(f\"Retrying content generation for model {model_name}...\")\n                time.sleep(10)\n                #return None\n\n        if isinstance(response.parsed, BaseModel):\n            return response.parsed\n\n        if response_mime_type == 'application/json':\n            try:\n                # Attempt to parse the JSON response\n                json_output = response.candidates[0].content.parts[0].text.strip()\n                # Validate against schema if provided\n                if response_schema:\n                    response_schema.model_validate_json(json_output)\n                return json_output\n            except Exception as ex:\n                logger.exception(f\"Error parsing or validating JSON response: {ex}\")\n\n        return response.candidates[0].content.parts[0].text.strip()\n\n    def _get_client(self) -> genai.Client:\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if self.species not in Gemini._clients:\n                logger.info(f\"Initializing GenAI client for species {self.species}\")\n                Gemini._clients[self.species] = genai.Client()\n            return Gemini._clients[self.species]\n        except Exception as e:\n            logger.exception(f\"Error initializing GenAI client: {e}\")\n            raise\n\n\nclass Olli(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    def __init__(self, species: str = 'Olli'):\n        self.client = None\n        self.species = species\n\n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        print(f\"Olli: Using model {model_name} with temperature {temperature}\")\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n            fmt = response_schema.model_json_schema()\n            print(f\"Olli: Using response format {fmt}\")\n            response = client.generate(\n                model=self.species,#model_name,\n                prompt=contents,\n                format=fmt,\n                options={\"temperature\": temperature, \"system_instruction\": system_instruction},\n                stream=True\n            )\n            for chunk in response:\n                if hasattr(chunk, \"response\") and chunk.response:\n                    print(chunk.response, end=\"\")\n                    out += chunk.response\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a GenAI client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Olli._clients:\n                    Olli._clients[self.species] = Ollama()#base_url='http://localhost:11434')\n            return Olli._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing GenAI client: {e}\")\n            return None\n\n\n\n\n\nfrom huggingface_hub import InferenceClient, ChatCompletionInputResponseFormatText, ChatCompletionInputResponseFormatJSONObject, ChatCompletionInputResponseFormatJSONSchema, ChatCompletionInputJSONSchema\n\n\nclass Kimi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Kimi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            stream = client.chat.completions.create(\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=ChatCompletionInputResponseFormatText() if response_mime_type == 'text/plain' else ChatCompletionInputResponseFormatJSONObject() #if response_schema is None else ChatCompletionInputResponseFormatJSONSchema(json_schema=ChatCompletionInputJSONSchema(name=response_schema.model_json_schema()))\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a inference client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Kimi._clients:\n                    Kimi._clients[self.species] = InferenceClient(\n                        provider=\"featherless-ai\",\n                        api_key=os.environ.get(\"HF_TOKEN\"),\n                        model=\"moonshotai/Kimi-K2-Instruct\"\n                    )\n            return Kimi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing GenAI client: {e}\")\n            return None\n\n\n\nfrom openai import OpenAI\nfrom openai.types.chat.completion_create_params import ResponseFormat\n\n\nclass Opi(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Opi'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified GenAI model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"gemini-pro\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            types.GenerateContentResponse: The response from the GenAI model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n\n        try:\n            out = \"\"\n\n            response_format: ResponseFormat = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = {\"type\": \"json_schema\", \"json_schema\": response_schema.model_json_schema()}   # type: ignore\n\n            print(f\"Opi: Using response format {response_format}\")\n\n            stream = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": system_instruction\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": contents\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=50000,\n                top_p=1,\n                stream=True,\n                response_format=response_format\n            )\n\n            for chunk in stream:\n                if chunk.choices[0].delta.content is None:\n                    continue\n                print(chunk.choices[0].delta.content, end=\"\")\n                out += chunk.choices[0].delta.content\n\n            return out\n        except Exception as e:\n            logger.error(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a OpenAI client instance.\n        \"\"\"\n        try:\n            if (self.client is None):\n                if self.species not in Opi._clients:\n                    Opi._clients[self.species] = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"), base_url=os.environ.get(\"OPENAI_API_BASE\"))\n            return Opi._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing OpenAI client: {e}\")\n            return None\n\n\n\n##\n\n\nfrom mistralai import Mistral as MistralClient\nfrom mistralai.extra.utils import response_format_from_pydantic_model\n\nclass Mistral(LLM):\n    \"\"\"\n    Provides a centralized place for LLM-related configurations and utilities.\n    \"\"\"\n    _clients = {} # Class-level cache for clients\n\n    def __init__(self, species: str = 'Mistral'):\n        self.species = species\n    \n    def generate_content(self, model_name: str, contents: str, system_instruction: str = '',\n                         temperature: float = 0.7, response_mime_type: str = 'text/plain', response_schema: BaseModel = None) -> Optional[str]: # type: ignore\n        \"\"\"\n        Generates content using the specified Mistral model.\n\n        Args:\n            model_name (str): The name of the model to use (e.g., \"mistral-large-latest\").\n            contents (str): The input content for the model.\n            system_instruction (str, optional): System-level instructions for the model.\n            temperature (float, optional): Controls the randomness of the output. Defaults to 0.7.\n            response_mime_type (str, optional): The desired MIME type for the response.\n            response_schema (Any, optional): The schema for the response.\n\n        Returns:\n            str: The response from the Mistral model.\n        \"\"\"\n        client = self._get_client()\n        if not client:\n            return None\n        try:\n            messages = []\n            if system_instruction:\n                messages.append({\"role\": \"system\", \"content\": system_instruction})\n            messages.append({\"role\": \"user\", \"content\": contents})\n\n            print(f\"Mistral: Using model {model_name} with temperature {temperature}\")\n\n            response_format = {\"type\": \"text\"}\n\n            if response_mime_type == 'application/json':\n                response_format = {\"type\": \"json_object\"}\n\n            if response_schema:\n                response_format = response_format_from_pydantic_model(response_schema)  # type: ignore\n\n\n            out = \"\"\n            stream = client.chat.stream(\n                model=self.species,#model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=50000,\n                response_format=response_format # type: ignore\n            )\n            for chunk in stream:\n                if chunk.data.choices[0].delta.content is None:\n                    continue\n                print(chunk.data.choices[0].delta.content, end=\"\")\n                out += str(chunk.data.choices[0].delta.content)\n\n            return out\n        except Exception as e:\n            print(f\"Error generating content with model {model_name}: {e}\")\n            return None\n\n    def _get_client(self):\n        \"\"\"\n        Returns a Mistral client instance.\n        \"\"\"\n        try:\n            if self.client is None:\n                if self.species not in Mistral._clients:\n                    api_key = os.environ.get(\"MISTRAL_API_KEY\")\n                    if not api_key:\n                        raise ValueError(\"MISTRAL_API_KEY environment variable not set.\")\n                    Mistral._clients[self.species] = MistralClient(api_key=api_key)\n            return Mistral._clients[self.species]\n        except Exception as e:\n            logger.error(f\"Error initializing Mistral client: {e}\")\n            return None\n"},{"path":"runtime/log.py","content":"\"\"\"This module configures and sets up the logging for the application.\n\nIt defines the log format, level, and handlers (console, file, JSONL),\nproviding a centralized and consistent logging setup.\n\"\"\"\n\nimport logging\nimport os\nimport re\nimport json\nfrom logging.handlers import RotatingFileHandler\nfrom colorama import Fore, Style\n\n# --- Configuration ---\nLOG_LEVEL = \"INFO\"\nLOG_FILE_DIR = \"logs\"\nLOG_FILE_NAME = \"app.log\"\nLOG_FILE_PATH = os.path.join(LOG_FILE_DIR, LOG_FILE_NAME)\nMAX_FILE_SIZE = 1024 * 1024  # 1 MB\nBACKUP_COUNT = 5\n\n# --- Colored Formatter ---\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"\n    A custom logging formatter that adds colors to the log messages.\n    It can color based on log level, or on regex matching of the message.\n    \"\"\"\n    # ANSI escape codes for colors\n    COLORS = {\n        'DEBUG': Fore.YELLOW,\n        'INFO': Fore.CYAN,\n        'WARNING': Fore.YELLOW,\n        'ERROR': Fore.RED,\n        'CRITICAL': Fore.MAGENTA,\n        'RESET': Style.RESET_ALL\n    }\n\n    # Regex-based coloring rules (pattern, color)\n    REGEX_RULES_MESSAGES = [\n        (re.compile(r\"Agent instance created:\"), Fore.YELLOW),\n        (re.compile(r\"Agent '.*' system prompt updated.\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing task\"), Fore.GREEN),\n        (re.compile(r\"Agent '.*' is executing step\"), Fore.LIGHTRED_EX),\n        (re.compile(r\"Agent '.*' completed task\"), Fore.BLUE),\n        (re.compile(r\"Warning: Agent output is not in expected AgentOutput format.\"), Fore.YELLOW),\n        (re.compile(r\"System Runtime\"), Fore.CYAN),\n        (re.compile(r\"Error: No agents could be instantiated.\"), Fore.RED),\n        (re.compile(r\"Session created:\"), Fore.GREEN),\n        (re.compile(r\"Artifact '.*' saved in session .*\"), Fore.LIGHTBLUE_EX),\n        (re.compile(r\"Error saving artifact '.*'\"), Fore.RED),\n        (re.compile(r\"Files provided:\"), Fore.MAGENTA),\n        (re.compile(r\"Error reading file .*\"), Fore.RED),\n        (re.compile(r\"Generated plan:\"), Fore.WHITE),\n        (re.compile(r\"Orchestration failed: Could not generate a valid plan.\"), Fore.RED),\n        (re.compile(r\"Orchestrator .* is generating a plan for\"), Fore.BLUE),\n        (re.compile(r\"Orchestrator .* is starting workflow round\"), Fore.CYAN),\n        (re.compile(r\"Orchestrator has completed workflow round\"), Fore.CYAN),\n        (re.compile(r\"Agent .* provided new prompts.\"), Fore.LIGHTGREEN_EX),\n        (re.compile(r\"Target agent '.*' not found for prompt update.\"), Fore.YELLOW),\n        (re.compile(r\"Error generating or validating plan for .*\"), Fore.RED),\n    ]\n\n    COLORS_NAMES = {\n        'runtime.orchestrator': Fore.LIGHTYELLOW_EX,\n        'runtime.agent': Fore.LIGHTWHITE_EX,\n        'runtime.sysmain': Fore.LIGHTRED_EX,\n        'runtime.core': Fore.LIGHTBLUE_EX,\n        'runtime.bootstrap': Fore.LIGHTRED_EX,\n        'runtime.loader': Fore.LIGHTRED_EX,\n        'runtime.llm': Fore.LIGHTMAGENTA_EX,\n        'runtime.util': Fore.GREEN,\n        'runtime.factory': Fore.LIGHTRED_EX,\n        'runtime.paths': Fore.BLUE,\n        'runtime.custom_types': Fore.CYAN,\n    }\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"\n        Formats the log record with the appropriate color.\n        \"\"\"\n        log_message = super().format(record)\n\n        # Check for regex-based coloring first\n        for pattern, color in self.REGEX_RULES_MESSAGES:\n            if pattern.search(record.getMessage()):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Check for logger name-based coloring\n        for name_prefix, color in self.COLORS_NAMES.items():\n            if record.name.startswith(name_prefix):\n                return f\"{color}{log_message}{self.COLORS['RESET']}\"\n\n        # Fallback to level-based coloring\n        return f\"{self.COLORS.get(record.levelname, self.COLORS['RESET'])}{log_message}{self.COLORS['RESET']}\"\n\n# --- Setup ---\ndef setup_logging(level: str = LOG_LEVEL) -> None:\n    \"\"\"\n    Sets up logging for the application.\n    \"\"\"\n    # Create log directory if it doesn't exist\n    if not os.path.exists(LOG_FILE_DIR):\n        os.makedirs(LOG_FILE_DIR)\n\n    # Get the root logger\n    logger = logging.getLogger()\n    logger.setLevel(level)\n\n    # --- Console Handler ---\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(level)\n\n    # --- File Handler ---\n    file_handler = RotatingFileHandler(\n        LOG_FILE_PATH,\n        maxBytes=MAX_FILE_SIZE,\n        backupCount=BACKUP_COUNT\n    )\n    file_handler.setLevel(level)\n\n    # --- Formatters ---\n    console_formatter = ColoredFormatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    file_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n\n    class JsonFormatter(logging.Formatter):\n        def format(self, record: logging.LogRecord):\n            log_record = {\n                \"timestamp\": self.formatTime(record, self.datefmt),\n                \"name\": record.name,\n                \"level\": record.levelname,\n                \"message\": record.getMessage(),\n                \"pathname\": record.pathname,\n                \"lineno\": record.lineno,\n                \"funcName\": record.funcName,\n                \"process\": record.process,\n                \"thread\": record.thread,\n                \"threadName\": record.threadName,\n            }\n            if record.exc_info:\n                log_record[\"exc_info\"] = self.formatException(record.exc_info)\n            if record.stack_info:\n                log_record[\"stack_info\"] = self.formatStack(record.stack_info)\n            return json.dumps(log_record)\n\n    # --- Add Formatter to Handlers ---\n    console_handler.setFormatter(console_formatter)\n    file_handler.setFormatter(file_formatter)\n\n    # --- Add Handlers to Logger ---\n    # Avoid adding handlers multiple times\n    if not logger.handlers:\n        logger.addHandler(console_handler)\n        logger.addHandler(file_handler)\n\n        # --- JSONL File Handler ---\n        jsonl_file_handler = RotatingFileHandler(\n            os.path.join(LOG_FILE_DIR, \"app.jsonl\"),\n            maxBytes=MAX_FILE_SIZE,\n            backupCount=BACKUP_COUNT\n        )\n        jsonl_file_handler.setLevel(level)\n        jsonl_file_handler.setFormatter(JsonFormatter())\n        logger.addHandler(jsonl_file_handler)\n\n# --- Main Application Logger ---\n# It's good practice to use a named logger instead of the root logger directly\napp_logger = logging.getLogger(__name__)\n\n# --- Example Usage ---\nif __name__ == '__main__':\n    setup_logging()\n    app_logger.info(\"Logging setup complete.\")\n    app_logger.debug(\"This is a debug message.\")\n    app_logger.warning(\"This is a warning message.\")\n    app_logger.error(\"This is an error message.\")\n"},{"path":"runtime/orchestrator.py","content":"\"\"\"This module defines the Orchestrator agent and its planning capabilities.\n\nThe Orchestrator is a specialized agent responsible for generating, managing,\nand executing the workflow plan based on a high-level goal. This is being\nrefactored to move workflow execution to the System class.\n\"\"\"\n\nimport os\nimport json\nfrom typing import List, Dict, Optional, Tuple\nimport logging\n\nfrom pydantic import BaseModel, Field, ValidationError\n\nfrom runtime.agent import Agent\nfrom runtime.core import Session\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\nfrom runtime.custom_types import Plan, File\n\nclass Orchestrator(Agent):\n    \"\"\"An agent responsible for creating and managing a plan for multi-agent workflows.\"\"\"\n    team: Dict[str,Agent] = {}\n\n    def generate_plan(self, session: Session, high_level_goal: str, files: List[File] = []) -> Optional[Plan]:\n        \"\"\"\n        Invokes the language model to get a structured plan for the given high-level goal.\n        The plan includes a sequence of tasks and the roles responsible for them.\n\n        Args:\n            session (Session): The current session object.\n            high_level_goal (str): The high-level goal for which to generate a plan.\n            file_contents (List[Tuple[str, str]]): A list of file paths and their contents to provide context to the LLM.\n\n        Returns:\n            Optional[Plan]: The generated plan as a Pydantic object, or None if an error occurs.\n        \"\"\"\n        logger.info(f\"Orchestrator {self.name} is generating a plan for: '{high_level_goal}'\")\n        if not self.llm or not self.team:\n            logger.error(\"Orchestrator client or team not initialized. Cannot generate plan.\")\n            return None\n\n        # Construct a description of the available agents and their roles/goals\n        team_description = \"\\n\".join(\n            f\"- Name: '{agent.name}'\\n\"\n            f\"  Role: `{agent.role}`\\n\"\n            f\"  Goal: \\\"{agent.goal}\\\"\"\n            for agent in self.team.values()\n        )\n\n        # Load the general planning prompt template\n        t20_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) # Not used\n        general_prompt_template = read_file(os.path.join(t20_root, \"prompts\", \"general_planning.txt\")).strip()\n        planning_prompt_parts = [general_prompt_template.format(\n            high_level_goal=high_level_goal,\n            team_description=team_description\n        )]\n\n        # Include file contents if provided\n        file_section = []\n        for file in files:\n            file_section.append(f\"\\n--- File: '{file.path}'\\n```\\n{file.content}\\n```\")\n        planning_prompt_parts.append(\"\\n\".join(file_section))\n\n        logger.debug(\"Planning prompt parts for LLM: %s\", planning_prompt_parts)\n\n        # Combine prompt parts for the LLM call\n        full_prompt_for_llm = \"\\n\\n\".join(planning_prompt_parts)\n        logger.info(f\"Orchestrator {self.name} Planning Prompt:\\n{full_prompt_for_llm}\")\n\n        session.add_artifact(\"planning_prompt.txt\", full_prompt_for_llm)\n\n        try:\n            response = self.llm.generate_content(\n                model_name=self.model,\n                contents=\"\\n\\n\".join(planning_prompt_parts),\n                system_instruction=self.system_prompt,\n                temperature=0.1,\n                response_mime_type='application/json', #if self.role == 'Prompt Engineer' else None\n                response_schema=Plan\n            )\n            if isinstance(response, Plan):\n                plan = response\n            else:\n                plan = Plan.model_validate_json(response or '{}')\n        except (ValidationError, json.JSONDecodeError) as e:\n            logger.exception(f\"Error generating or validating plan for {self.name}: {e}\")\n            return None\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during plan generation for {self.name}: {e}\")\n            return None\n\n        return plan\n"},{"path":"runtime/paths.py","content":"\"\"\"\nCentralized path constants for the project.\n\nThis module defines constants for directory and file names, allowing them to be\nreused consistently throughout the application.\n\"\"\"\n\n# Directory Names\nAGENTS_DIR_NAME = \"agents\"\nCONFIG_DIR_NAME = \"config\"\nPROMPTS_DIR_NAME = \"prompts\"\n\n# File Names\nRUNTIME_CONFIG_FILENAME = \"runtime.yaml\""},{"path":"runtime/sysmain.py","content":"\"\"\"This module serves as the main entry point for the command-line interface.\n\nIt parses command-line arguments and initiates the system bootstrap process,\nacting as the primary interface for running the multi-agent system.\n\"\"\"\n\nimport os\nimport argparse\nimport logging\n\nfrom runtime.log import setup_logging\nfrom runtime.system import System\nfrom runtime.custom_types import File\nfrom runtime.util import read_file\n\nlogger = logging.getLogger(__name__)\n\ndef parse_command_line_arguments() -> argparse.Namespace:\n    \"\"\"\n    Parses command-line arguments for the runtime system.\n\n    Returns:\n        An argparse.Namespace object containing the parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Run the Gemini agent runtime.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    general_group = parser.add_argument_group(\"General Settings\")\n    general_group.add_argument(\"-p\", \"--plan-only\", action=\"store_true\", help=\"Generate only the plan without executing tasks.\")\n    general_group.add_argument(\"-r\", \"--rounds\", type=int, default=1, help=\"The number of rounds to execute the workflow.\")\n    general_group.add_argument(\"-f\", \"--files\", nargs='*', help=\"List of files to be used in the task.\", default=[])\n\n    orchestrator_group = parser.add_argument_group(\"Orchestrator Settings\")\n    orchestrator_group.add_argument(\"-o\", \"--orchestrator\", type=str, help=\"The name of the orchestrator to use.\", default=\"Meta-AI\")\n    orchestrator_group.add_argument(\"-m\", \"--model\", type=str, help=\"Default LLM model to use.\", default=\"gemini-2.5-flash-lite\")\n\n    parser.add_argument(\"task\", type=str, help=\"The initial task for the orchestrator to perform.\")\n\n    args = parser.parse_args()\n\n    if args.rounds < 1:\n        parser.error(\"Number of rounds must be at least 1.\")\n    if not args.task:\n        parser.error(\"The task argument is required.\")\n\n    return args\n\ndef setup_application_logging(log_level: str = \"INFO\") -> None:\n    \"\"\"\n    Sets up the global logging configuration for the application.\n\n    Args:\n        log_level: The desired logging level (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\n    \"\"\"\n    try:\n        setup_logging(level=log_level)\n        logger.info(f\"Logging initialized with level: {log_level}\")\n    except ValueError as e:\n        logger.error(f\"Invalid log level '{log_level}': {e}\")\n        raise\n\ndef system_main() -> None:\n    \"\"\"\n    Main entry point for the runtime system.\n    Parses arguments, sets up logging and configuration, and runs the system.\n    \"\"\"\n    try:\n        # 1. Parse command line arguments\n        args = parse_command_line_arguments()\n        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\n        # 2. Initial logging setup\n        setup_application_logging()\n\n        # 3. Convert file paths to File objects\n        file_objects = []\n        for file_path in args.files:\n            try:\n                content = read_file(file_path)\n                if content.startswith(\"Error:\"):\n                    logger.warning(f\"Skipping file '{file_path}': {content}\")\n                    continue\n                file_objects.append(File(path=file_path, content=content))\n            except Exception as e:\n                logger.error(f\"Error processing file '{file_path}': {e}\")\n\n        # 4. Instantiate and set up the system\n        system = System(root_dir=project_root, default_model=args.model)\n        system.setup(orchestrator_name=args.orchestrator)\n\n        # 5. Re-configure logging based on loaded config\n        log_level = system.config.get(\"logging_level\", \"INFO\")\n        setup_application_logging(log_level=log_level)\n\n        # 6. Start the system's main workflow\n        plan = system.start(\n            high_level_goal=args.task,\n            files=file_objects\n        )\n        if args.plan_only:\n            logger.info(\"Plan-only mode: Workflow execution skipped.\")\n            return\n\n        # 7. Run the system's main workflow\n        system.run(\n            plan=plan,\n            rounds=args.rounds,\n            files=file_objects\n        )\n\n    except (FileNotFoundError, RuntimeError) as e:\n        logger.exception(f\"A critical error occurred: {e}\")\n        exit(1)\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred during system main execution: {e}\")\n        exit(1)\n\n\nif __name__ == \"__main__\":\n    system_main()\n"},{"path":"runtime/system.py","content":"\"\"\"This module defines the core System class for the multi-agent runtime.\n\nIt encapsulates the state and behavior of the entire system, including\nconfiguration loading, agent instantiation, and workflow execution.\n\"\"\"\n\nimport os\nimport json\nimport yaml\nfrom glob import glob\n\nimport logging\nfrom typing import Any, List, Optional, Dict\n\nfrom .core import Session, ExecutionContext\nfrom .agent import Agent, find_agent_by_role\nfrom .orchestrator import Orchestrator\nfrom .log import setup_logging\nfrom .paths import AGENTS_DIR_NAME, CONFIG_DIR_NAME, PROMPTS_DIR_NAME, RUNTIME_CONFIG_FILENAME\n\nfrom .custom_types import AgentOutput, Artifact, Plan, File\nlogger = logging.getLogger(__name__)\n\nclass System:\n    \"\"\"\n    Represents the entire multi-agent system, handling setup, execution, and state.\n    \"\"\"\n    def __init__(self, root_dir: str, default_model: str = \"Olli\"):\n        \"\"\"\n        Initializes the System.\n\n        Args:\n            root_dir (str): The root directory of the project.\n            default_model (str): The default LLM model to use.\n        \"\"\"\n        self.root_dir = root_dir\n        self.default_model = default_model\n        self.config: dict = {}\n        self.agents: List[Agent] = []\n        self.session: Optional[Session] = None\n        self.orchestrator: Optional[Orchestrator] = None\n\n    def setup(self, orchestrator_name: Optional[str] = None) -> None:\n        \"\"\"\n        Sets up the system by loading configurations and instantiating agents.\n\n        Args:\n            orchestrator_name (str, optional): The name of the orchestrator to use.\n\n        Raises:\n            RuntimeError: If no agents or orchestrator can be set up.\n        \"\"\"\n        logger.info(\"--- System Setup ---\")\n        print(f\"Using default model: {self.default_model}\")\n\n        self.config = self._load_config(os.path.join(self.root_dir, CONFIG_DIR_NAME, RUNTIME_CONFIG_FILENAME))\n        agent_specs = self._load_agent_templates(os.path.join(self.root_dir, AGENTS_DIR_NAME), self.config, self.default_model)\n        prompts = self._load_prompts(os.path.join(self.root_dir, PROMPTS_DIR_NAME))\n\n        all_team_members = {member.lower() for spec in agent_specs if 'team' in spec for member in spec['team']}\n\n        agents: List[Agent] = []\n        for spec in agent_specs:\n            spec[\"model\"] = spec.get(\"model\", self.default_model)\n            if spec['name'].lower() not in all_team_members:\n                agent = self._instantiate_agent(spec, prompts, agent_specs)\n                if agent:\n                    agents.append(agent)\n        self.agents = agents\n\n        if not self.agents:\n            raise RuntimeError(\"No agents could be instantiated. System setup failed.\")\n\n        orchestrator: Optional[Agent] = None\n        if orchestrator_name:\n            orchestrator = next((agent for agent in self.agents if agent.name.lower() == orchestrator_name.lower()), None)\n        else:\n            orchestrator = find_agent_by_role(self.agents, 'Orchestrator')\n\n        if not orchestrator:\n            error_msg = f\"Orchestrator with name '{orchestrator_name}' not found.\" if orchestrator_name else \"Orchestrator with role 'Orchestrator' not found.\"\n            raise RuntimeError(f\"{error_msg} System setup failed.\")\n\n        if not isinstance(orchestrator, Orchestrator):\n            raise RuntimeError(f\"Agent '{orchestrator.name}' is not a valid Orchestrator instance. System setup failed.\")\n\n        self.orchestrator = orchestrator\n        self.session = Session(agents=self.agents, project_root=\"./\")\n        logger.info(\"--- System Setup Complete ---\")\n\n    def start(self, high_level_goal: str, files: List[File] = []) -> Plan:\n        \"\"\"\n        Runs the main workflow of the system.\n\n        Args:\n            initial_task (str): The initial task for the orchestrator to perform.\n            plan_only (bool): If True, only generates the plan without executing tasks.\n            rounds (int): The number of rounds to execute the workflow.\n            files (List[str]): List of files to be used in the task.\n\n        Raises:\n            RuntimeError: If the system is not set up before running.\n        \"\"\"\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call setup() before start().\")\n\n        plan: Optional[Plan] = self.orchestrator.generate_plan(self.session, high_level_goal, files)\n        if not plan:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        logger.debug(f\"{plan.model_dump_json()}\")\n\n        if not plan or not plan.tasks or not plan.roles:\n            raise RuntimeError(\"Orchestration failed: Could not generate a valid plan.\")\n\n        print(f\"\\n\\nInitial Plan: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n        self.session.add_artifact(\"initial_plan.json\", plan.model_dump_json(indent=4))\n\n        return plan\n\n    def run(self, plan: Plan, rounds: int = 1, files: List[File] = []) -> None:\n        if not self.orchestrator or not self.session:\n            raise RuntimeError(\"System is not set up. Please call start() before run().\")\n\n        logger.info(\"--- Starting Workflow ---\")\n\n        context = ExecutionContext(session=self.session, plan=plan)\n\n        print(f\"\\n\\nInitial Files: {plan.model_dump_json(indent=4)}\\n\\n\\n\")\n\n        #context.record_initial(\"files\", \"\\n\".join([file.model_dump_json(indent=4) for file in files]))\n        context.record_initial(\"files\", Artifact(task='initial',files=files).model_dump_json())\n\n        for context.round_num in range(1, rounds + 1):\n            logger.info(f\"System is starting workflow round {context.round_num} for goal: '{plan.high_level_goal}'\")\n\n            team_by_name = {agent.name: agent for agent in self.orchestrator.team.values()} if self.orchestrator.team else {}\n\n            context.reset()\n\n            while context.step_index < len(plan.tasks):\n                step = context.current_step()\n                agent_name = step.agent\n\n                delegate_agent = team_by_name.get(agent_name)\n                if not delegate_agent:\n                    delegate_agent = next((agent for agent in self.agents if agent.name.lower() == agent_name.lower()), None)\n\n                if not delegate_agent:\n                    logger.warning(f\"No agent found with name '{agent_name}'. Execution will continue with the Orchestrator as the fallback agent.\")\n                    delegate_agent = self.orchestrator\n\n                logger.info(f\"Agent '{delegate_agent.name}' is executing step {context.step_index + 1}/{len(plan.tasks)}: '{step.description}' (Role: {step.role})\")\n\n                result = delegate_agent.execute_task(context)\n                if result:\n                    context.record_artifact(f\"{delegate_agent.name}_result.txt\", result, True)\n                    try:\n                        agent_output = AgentOutput.model_validate_json(result)\n                        if agent_output.team and agent_output.team.prompts:\n                            logger.info(f\"Agent {delegate_agent.name} provided new prompts.\")\n                            for prompt_data in agent_output.team.prompts:\n                                self._update_agent_prompt(self.session, prompt_data.agent, prompt_data.system_prompt)\n                    except (json.JSONDecodeError, Exception) as e:\n                        logger.warning(f\"Could not parse agent output as AgentOutput: {e}. Treating as plain text.\")\n                context.next()\n            logger.info(f\"System has completed workflow round {context.round_num}.\")\n        logger.info(\"--- Workflow Complete ---\")\n\n    def _update_agent_prompt(self, session: Session, agent_name: str, new_prompt: str) -> None:\n        \"\"\"\n        Updates an agent's system prompt.\n\n        Args:\n            session (Session): The current session object.\n            agent_name (str): The name or role of the agent to update.\n            new_prompt (str): The new system prompt.\n        \"\"\"\n        if not (agent_name and new_prompt):\n            return\n\n        if not (self.orchestrator and self.orchestrator.team):\n            logger.warning(\"Orchestrator or its team is not initialized. Cannot update agent prompt.\")\n            return\n\n        # Combine team agents and session agents for a comprehensive search, ensuring uniqueness\n        all_agents = {agent.name: agent for agent in list(self.orchestrator.team.values()) + session.agents}.values()\n\n        # First, try to find by name\n        target_agent = next((agent for agent in all_agents if agent.name.lower() == agent_name.lower()), None)\n\n        # If not found by name, try to find by role\n        if not target_agent:\n            target_agent = next((agent for agent in all_agents if agent.role.lower() == agent_name.lower()), None)\n\n        if target_agent:\n            target_agent.update_system_prompt(new_prompt)\n            logger.info(f\"Agent '{target_agent.name}' (matched by '{agent_name}') system prompt updated.\")\n        else:\n            logger.warning(f\"Target agent '{agent_name}' not found for prompt update.\")\n\n    def _instantiate_agent(self, agent_spec: Dict[str, Any], prompts: Dict[str, str], all_agent_specs: List[Dict[str, Any]]) -> Optional[Agent]:\n        \"\"\"\n        Instantiates an Agent (or a subclass) based on the provided specification.\n\n        Args:\n            agent_spec (Dict[str, Any]): A dictionary containing the agent's specifications.\n            prompts (Dict[str, str]): A dictionary of available system prompts.\n            all_agent_specs (List[Dict[str, Any]]): A list of all agent specifications for team resolution.\n\n        Returns:\n            Optional[Agent]: An instantiated Agent object, or None if the prompt is not found.\n        \"\"\"\n        prompt_key = f\"{agent_spec['name'].lower()}_instructions.txt\"\n        if prompt_key not in prompts:\n            logger.warning(\n                f\"Prompt for agent '{agent_spec['name']}' not found with key '{prompt_key}'. Empty system prompt will be used.\"\n            )\n\n        agent = Orchestrator(\n            name=agent_spec.get(\"name\", \"Unnamed Agent\"),\n            role=agent_spec.get(\"role\", \"Agent\"),\n            goal=agent_spec.get(\"goal\", \"\"),\n            model=agent_spec.get(\"model\", \"default-model\"),\n            system_prompt=\"\" if prompt_key not in prompts else prompts[prompt_key],\n        )\n\n        if agent_spec.get(\"delegation\") and \"team\" in agent_spec:\n            agent.team = {}\n            for team_member_name in agent_spec[\"team\"]:\n                member_spec = next(\n                    (\n                        s\n                        for s in all_agent_specs\n                        if s[\"name\"].lower() == team_member_name.lower()\n                    ),\n                    None,\n                )\n                if member_spec:\n                    member_agent = self._instantiate_agent(\n                        member_spec, prompts, all_agent_specs\n                    )  # Recursive call\n                else:\n                    logger.warning(\n                        f\"Team member '{team_member_name}' not found in agent specs.\"\n                    )\n                    member_agent = self._instantiate_agent(\n                        agent_spec={\n                            \"name\": team_member_name,\n                            \"role\": \"Unknown\",\n                            \"goal\": \"\",\n                            \"model\": agent_spec.get(\"model\", \"default-model\"),\n                            \"system_prompt\": \"\",\n                        },\n                        prompts=prompts,\n                        all_agent_specs=all_agent_specs,\n                    )  # Recursive call\n                if member_agent:\n                    agent.team[team_member_name] = member_agent\n        return agent\n\n    def _load_config(self, config_path: str) -> Any:\n        \"\"\"\n        Loads the runtime configuration from a YAML file.\n\n        Args:\n            config_path (str): The absolute path to the configuration file.\n\n        Returns:\n            dict: The loaded configuration as a dictionary.\n        \"\"\"\n        logger.info(f\"Loading configuration from: {config_path}\")\n        with open(config_path, 'r', encoding='utf-8') as f:\n            return yaml.safe_load(f)\n\n    def _load_agent_templates(self, agents_dir: str, config: dict, default_model: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Loads all agent specifications from YAML files within a specified directory.\n\n        Args:\n            agents_dir (str): The absolute path to the directory containing agent YAML files.\n\n        Returns:\n            list: A list of dictionaries, where each dictionary represents an agent's specification.\n        \"\"\"\n        logger.info(f\"Loading agent templates from: {agents_dir}\")\n        agent_files = glob(os.path.join(agents_dir, '*.yaml'))\n        templates = []\n        for agent_file in agent_files:\n            with open(agent_file, 'r', encoding='utf-8') as f:\n                template = yaml.safe_load(f)\n                # Resolve prompt path relative to the agent file's directory\n                if 'system_prompt' in template and template['system_prompt']:\n                    base_dir = os.path.dirname(agent_file)\n                    prompt_path = os.path.abspath(os.path.join(base_dir, template['system_prompt']))\n                    template['system_prompt_path'] = prompt_path\n                template[\"model\"] = default_model\n                templates.append(template)\n        logger.info(f\"{len(templates)} agent templates loaded.\")\n        return templates\n\n    def _load_prompts(self, prompts_dir: str) -> Dict[str, str]:\n        \"\"\"\n        Loads all system prompts from text files within a specified directory.\n\n        Args:\n            prompts_dir (str): The absolute path to the directory containing prompt text files.\n\n        Returns:\n            dict: A dictionary where keys are prompt filenames (e.g., 'agent_instructions.txt')\n                and values are the content of the prompt files.\n        \"\"\"\n        logger.info(f\"Loading prompts from: {prompts_dir}\")\n        prompt_files = glob(os.path.join(prompts_dir, '*.txt'))\n        prompts = {}\n        for prompt_file in prompt_files:\n            with open(prompt_file, 'r', encoding='utf-8') as f:\n                # Use the absolute path as the key for reliable lookup\n                prompts[os.path.basename(prompt_file)] = f.read()\n        logger.info(f\"{len(prompts)} prompts loaded.\")\n        return prompts\n"},{"path":"runtime/util.py","content":"\"\"\"This module provides miscellaneous utility functions for the runtime.\n\nIt includes helper functions that are used across different modules\nbut do not belong to a specific component.\n\"\"\"\n\nimport os\n\ndef read_file(file_path: str) -> str:\n    \"\"\"Reads content from a given file path.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except FileNotFoundError:\n        return f\"Error: File not found at {file_path}\"\n    except Exception as e:\n        return f\"Error reading file {file_path}: {e}\"\n"}]}

--- Artifact '__1/__step_1_Lyra_result.txt' from (Prompt Engineer) in [T2]:
{"output":"Based on the provided Task Agnostic Steps (TAS), the following documentation structure is proposed:\n\n1.  **Introduction**\n    *   Purpose and Scope of the Runtime Documentation\n    *   Target Audience\n    *   Overview of Runtime Components\n\n2.  **Getting Started**\n    *   Installation and Setup\n    *   Quick Start Guide / Basic Usage Example\n\n3.  **Core Concepts**\n    *   Explanation of key concepts (e.g., Agents, Sessions, Context, Plans, Artifacts)\n    *   Workflow Execution Model\n\n4.  **API Reference**\n    *   Detailed documentation for each public API, including:\n        *   Function/Method Signature\n        *   Parameters (name, type, description, optional/required)\n        *   Return Values (type, description)\n        *   Error Handling\n        *   Code Examples\n\n5.  **Advanced Usage & Tutorials**\n    *   In-depth guides on specific features or complex scenarios.\n    *   Examples of extending the runtime (e.g., custom agents, LLM integrations).\n\n6.  **Contribution Guide**\n    *   Information for developers who wish to contribute to the runtime.\n    *   Coding standards, pull request process, etc.\n\n7.  **Troubleshooting & FAQ**\n    *   Common issues and their solutions.\n    *   Frequently asked questions.\n\n**Key Areas to be Documented:**\n\n*   **Agent Class:** Details on its lifecycle, task execution, and helper functions.\n*   **ExecutionContext:** Explanation of state management, artifact handling, and context.\n*   **Session Class:** Information on session management, artifact storage, and directory structure.\n*   **LLM Abstraction:** How different LLMs are integrated and used.\n*   **Orchestrator:** Role in plan generation and workflow management.\n*   **System Class:** Overall system architecture, configuration, and execution flow.\n*   **Custom Types:** Definitions and usage of Pydantic models (Plan, Task, AgentOutput, etc.).\n*   **Configuration Files:** Structure and usage of configuration files (e.g., `runtime.yaml`).\n*   **Prompts:** Explanation of prompt management and usage within the system.\n*   **Logging:** Configuration and usage of the logging system.\n*   **Command-Line Interface (CLI):** Usage and arguments for running the system.","artifact":null,"team":null,"reasoning":"The user wants me to define the overall documentation structure and identify key areas to be documented based on the extracted Task Agnostic Steps (TAS). The TAS provided a list of steps, each with a description and keywords. I have synthesized these into a logical documentation structure and then identified specific components and concepts from the provided code that need to be documented within that structure. The structure follows a typical pattern for technical documentation, starting with an introduction and getting started guide, moving to core concepts and API details, and concluding with advanced topics and publishing information. The key areas to be documented are derived directly from the filenames and core classes within the provided Python code, ensuring comprehensive coverage of the runtime system."}

--- Artifact '__1/__step_2_Lyra_result.txt' from (Prompt Engineer) in [T3]:
{"output":"The following system prompts have been developed for each agent involved in the documentation process, ensuring clarity and alignment with their roles and the overall goal of generating documentation for the runtime.","artifact":{"task":"T3","files":[{"path":"runtime/prompts/tas_extractor_instructions.txt","content":"You are the Task-Agnostic Step (TAS) Extractor. Your goal is to identify and extract all \"Task Agnostic Steps\" (TAS) from the provided code and context. These steps should represent the fundamental actions or stages required to achieve the high-level goal, independent of specific implementation details or agent assignments. Focus on the \"what\" needs to be done, not the \"how\" or \"who\".\n\nFor each identified TAS, provide the following:\n- **ID:** A unique identifier for the step (e.g., TAS-1, TAS-2).\n- **Description:** A clear and concise description of the step.\n- **Purpose:** Explain the purpose of this step in relation to the overall goal.\n- **Keywords:** A list of keywords or concepts associated with the step.\n\nExample Output Format:\n```json\n[\n  {\n    \"id\": \"TAS-1\",\n    \"description\": \"Understand the overall goal of the project.\",\n    \"purpose\": \"To establish the primary objective that guides all subsequent actions and documentation efforts.\",\n    \"keywords\": [\"goal\", \"objective\", \"overview\"]\n  },\n  {\n    \"id\": \"TAS-2\",\n    \"description\": \"Identify and define the roles and responsibilities of each agent involved.\",\n    \"purpose\": \"To clarify the contribution of each agent to the project and ensure proper task delegation.\",\n    \"keywords\": [\"roles\", \"responsibilities\", \"agents\", \"team\"]\n  }\n]\n```\n\nAnalyze the provided code and context to extract these steps. Do not assign agents or specific tasks yet; focus solely on the abstract steps required to achieve the high-level goal."},{"path":"runtime/prompts/prompt_engineer_instructions.txt","content":"You are the Prompt Engineer. Your primary responsibility is to structure workflows and ensure clarity in agent instructions and system prompt engineering. For the documentation generation goal, you will:\n\n1.  **Define Documentation Structure:** Based on the Task Agnostic Steps (TAS) extracted by the TAS Extractor, define a logical and comprehensive structure for the runtime documentation. Identify key sections and areas that need to be documented.\n2.  **Develop System Prompts:** Create clear, concise, and effective system prompts for each agent involved in the documentation process. Ensure these prompts align with the agent's role, the overall goal, and the defined documentation structure.\n3.  **Oversee and Refine:** Review the prompts and workflow for clarity, coherence, and alignment with the project goals. Iterate on prompts as needed to improve agent performance and output quality.\n\n**Your current sub-task is to develop system prompts for each agent involved in the documentation process, ensuring clarity and alignment with their roles and the overall goal.**\n\nWhen developing system prompts, consider the following:\n*   **Clarity:** Prompts should be unambiguous and easy for the agent to understand.\n*   **Role Alignment:** Prompts must clearly define the agent's specific role and responsibilities within the context of the documentation task.\n*   **Goal Orientation:** Prompts should guide the agent towards achieving the overall goal of generating runtime documentation.\n*   **Contextual Awareness:** Prompts should provide necessary context, such as the overall goal, previous steps, and available information.\n*   **Output Format:** Specify the desired output format when necessary (e.g., JSON, Markdown, specific structure).\n\nRefer to the provided plan and any available artifacts for context. Your output should be a JSON object containing a list of `Prompt` objects, each specifying the `agent`, `role`, and `system_prompt` for a particular agent."},{"path":"runtime/prompts/designer_instructions.txt","content":"You are the Designer. Your role is to generate aesthetic layouts, color palettes, typography, and UI flows for the runtime documentation. Ensure that your designs are accessible, visually balanced, and enhance the user's understanding of the technical content.\n\nYour specific goal is to:\n1.  **Propose a Visual Style:** Define a consistent visual theme for the documentation, including color palettes, typography, and layout principles.\n2.  **Design UI Elements:** Create mockups or wireframes for key documentation pages, focusing on readability, navigation, and user experience.\n3.  **Ensure Accessibility:** Adhere to accessibility guidelines (e.g., WCAG) in all design decisions.\n4.  **Provide Design Specifications:** Deliver clear specifications for the implementation of the design, including any necessary assets (e.g., icons, images).\n\nWhen creating designs, consider the target audience and the technical nature of the content. The goal is to make complex information accessible and engaging through thoughtful visual design."},{"path":"runtime/prompts/documentation_writer_instructions.txt","content":"You are the Documentation Writer. Your primary role is to generate clear, concise, and accurate technical documentation based on provided specifications, source code, and developer notes.\n\nYour specific goals are to:\n1.  **Draft Documentation Sections:** Write the content for various sections of the runtime documentation based on the defined structure and provided inputs.\n2.  **Ensure Accuracy and Clarity:** Verify the technical accuracy of the content and ensure it is easy to understand for the target audience.\n3.  **Adhere to Standards:** Follow project-specific documentation standards and style guides.\n4.  **Incorporate Feedback:** Revise documentation based on feedback from reviewers.\n\nWhen writing, focus on:\n*   **Technical Correctness:** Ensure all information is accurate and up-to-date.\n*   **Readability:** Use clear language, concise sentences, and appropriate formatting (e.g., code blocks, lists).\n*   **Completeness:** Cover all necessary details for each documented feature or component.\n*   **Consistency:** Maintain a consistent tone and style throughout the documentation.\n\nRefer to the overall goal, the defined documentation structure, and any provided artifacts or specifications to guide your writing process."},{"path":"runtime/prompts/codex_instructions.txt","content":"You are Kodax, the Engineer. Your role is to implement the features and provide the technical details necessary for generating the runtime documentation. This includes writing or refining code snippets, explaining technical concepts, and ensuring the code examples are accurate and functional.\n\nYour specific goals are to:\n1.  **Provide Code Examples:** Generate accurate and relevant code examples for the documentation.\n2.  **Explain Technical Details:** Offer clear explanations of the runtime's technical implementation, architecture, and key components.\n3.  **Refine Documentation Content:** Assist the Documentation Writer by providing technical insights and reviewing generated content for accuracy.\n4.  **Implement Documentation Structure:** If required, contribute to the implementation of the documentation's structure, such as generating Markdown files or integrating code snippets.\n\nFocus on providing precise, actionable technical information that supports the creation of high-quality runtime documentation."},{"path":"runtime/prompts/lyra_instructions.txt","content":"You are Lyra, the Prompt Engineer and Architect. Your overarching goal is to ensure the smooth execution of the multi-agent system by architecting the workflow and engineering the system prompts for each agent. You are responsible for structuring the overall process, defining agent interactions, and refining prompts to guarantee clarity, coherence, and alignment with the high-level goal.\n\nYour specific sub-task is to **Develop system prompts for each agent involved in the documentation process, ensuring clarity and alignment with their roles and the overall goal.**\n\nWhen developing system prompts, you should:\n*   **Understand the High-Level Goal:** Keep the ultimate objective ('Generate documentation for the runtime') in mind.\n*   **Analyze Agent Roles:** Clearly define the purpose and responsibilities of each agent within the documentation workflow.\n*   **Ensure Clarity and Specificity:** Craft prompts that are unambiguous and provide precise instructions.\n*   **Promote Coherence:** Ensure that the prompts for different agents work together harmoniously to achieve the overall goal.\n*   **Iterate and Refine:** Be prepared to update prompts based on feedback and observed agent performance.\n\nYour output should be a JSON object containing a list of `Prompt` objects, each detailing the `agent`, `role`, and `system_prompt` for a specific agent. You will use the provided plan and artifacts to inform your prompt engineering decisions."},{"path":"runtime/prompts/aurora_instructions.txt","content":"You are Aurora, the Designer. Your role is to create the visual aspects of the runtime documentation. This includes defining the overall aesthetic, color schemes, typography, and layout. You should ensure the design is user-friendly, accessible, and effectively presents the technical information.\n\nYour specific goals are to:\n1.  **Define Visual Style:** Propose a coherent visual style guide for the documentation, including color palettes, fonts, and spacing.\n2.  **Design Layouts:** Create mockups or wireframes for key documentation pages, focusing on readability and navigation.\n3.  **Ensure Accessibility:** Implement designs that meet accessibility standards.\n4.  **Provide Design Assets:** Deliver any necessary visual assets (icons, images) and specifications for implementation.\n\nFocus on creating a visually appealing and intuitive interface that complements the technical content and enhances the user experience."},{"path":"runtime/prompts/lex_instructions.txt","content":"You are Lex, the Documentation Writer. Your core responsibility is to generate the content for the runtime documentation. You will take the defined structure and technical details and produce clear, concise, and accurate written material.\n\nYour specific goals are to:\n1.  **Write Documentation Content:** Draft the text for each section of the documentation based on the provided structure and inputs.\n2.  **Ensure Accuracy and Clarity:** Review and refine the content for technical accuracy and readability.\n3.  **Format Content:** Structure the documentation using appropriate markup (e.g., Markdown) and formatting.\n\nWhen writing, ensure your output is directly usable for the final documentation. Focus on conveying technical information effectively to the intended audience."}]},"team":null,"reasoning":null}



Please execute your sub-task, keeping the overall goal and your role's specific goal in mind to ensure your output is relevant to the project.